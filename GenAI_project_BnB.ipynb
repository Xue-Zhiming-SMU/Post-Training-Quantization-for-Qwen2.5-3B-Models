{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Xue-Zhiming-SMU/Post-Training-Quantization-for-Qwen2.5-3B-Models/blob/main/GenAI_project_BnB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "w-Zd8Tzy20SA"
      },
      "outputs": [],
      "source": [
        "!pip install -U transformers torch accelerate\n",
        "!pip install -U datasets\n",
        "!pip install -U bitsandbytes>=0.39.0\n",
        "!pip install pynvml torch-summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYGB95B9ECk_"
      },
      "source": [
        "# PTQ Evaluation Using BitsAndBytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Amufz2uO_4Pg"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "import random\n",
        "import gc\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "import json\n",
        "import os\n",
        "import pynvml\n",
        "import threading\n",
        "import statistics\n",
        "from datasets import load_dataset, DownloadMode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTLHFZ0XKWbO"
      },
      "source": [
        "## MMLU Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8CIPHonz8hSA"
      },
      "outputs": [],
      "source": [
        "# Load MMLU test dataset\n",
        "from datasets import load_dataset\n",
        "mmlu_test = load_dataset(\"cais/mmlu\", \"all\", split=\"test\")\n",
        "\n",
        "print('----------------------')\n",
        "print(f\"Test set size: {len(mmlu_test)}\")\n",
        "print('----------------------')\n",
        "print(mmlu_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_MdUla-x8llC"
      },
      "outputs": [],
      "source": [
        "# Set a seed for reproducibility\n",
        "import random\n",
        "random.seed(42)\n",
        "\n",
        "# Take a random sample of 200 from the test dataset\n",
        "samples_200_mmlu_test = random.sample(list(mmlu_test), 200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5quDosky_80Q"
      },
      "outputs": [],
      "source": [
        "def evaluate_mmlu(model, tokenizer, dataset, max_new_tokens=1):\n",
        "    # --- Existing setup ---\n",
        "    start_time = time.time()\n",
        "    total_tokens_processed = 0\n",
        "    results = {}\n",
        "    correct = 0\n",
        "    total_perplexity = 0\n",
        "    perplexity_count = 0\n",
        "    choice_letters = [\"A\", \"B\", \"C\", \"D\"]\n",
        "    # --- New tracking lists ---\n",
        "    memory_readings_gb = [] # To store memory allocated per step\n",
        "    previous_memory_allocated = 0 # For leak check\n",
        "\n",
        "    # --- Existing loop ---\n",
        "    for i, sample in enumerate(tqdm(dataset)):\n",
        "        # --- For Memory Leak Check ---\n",
        "        memory_before_sample_gb = torch.cuda.memory_allocated() / (1024 ** 3)\n",
        "        if i > 0 and memory_before_sample_gb > previous_memory_allocated:\n",
        "             pass # You can add more sophisticated logging here if needed\n",
        "\n",
        "        subject = sample[\"subject\"]\n",
        "        if subject not in results:\n",
        "             # ... (rest of results initialization) ...\n",
        "             results[subject] = {\"correct\": 0, \"total\": 0, \"perplexity\": 0, \"exact_matches\": 0}\n",
        "\n",
        "        results[subject][\"total\"] += 1\n",
        "        question = sample[\"question\"] + \"\\n\"\n",
        "        # ... (rest of question formatting) ...\n",
        "        for j, choice_text in enumerate(sample[\"choices\"]):\n",
        "            choice_letter = choice_letters[j]\n",
        "            question += f\"{choice_letter}. {choice_text}\\n\"\n",
        "        question += \"Answer:\"\n",
        "        correct_idx = sample[\"answer\"]\n",
        "        correct_letter = choice_letters[correct_idx]\n",
        "        inputs = tokenizer(question, return_tensors=\"pt\").to(model.device)\n",
        "        input_length = inputs.input_ids.shape[1]\n",
        "\n",
        "        # --- Record Memory BEFORE major operations for this sample ---\n",
        "        # Taking it here captures memory state just before inference/generation\n",
        "        current_memory_allocated_gb = torch.cuda.memory_allocated() / (1024 ** 3)\n",
        "        memory_readings_gb.append(current_memory_allocated_gb)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # --- Existing Perplexity Calculation ---\n",
        "            try:\n",
        "                outputs = model(inputs.input_ids, labels=inputs.input_ids)\n",
        "                loss = outputs.loss\n",
        "                if not torch.isnan(loss) and not torch.isinf(loss):\n",
        "                    perplexity = torch.exp(loss).item()\n",
        "                    if 0 < perplexity < float('inf'):\n",
        "                        total_perplexity += perplexity\n",
        "                        results[subject][\"perplexity\"] += perplexity\n",
        "                        perplexity_count += 1\n",
        "                    # else: print(...) # Optional: keep filtering message\n",
        "                # else: print(...) # Optional: keep NaN/Inf message\n",
        "            except Exception as e:\n",
        "                print(f\"Perplexity calculation error: {e}\")\n",
        "                perplexity = None\n",
        "\n",
        "            # --- Existing Generation ---\n",
        "            gen_outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=max_new_tokens,\n",
        "                pad_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "        # --- Existing Prediction Parsing ---\n",
        "        generated_text = tokenizer.decode(gen_outputs[0], skip_special_tokens=True)\n",
        "        predicted_text = generated_text[len(question):].strip()\n",
        "        predicted_letter = None\n",
        "        # ... (rest of prediction logic) ...\n",
        "        if predicted_text in choice_letters: predicted_letter = predicted_text\n",
        "        elif predicted_text and predicted_text[0] in choice_letters: predicted_letter = predicted_text[0]\n",
        "        else:\n",
        "             for letter in choice_letters:\n",
        "                 if letter in predicted_text: predicted_letter = letter; break\n",
        "\n",
        "        is_correct = predicted_letter == correct_letter\n",
        "        if is_correct:\n",
        "            correct += 1\n",
        "            results[subject][\"correct\"] += 1\n",
        "\n",
        "        generated_length = gen_outputs.shape[1] - input_length\n",
        "        total_tokens_processed += generated_length\n",
        "\n",
        "        # --- Update memory for leak check AFTER sample processing ---\n",
        "        previous_memory_allocated = torch.cuda.memory_allocated() / (1024 ** 3)\n",
        "\n",
        "    # --- Final Calculations ---\n",
        "    total_time = time.time() - start_time\n",
        "    accuracy = correct / len(dataset) if len(dataset) > 0 else 0\n",
        "    avg_perplexity = (total_perplexity / perplexity_count) if perplexity_count > 0 else None\n",
        "    total_throughput = total_tokens_processed / total_time if total_time > 0 else 0\n",
        "    avg_inference_speed = total_throughput # Keep consistent if measuring same thing\n",
        "    avg_inference_latency = (total_time * 1000) / total_tokens_processed if total_tokens_processed > 0 else 0\n",
        "\n",
        "    # --- NEW: Calculate average memory from readings ---\n",
        "    avg_memory_allocated_gb = statistics.mean(memory_readings_gb) if memory_readings_gb else 0\n",
        "\n",
        "    # --- Peak memory still uses torch function (reset before calling evaluate_mmlu) ---\n",
        "    max_memory_allocated_gb = torch.cuda.max_memory_allocated() / (1024 ** 3)\n",
        "\n",
        "    for subject in results:\n",
        "        results[subject][\"accuracy\"] = results[subject][\"correct\"] / results[subject][\"total\"] if results[subject][\"total\"] > 0 else 0\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"correct\": correct,\n",
        "        \"total\": len(dataset),\n",
        "        \"subject_results\": results,\n",
        "        \"inference_speed\": avg_inference_speed,\n",
        "        \"throughput\": total_throughput,\n",
        "        \"inference_latency\": avg_inference_latency,\n",
        "        \"perplexity\": avg_perplexity,\n",
        "        \"peak_gpu_memory_gb\": max_memory_allocated_gb,\n",
        "        \"avg_gpu_memory_gb\": avg_memory_allocated_gb, # Use the new calculated average\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ehnsjvmTk8VE"
      },
      "outputs": [],
      "source": [
        "# Function to get GPU utilization\n",
        "gpu_utilization_readings = []\n",
        "# Event to signal the polling thread to stop\n",
        "stop_polling_event = threading.Event()\n",
        "\n",
        "# Function for the polling thread\n",
        "def poll_gpu_utilization(handle, interval=1.0):\n",
        "    \"\"\"Polls GPU utilization at a set interval.\"\"\"\n",
        "    global gpu_utilization_readings\n",
        "    while not stop_polling_event.is_set():\n",
        "        try:\n",
        "            utilization = pynvml.nvmlDeviceGetUtilizationRates(handle)\n",
        "            gpu_utilization_readings.append(utilization.gpu)\n",
        "        except pynvml.NVMLError as e:\n",
        "            print(f\"NVML Error polling utilization: {e}\")\n",
        "            # Decide if you want to break or continue\n",
        "            time.sleep(interval) # Wait before retrying or next poll\n",
        "            continue # Skip appending if error occurred\n",
        "        time.sleep(interval)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 838
        },
        "id": "JJmhJ0Do_-Qy",
        "outputId": "9060b3ea-c128-4536-9428-e80af5a32bdd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NVML handle not found. Attempting to initialize NVML...\n",
            "NVML initialized successfully.\n",
            "Loading model: Qwen/Qwen2.5-3B (FP32)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/683 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "52c27c0157984a90b4e3a211c043e1a4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/35.6k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6bcf191328ef42358cf5f664d833ce97"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a492660dd9e94e8181b89bc8cf71eb23"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/3.97G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "641f0f2d4494420fa60992973dd3abf1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2f30c01744d5438cbf57d835ab0d8026"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "003b4b5484b54d1dad307fe372e518cb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/138 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0e4b6791487a4b749c69a49dcefd1784"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded.\n",
            "Loading tokenizer...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/7.23k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "545eb547d3c9443c9f9ca8abbb1d82a0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "90c1f932db7347c4811303747715edbc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6bf82dc24b7a414b8bd5b3e6c6619a6e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cadd7afc245b44fcac160befc3185e72"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizer loaded.\n",
            "Starting MMLU evaluation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [00:47<00:00,  4.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MMLU evaluation finished.\n",
            "Stopping GPU polling thread...\n",
            "Average GPU Utilization: 47.33%\n",
            "Polling stopped.\n",
            "Collecting metrics...\n",
            "\n",
            "===== FP32 DETAILED MODEL METRICS =====\n",
            "--------------------------------------------------\n",
            "PPL (Perplexity)                   : 7.0457\n",
            "Accuracy                           : 61.0000\n",
            "Memory Footprint (Model Size) (GB) : 11.4960\n",
            "Inference Latency (ms/token)       : 239.3102\n",
            "Avg GPU Utilization (%)            : 47.3263\n",
            "Avg GPU Memory Allocated (GB)      : 11.5864\n",
            "--------------------------------------------------\n",
            "Starting cleanup for FP32 cell...\n",
            "FP32 model deleted.\n",
            "GPU cache cleared and garbage collected after FP32 run.\n"
          ]
        }
      ],
      "source": [
        "# --- FP32 Evaluation Cell ---\n",
        "\n",
        "if 'handle' not in locals() or not handle:\n",
        "    print(\"NVML handle not found. Attempting to initialize NVML...\")\n",
        "    try:\n",
        "        pynvml.nvmlInit()\n",
        "        handle = pynvml.nvmlDeviceGetHandleByIndex(0) # Assuming GPU 0\n",
        "        print(\"NVML initialized successfully.\")\n",
        "        # Define the stop event here if this is the first initialization\n",
        "        if 'stop_polling_event' not in locals():\n",
        "             stop_polling_event = threading.Event()\n",
        "    except pynvml.NVMLError as error:\n",
        "        print(f\"Failed to initialize NVML: {error}. Skipping GPU utilization polling.\")\n",
        "        handle = None # Ensure handle is None if init failed\n",
        "elif 'stop_polling_event' not in locals():\n",
        "     # If handle exists but event doesn't, create event\n",
        "     print(\"NVML handle found, initializing stop event.\")\n",
        "     stop_polling_event = threading.Event()\n",
        "\n",
        "\n",
        "# --- Polling Setup ---\n",
        "# Clear previous readings and reset event for this specific evaluation\n",
        "gpu_utilization_readings = []\n",
        "if 'stop_polling_event' in locals(): # Ensure event exists before clearing\n",
        "    stop_polling_event.clear()\n",
        "else:\n",
        "    # This case should ideally not happen if initialized correctly above/before\n",
        "    print(\"Warning: stop_polling_event not defined. Polling might not stop correctly.\")\n",
        "    stop_polling_event = threading.Event() # Define as fallback\n",
        "\n",
        "# Start GPU polling thread (only if NVML handle exists)\n",
        "polling_thread = None\n",
        "if handle:\n",
        "    polling_thread = threading.Thread(target=poll_gpu_utilization, args=(handle, 1.0), daemon=True)\n",
        "    polling_thread.start()\n",
        "else:\n",
        "    # Message already printed during init check if NVML failed\n",
        "    pass\n",
        "\n",
        "# --- Evaluation ---\n",
        "# Clean memory before starting model loading/evaluation\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "fp32_metrics = {}\n",
        "fp32_results_data = {} # To store results\n",
        "\n",
        "# Define model variable outside try for finally block access\n",
        "model = None\n",
        "\n",
        "try:\n",
        "    # Load full precision model (Use your actual model name)\n",
        "    model_name = \"Qwen/Qwen2.5-3B\"\n",
        "    print(f\"Loading model: {model_name} (FP32)...\")\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        torch_dtype=torch.float32, # Explicitly FP32\n",
        "        device_map=\"cuda:0\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "    print(\"Model loaded.\")\n",
        "\n",
        "    # Load tokenizer (can often reuse if already loaded)\n",
        "    print(\"Loading tokenizer...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    print(\"Tokenizer loaded.\")\n",
        "\n",
        "    # --- reset_peak_memory_stats REMOVED ---\n",
        "\n",
        "    # Run evaluation\n",
        "    print(\"Starting MMLU evaluation...\")\n",
        "    fp32_results_data = evaluate_mmlu(model, tokenizer, samples_200_mmlu_test, max_new_tokens=1)\n",
        "    print(\"MMLU evaluation finished.\")\n",
        "\n",
        "    # Stop the polling thread AFTER evaluation is done (if it was started)\n",
        "    avg_gpu_utilization = 'N/A'\n",
        "    if polling_thread and polling_thread.is_alive():\n",
        "        print(\"Stopping GPU polling thread...\")\n",
        "        stop_polling_event.set()\n",
        "        polling_thread.join(timeout=5) # Add timeout for safety\n",
        "        if polling_thread.is_alive():\n",
        "             print(\"Warning: Polling thread did not stop in time.\")\n",
        "        if gpu_utilization_readings:\n",
        "            avg_gpu_utilization = statistics.mean(gpu_utilization_readings)\n",
        "            print(f\"Average GPU Utilization: {avg_gpu_utilization:.2f}%\")\n",
        "        else:\n",
        "             # If polling ran but got no readings (e.g., error in poll function)\n",
        "             print(\"Polling ran but collected no readings.\")\n",
        "             avg_gpu_utilization = 0\n",
        "        print(\"Polling stopped.\")\n",
        "    elif handle: # Polling should have started but didn't/failed early\n",
        "        print(\"Polling thread was not running or failed to start correctly.\")\n",
        "        avg_gpu_utilization = 0\n",
        "    else: # NVML wasn't initialized\n",
        "        print(\"GPU polling was skipped (NVML not initialized).\")\n",
        "        avg_gpu_utilization = 'N/A'\n",
        "\n",
        "\n",
        "    # Collect the desired metrics\n",
        "    print(\"Collecting metrics...\")\n",
        "    fp32_metrics = {\n",
        "        \"PPL (Perplexity)\": fp32_results_data.get('perplexity', 'N/A'),\n",
        "        \"Accuracy\": fp32_results_data.get('accuracy', 'N/A') * 100,\n",
        "        \"Memory Footprint (Model Size) (GB)\": model.get_memory_footprint() / (1024 ** 3),\n",
        "        \"Inference Latency (ms/token)\": fp32_results_data.get('inference_latency', 'N/A'),\n",
        "        \"Avg GPU Utilization (%)\": avg_gpu_utilization, # Use calculated average\n",
        "        \"Avg GPU Memory Allocated (GB)\": fp32_results_data.get('avg_gpu_memory_gb', 'N/A'), # Use calculated average\n",
        "    }\n",
        "\n",
        "    # Print metrics in a formatted way\n",
        "    print(\"\\n===== FP32 DETAILED MODEL METRICS =====\")\n",
        "    print(\"-\" * 50)\n",
        "    # Ensure metrics dictionary is not empty before finding max length\n",
        "    if fp32_metrics:\n",
        "        max_key_length = max(len(key) for key in fp32_metrics.keys())\n",
        "        for key, value in fp32_metrics.items():\n",
        "             # Basic type checking for formatting\n",
        "             if isinstance(value, (float, int)) and value != 'N/A':\n",
        "                 print(f\"{key.ljust(max_key_length)} : {value:.4f}\")\n",
        "             else:\n",
        "                 print(f\"{key.ljust(max_key_length)} : {value}\")\n",
        "    else:\n",
        "        print(\"No metrics collected.\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "except pynvml.NVMLError as nvml_error:\n",
        "     print(f\"NVML Error during FP32 evaluation steps: {nvml_error}\")\n",
        "except Exception as e:\n",
        "    print(f\"FP32 Evaluation error: {e}\")\n",
        "    # Ensure polling stops even if there's an error mid-evaluation\n",
        "    if polling_thread and polling_thread.is_alive():\n",
        "        print(\"Stopping polling thread due to evaluation error...\")\n",
        "        stop_polling_event.set()\n",
        "        try:\n",
        "            polling_thread.join(timeout=5)\n",
        "        except Exception as join_e:\n",
        "            print(f\"Error stopping polling thread after exception: {join_e}\")\n",
        "\n",
        "finally:\n",
        "    # --- Correct Cleanup (No NVML Shutdown Here) ---\n",
        "    print(\"Starting cleanup for FP32 cell...\")\n",
        "    # Clean up GPU memory specific to this cell's model\n",
        "    if 'model' in locals() and model is not None: # Check if model exists\n",
        "        del model\n",
        "        print(\"FP32 model deleted.\")\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    print(\"GPU cache cleared and garbage collected after FP32 run.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396
        },
        "id": "FpPkswUaDXzJ",
        "outputId": "f4201917-a716-4bd1-82a4-08613ea18b78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model: Qwen/Qwen2.5-3B (FP16)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "15a1d3fe4ad54560890dcf213e666a55"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded.\n",
            "Loading tokenizer...\n",
            "Tokenizer loaded.\n",
            "Starting MMLU evaluation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [00:21<00:00,  9.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MMLU evaluation finished.\n",
            "\n",
            "===== FP16 DETAILED MODEL METRICS =====\n",
            "--------------------------------------------------\n",
            "PPL (Perplexity)                   : 7.0464\n",
            "Accuracy                           : 61.0000\n",
            "Memory Footprint (Model Size) (GB) : 5.7480\n",
            "Inference Latency (ms/token)       : 108.9004\n",
            "Avg GPU Utilization (%)            : 57.9231\n",
            "Avg GPU Memory Allocated (GB)      : 5.7913\n",
            "--------------------------------------------------\n",
            "Starting cleanup for FP16 cell...\n",
            "FP16 model deleted.\n",
            "GPU cache cleared and garbage collected after FP16 run.\n"
          ]
        }
      ],
      "source": [
        "# --- FP16 Evaluation Cell ---\n",
        "\n",
        "# Assume 'handle' and 'stop_polling_event' exist from a previous cell.\n",
        "if 'handle' not in locals() or not handle:\n",
        "    print(\"Error: NVML handle not found. Please initialize NVML in a prior cell.\")\n",
        "    handle = None # Prevent polling\n",
        "elif 'stop_polling_event' not in locals():\n",
        "    print(\"Error: stop_polling_event not found. Please initialize in a prior cell.\")\n",
        "    # If needed, define fallback: stop_polling_event = threading.Event()\n",
        "\n",
        "\n",
        "# Clear previous readings and reset event\n",
        "gpu_utilization_readings = []\n",
        "if 'stop_polling_event' in locals():\n",
        "    stop_polling_event.clear()\n",
        "\n",
        "# Start GPU polling thread\n",
        "polling_thread = None\n",
        "if handle:\n",
        "    polling_thread = threading.Thread(target=poll_gpu_utilization, args=(handle, 1.0), daemon=True)\n",
        "    polling_thread.start()\n",
        "\n",
        "# Clean memory before starting\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "fp16_metrics = {}\n",
        "fp16_results_data = {}\n",
        "model_fp16 = None # Define outside try\n",
        "\n",
        "try:\n",
        "    # Load FP16 model\n",
        "    model_name = \"Qwen/Qwen2.5-3B\"\n",
        "    print(f\"Loading model: {model_name} (FP16)...\")\n",
        "    model_fp16 = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        torch_dtype=torch.float16, # Explicitly FP16\n",
        "        device_map=\"cuda:0\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "    print(\"Model loaded.\")\n",
        "\n",
        "    # Load tokenizer\n",
        "    print(\"Loading tokenizer...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    print(\"Tokenizer loaded.\")\n",
        "\n",
        "    # Run evaluation\n",
        "    print(\"Starting MMLU evaluation...\")\n",
        "    fp16_results_data = evaluate_mmlu(model_fp16, tokenizer, samples_200_mmlu_test, max_new_tokens=1)\n",
        "    print(\"MMLU evaluation finished.\")\n",
        "\n",
        "    # Stop polling thread and calculate average utilization\n",
        "    avg_gpu_utilization = 'N/A'\n",
        "    if polling_thread and polling_thread.is_alive():\n",
        "        stop_polling_event.set()\n",
        "        polling_thread.join(timeout=5)\n",
        "        if gpu_utilization_readings:\n",
        "            avg_gpu_utilization = statistics.mean(gpu_utilization_readings)\n",
        "        else:\n",
        "             avg_gpu_utilization = 0 # Polling ran but got no readings\n",
        "    elif handle:\n",
        "        avg_gpu_utilization = 0 # Polling failed to start/run correctly\n",
        "\n",
        "    # Collect metrics\n",
        "    fp16_metrics = {\n",
        "        \"PPL (Perplexity)\": fp16_results_data.get('perplexity', 'N/A'),\n",
        "        \"Accuracy\": fp16_results_data.get('accuracy', 'N/A') * 100,\n",
        "        \"Memory Footprint (Model Size) (GB)\": model_fp16.get_memory_footprint() / (1024 ** 3),\n",
        "        \"Inference Latency (ms/token)\": fp16_results_data.get('inference_latency', 'N/A'),\n",
        "        \"Avg GPU Utilization (%)\": avg_gpu_utilization,\n",
        "        \"Avg GPU Memory Allocated (GB)\": fp16_results_data.get('avg_gpu_memory_gb', 'N/A'),\n",
        "    }\n",
        "\n",
        "    # Print metrics\n",
        "    print(\"\\n===== FP16 DETAILED MODEL METRICS =====\")\n",
        "    print(\"-\" * 50)\n",
        "    if fp16_metrics:\n",
        "        max_key_length = max(len(key) for key in fp16_metrics.keys())\n",
        "        for key, value in fp16_metrics.items():\n",
        "             if isinstance(value, (float, int)) and value != 'N/A':\n",
        "                 print(f\"{key.ljust(max_key_length)} : {value:.4f}\")\n",
        "             else:\n",
        "                 print(f\"{key.ljust(max_key_length)} : {value}\")\n",
        "    else:\n",
        "        print(\"No metrics collected.\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "except pynvml.NVMLError as nvml_error:\n",
        "     print(f\"NVML Error during FP16 evaluation steps: {nvml_error}\")\n",
        "except Exception as e:\n",
        "    print(f\"FP16 Evaluation error: {e}\")\n",
        "    # Ensure polling stops on error\n",
        "    if polling_thread and polling_thread.is_alive():\n",
        "        stop_polling_event.set()\n",
        "        try:\n",
        "            polling_thread.join(timeout=5)\n",
        "        except Exception as join_e:\n",
        "            print(f\"Error stopping polling thread after exception: {join_e}\")\n",
        "\n",
        "finally:\n",
        "    # Clean up GPU memory (No NVML Shutdown)\n",
        "    print(\"Starting cleanup for FP16 cell...\")\n",
        "    if 'model_fp16' in locals() and model_fp16 is not None:\n",
        "        del model_fp16\n",
        "        print(\"FP16 model deleted.\")\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    print(\"GPU cache cleared and garbage collected after FP16 run.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "DV7o-7P0DwrK",
        "outputId": "71f86fb8-0d2e-4122-d074-329317bb61f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model: Qwen/Qwen2.5-3B (NF4 Quantized)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "79c86257816640879881bb256caea9b0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded.\n",
            "Loading tokenizer...\n",
            "Tokenizer loaded.\n",
            "Starting MMLU evaluation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/200 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/bitsandbytes/nn/modules.py:451: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
            "  warnings.warn(\n",
            "100%|██████████| 200/200 [01:22<00:00,  2.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MMLU evaluation finished.\n",
            "\n",
            "===== NF4 DETAILED MODEL METRICS =====\n",
            "--------------------------------------------------\n",
            "PPL (Perplexity)                   : 7.9842\n",
            "Accuracy                           : 58.5000\n",
            "Memory Footprint (Model Size) (GB) : 1.8720\n",
            "Inference Latency (ms/token)       : 412.4234\n",
            "Avg GPU Utilization (%)            : 79.9706\n",
            "Avg GPU Memory Allocated (GB)      : 1.9578\n",
            "--------------------------------------------------\n",
            "Starting cleanup for NF4 cell...\n",
            "NF4 model deleted.\n",
            "GPU cache cleared and garbage collected after NF4 run.\n"
          ]
        }
      ],
      "source": [
        "# --- FP32 to NF4 Evaluation Cell ---\n",
        "\n",
        "# Assume 'handle' and 'stop_polling_event' exist from a previous cell.\n",
        "if 'handle' not in locals() or not handle:\n",
        "    print(\"Error: NVML handle not found. Please initialize NVML in a prior cell.\")\n",
        "    handle = None # Prevent polling\n",
        "elif 'stop_polling_event' not in locals():\n",
        "    print(\"Error: stop_polling_event not found. Please initialize in a prior cell.\")\n",
        "    # If needed, define fallback: stop_polling_event = threading.Event()\n",
        "\n",
        "\n",
        "# Clear previous readings and reset event\n",
        "gpu_utilization_readings = []\n",
        "if 'stop_polling_event' in locals():\n",
        "    stop_polling_event.clear()\n",
        "\n",
        "# Start GPU polling thread\n",
        "polling_thread = None\n",
        "if handle:\n",
        "    polling_thread = threading.Thread(target=poll_gpu_utilization, args=(handle, 1.0), daemon=True)\n",
        "    polling_thread.start()\n",
        "\n",
        "# Clean memory before starting\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "# Configure quantization to NF4\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float32,\n",
        "    bnb_4bit_use_double_quant=True\n",
        ")\n",
        "\n",
        "nf4_metrics = {}\n",
        "nf4_results_data = {}\n",
        "model_quantized = None # Define outside try\n",
        "\n",
        "try:\n",
        "    # Load quantized model\n",
        "    model_name = \"Qwen/Qwen2.5-3B\"\n",
        "    print(f\"Loading model: {model_name} (NF4 Quantized)...\")\n",
        "    model_quantized = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"cuda:0\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "    print(\"Model loaded.\")\n",
        "\n",
        "    # Load tokenizer\n",
        "    print(\"Loading tokenizer...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    print(\"Tokenizer loaded.\")\n",
        "\n",
        "    # Run evaluation\n",
        "    print(\"Starting MMLU evaluation...\")\n",
        "    nf4_results_data = evaluate_mmlu(model_quantized, tokenizer, samples_200_mmlu_test, max_new_tokens=1)\n",
        "    print(\"MMLU evaluation finished.\")\n",
        "\n",
        "    # Stop polling thread and calculate average utilization\n",
        "    avg_gpu_utilization = 'N/A'\n",
        "    if polling_thread and polling_thread.is_alive():\n",
        "        stop_polling_event.set()\n",
        "        polling_thread.join(timeout=5)\n",
        "        if gpu_utilization_readings:\n",
        "            avg_gpu_utilization = statistics.mean(gpu_utilization_readings)\n",
        "        else:\n",
        "             avg_gpu_utilization = 0 # Polling ran but got no readings\n",
        "    elif handle:\n",
        "        avg_gpu_utilization = 0 # Polling failed to start/run correctly\n",
        "\n",
        "    # Collect metrics\n",
        "    nf4_metrics = {\n",
        "        \"PPL (Perplexity)\": nf4_results_data.get('perplexity', 'N/A'),\n",
        "        \"Accuracy\": nf4_results_data.get('accuracy', 'N/A') * 100,\n",
        "        \"Memory Footprint (Model Size) (GB)\": model_quantized.get_memory_footprint() / (1024 ** 3),\n",
        "        \"Inference Latency (ms/token)\": nf4_results_data.get('inference_latency', 'N/A'),\n",
        "        \"Avg GPU Utilization (%)\": avg_gpu_utilization,\n",
        "        \"Avg GPU Memory Allocated (GB)\": nf4_results_data.get('avg_gpu_memory_gb', 'N/A'),\n",
        "    }\n",
        "\n",
        "    # Print metrics\n",
        "    print(\"\\n===== NF4 DETAILED MODEL METRICS =====\")\n",
        "    print(\"-\" * 50)\n",
        "    if nf4_metrics:\n",
        "        max_key_length = max(len(key) for key in nf4_metrics.keys())\n",
        "        for key, value in nf4_metrics.items():\n",
        "             if isinstance(value, (float, int)) and value != 'N/A':\n",
        "                 print(f\"{key.ljust(max_key_length)} : {value:.4f}\")\n",
        "             else:\n",
        "                 print(f\"{key.ljust(max_key_length)} : {value}\")\n",
        "    else:\n",
        "        print(\"No metrics collected.\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "except pynvml.NVMLError as nvml_error:\n",
        "     print(f\"NVML Error during NF4 evaluation steps: {nvml_error}\")\n",
        "except Exception as e:\n",
        "    print(f\"NF4 Evaluation error: {e}\")\n",
        "    # Ensure polling stops on error\n",
        "    if polling_thread and polling_thread.is_alive():\n",
        "        stop_polling_event.set()\n",
        "        try:\n",
        "            polling_thread.join(timeout=5)\n",
        "        except Exception as join_e:\n",
        "            print(f\"Error stopping polling thread after exception: {join_e}\")\n",
        "\n",
        "finally:\n",
        "    # Clean up GPU memory (No NVML Shutdown)\n",
        "    print(\"Starting cleanup for NF4 cell...\")\n",
        "    if 'model_quantized' in locals() and model_quantized is not None:\n",
        "        del model_quantized\n",
        "        print(\"NF4 model deleted.\")\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    print(\"GPU cache cleared and garbage collected after NF4 run.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396
        },
        "id": "qWNd4y1iFsF9",
        "outputId": "699cc0f3-474d-461b-fefb-678996a60310"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model: Qwen/Qwen2.5-3B (NF4 Quantized, FP16 Compute)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "277132f34de04b489f7aede43cd2278b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded.\n",
            "Loading tokenizer...\n",
            "Tokenizer loaded.\n",
            "Starting MMLU evaluation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [00:39<00:00,  5.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MMLU evaluation finished.\n",
            "\n",
            "===== NF4 (FP16 COMPUTE) DETAILED MODEL METRICS =====\n",
            "--------------------------------------------------\n",
            "PPL (Perplexity)                   : 7.9823\n",
            "Accuracy                           : 58.5000\n",
            "Memory Footprint (Model Size) (GB) : 1.8720\n",
            "Inference Latency (ms/token)       : 197.2757\n",
            "Avg GPU Utilization (%)            : 47.0217\n",
            "Avg GPU Memory Allocated (GB)      : 1.9578\n",
            "--------------------------------------------------\n",
            "Starting cleanup for NF4 (FP16 Compute) cell...\n",
            "NF4 (FP16 compute) model deleted.\n",
            "GPU cache cleared and garbage collected after NF4 (FP16 Compute) run.\n"
          ]
        }
      ],
      "source": [
        "# --- NF4 (FP16 Compute) Evaluation Cell ---\n",
        "\n",
        "# Assume 'handle' and 'stop_polling_event' exist from a previous cell.\n",
        "if 'handle' not in locals() or not handle:\n",
        "    print(\"Error: NVML handle not found. Please initialize NVML in a prior cell.\")\n",
        "    handle = None # Prevent polling\n",
        "elif 'stop_polling_event' not in locals():\n",
        "    print(\"Error: stop_polling_event not found. Please initialize in a prior cell.\")\n",
        "    # If needed, define fallback: stop_polling_event = threading.Event()\n",
        "\n",
        "# Clear previous readings and reset event\n",
        "gpu_utilization_readings = []\n",
        "if 'stop_polling_event' in locals():\n",
        "    stop_polling_event.clear()\n",
        "\n",
        "# Start GPU polling thread\n",
        "polling_thread = None\n",
        "if handle:\n",
        "    polling_thread = threading.Thread(target=poll_gpu_utilization, args=(handle, 1.0), daemon=True)\n",
        "    polling_thread.start()\n",
        "\n",
        "# Clean memory before starting\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "# Configure quantization to NF4 with FP16 compute dtype\n",
        "bnb_config_fp16_compute = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16, # Use FP16 for computation\n",
        "    bnb_4bit_use_double_quant=True\n",
        ")\n",
        "\n",
        "nf4_fp16_compute_metrics = {}\n",
        "nf4_fp16_results_data = {}\n",
        "model_quantized_fp16_compute = None # Define outside try\n",
        "\n",
        "try:\n",
        "    # Load the quantized model with the FP16 compute config\n",
        "    model_name = \"Qwen/Qwen2.5-3B\"\n",
        "    print(f\"Loading model: {model_name} (NF4 Quantized, FP16 Compute)...\")\n",
        "    model_quantized_fp16_compute = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        quantization_config=bnb_config_fp16_compute, # Use the specific config\n",
        "        device_map=\"cuda:0\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "    print(\"Model loaded.\")\n",
        "\n",
        "    # Load tokenizer\n",
        "    print(\"Loading tokenizer...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    print(\"Tokenizer loaded.\")\n",
        "\n",
        "    # Run evaluation\n",
        "    print(\"Starting MMLU evaluation...\")\n",
        "    nf4_fp16_results_data = evaluate_mmlu(model_quantized_fp16_compute, tokenizer, samples_200_mmlu_test, max_new_tokens=1)\n",
        "    print(\"MMLU evaluation finished.\")\n",
        "\n",
        "    # Stop polling thread and calculate average utilization\n",
        "    avg_gpu_utilization = 'N/A'\n",
        "    if polling_thread and polling_thread.is_alive():\n",
        "        stop_polling_event.set()\n",
        "        polling_thread.join(timeout=5)\n",
        "        if gpu_utilization_readings:\n",
        "            avg_gpu_utilization = statistics.mean(gpu_utilization_readings)\n",
        "        else:\n",
        "             avg_gpu_utilization = 0 # Polling ran but got no readings\n",
        "    elif handle:\n",
        "        avg_gpu_utilization = 0 # Polling failed to start/run correctly\n",
        "\n",
        "    # Collect metrics\n",
        "    nf4_fp16_compute_metrics = {\n",
        "        \"PPL (Perplexity)\": nf4_fp16_results_data.get('perplexity', 'N/A'),\n",
        "        \"Accuracy\": nf4_fp16_results_data.get('accuracy', 'N/A') * 100,\n",
        "        \"Memory Footprint (Model Size) (GB)\": model_quantized_fp16_compute.get_memory_footprint() / (1024 ** 3),\n",
        "        \"Inference Latency (ms/token)\": nf4_fp16_results_data.get('inference_latency', 'N/A'),\n",
        "        \"Avg GPU Utilization (%)\": avg_gpu_utilization,\n",
        "        \"Avg GPU Memory Allocated (GB)\": nf4_fp16_results_data.get('avg_gpu_memory_gb', 'N/A'),\n",
        "    }\n",
        "\n",
        "    # Print metrics\n",
        "    print(\"\\n===== NF4 (FP16 COMPUTE) DETAILED MODEL METRICS =====\")\n",
        "    print(\"-\" * 50)\n",
        "    if nf4_fp16_compute_metrics:\n",
        "        max_key_length = max(len(key) for key in nf4_fp16_compute_metrics.keys())\n",
        "        for key, value in nf4_fp16_compute_metrics.items():\n",
        "             if isinstance(value, (float, int)) and value != 'N/A':\n",
        "                 print(f\"{key.ljust(max_key_length)} : {value:.4f}\")\n",
        "             else:\n",
        "                 print(f\"{key.ljust(max_key_length)} : {value}\")\n",
        "    else:\n",
        "        print(\"No metrics collected.\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "except pynvml.NVMLError as nvml_error:\n",
        "     print(f\"NVML Error during NF4 (FP16 Compute) evaluation steps: {nvml_error}\")\n",
        "except Exception as e:\n",
        "    print(f\"NF4 (FP16 Compute) Evaluation error: {e}\")\n",
        "    # Ensure polling stops on error\n",
        "    if polling_thread and polling_thread.is_alive():\n",
        "        stop_polling_event.set()\n",
        "        try:\n",
        "            polling_thread.join(timeout=5)\n",
        "        except Exception as join_e:\n",
        "            print(f\"Error stopping polling thread after exception: {join_e}\")\n",
        "\n",
        "finally:\n",
        "    # Clean up GPU memory (No NVML Shutdown)\n",
        "    print(\"Starting cleanup for NF4 (FP16 Compute) cell...\")\n",
        "    if 'model_quantized_fp16_compute' in locals() and model_quantized_fp16_compute is not None:\n",
        "        del model_quantized_fp16_compute\n",
        "        print(\"NF4 (FP16 compute) model deleted.\")\n",
        "    # Removed cleanup for 'model_fp16_temp' as it's no longer loaded\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    print(\"GPU cache cleared and garbage collected after NF4 (FP16 Compute) run.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396
        },
        "id": "8_tvYPmiHk55",
        "outputId": "76615191-856e-4c32-f1f4-235677b4a9d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model: Qwen/Qwen2.5-3B (FP4 Quantized, FP32 Compute)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8ea7d0d70c60423188d46fc35cf00bb5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded.\n",
            "Loading tokenizer...\n",
            "Tokenizer loaded.\n",
            "Starting MMLU evaluation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:20<00:00,  2.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MMLU evaluation finished.\n",
            "\n",
            "===== FP4 (FP32 COMPUTE) DETAILED MODEL METRICS =====\n",
            "--------------------------------------------------\n",
            "PPL (Perplexity)                   : 9.2376\n",
            "Accuracy                           : 50.5000\n",
            "Memory Footprint (Model Size) (GB) : 1.8720\n",
            "Inference Latency (ms/token)       : 404.7842\n",
            "Avg GPU Utilization (%)            : 88.9111\n",
            "Avg GPU Memory Allocated (GB)      : 1.9578\n",
            "--------------------------------------------------\n",
            "Starting cleanup for FP4 (FP32 Compute) cell...\n",
            "FP4 (FP32 Compute) model deleted.\n",
            "GPU cache cleared and garbage collected after FP4 (FP32 Compute) run.\n"
          ]
        }
      ],
      "source": [
        "# --- FP4 (FP32 Compute) Evaluation Cell ---\n",
        "\n",
        "# Assume 'handle' and 'stop_polling_event' exist from a previous cell.\n",
        "if 'handle' not in locals() or not handle:\n",
        "    print(\"Error: NVML handle not found. Please initialize NVML in a prior cell.\")\n",
        "    handle = None # Prevent polling\n",
        "elif 'stop_polling_event' not in locals():\n",
        "    print(\"Error: stop_polling_event not found. Please initialize in a prior cell.\")\n",
        "    # If needed, define fallback: stop_polling_event = threading.Event()\n",
        "\n",
        "# Clear previous readings and reset event\n",
        "gpu_utilization_readings = []\n",
        "if 'stop_polling_event' in locals():\n",
        "    stop_polling_event.clear()\n",
        "\n",
        "# Start GPU polling thread\n",
        "polling_thread = None\n",
        "if handle:\n",
        "    polling_thread = threading.Thread(target=poll_gpu_utilization, args=(handle, 1.0), daemon=True)\n",
        "    polling_thread.start()\n",
        "\n",
        "# Clean memory before starting\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "# Configure quantization to FP4 with FP32 compute dtype\n",
        "bnb_config_fp4 = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"fp4\",           # Quantization type is FP4\n",
        "    bnb_4bit_compute_dtype=torch.float32,# Compute type is FP32\n",
        "    bnb_4bit_use_double_quant=True\n",
        ")\n",
        "\n",
        "fp4_metrics = {}\n",
        "fp4_results_data = {}\n",
        "model_quantized_fp4 = None # Define outside try\n",
        "\n",
        "try:\n",
        "    # Load the quantized model with the FP4 config\n",
        "    model_name = \"Qwen/Qwen2.5-3B\"\n",
        "    print(f\"Loading model: {model_name} (FP4 Quantized, FP32 Compute)...\")\n",
        "    model_quantized_fp4 = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        quantization_config=bnb_config_fp4, # Use the FP4 config\n",
        "        device_map=\"cuda:0\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "    print(\"Model loaded.\")\n",
        "\n",
        "    # Load tokenizer\n",
        "    print(\"Loading tokenizer...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    print(\"Tokenizer loaded.\")\n",
        "\n",
        "    # Run evaluation\n",
        "    print(\"Starting MMLU evaluation...\")\n",
        "    fp4_results_data = evaluate_mmlu(model_quantized_fp4, tokenizer, samples_200_mmlu_test, max_new_tokens=1)\n",
        "    print(\"MMLU evaluation finished.\")\n",
        "\n",
        "    # Stop polling thread and calculate average utilization\n",
        "    avg_gpu_utilization = 'N/A'\n",
        "    if polling_thread and polling_thread.is_alive():\n",
        "        stop_polling_event.set()\n",
        "        polling_thread.join(timeout=5)\n",
        "        if gpu_utilization_readings:\n",
        "            avg_gpu_utilization = statistics.mean(gpu_utilization_readings)\n",
        "        else:\n",
        "             avg_gpu_utilization = 0 # Polling ran but got no readings\n",
        "    elif handle:\n",
        "        avg_gpu_utilization = 0 # Polling failed to start/run correctly\n",
        "\n",
        "    # Collect metrics\n",
        "    fp4_metrics = {\n",
        "        \"PPL (Perplexity)\": fp4_results_data.get('perplexity', 'N/A'),\n",
        "        \"Accuracy\": fp4_results_data.get('accuracy', 'N/A') * 100,\n",
        "        \"Memory Footprint (Model Size) (GB)\": model_quantized_fp4.get_memory_footprint() / (1024 ** 3),\n",
        "        \"Inference Latency (ms/token)\": fp4_results_data.get('inference_latency', 'N/A'),\n",
        "        \"Avg GPU Utilization (%)\": avg_gpu_utilization,\n",
        "        \"Avg GPU Memory Allocated (GB)\": fp4_results_data.get('avg_gpu_memory_gb', 'N/A'),\n",
        "    }\n",
        "\n",
        "    # Print metrics\n",
        "    print(\"\\n===== FP4 (FP32 COMPUTE) DETAILED MODEL METRICS =====\")\n",
        "    print(\"-\" * 50)\n",
        "    if fp4_metrics:\n",
        "        max_key_length = max(len(key) for key in fp4_metrics.keys())\n",
        "        for key, value in fp4_metrics.items():\n",
        "             if isinstance(value, (float, int)) and value != 'N/A':\n",
        "                 print(f\"{key.ljust(max_key_length)} : {value:.4f}\")\n",
        "             else:\n",
        "                 print(f\"{key.ljust(max_key_length)} : {value}\")\n",
        "    else:\n",
        "        print(\"No metrics collected.\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "except pynvml.NVMLError as nvml_error:\n",
        "     print(f\"NVML Error during FP4 (FP32 Compute) evaluation steps: {nvml_error}\")\n",
        "except Exception as e:\n",
        "    print(f\"FP4 (FP32 Compute) Evaluation error: {e}\")\n",
        "    # Ensure polling stops on error\n",
        "    if polling_thread and polling_thread.is_alive():\n",
        "        stop_polling_event.set()\n",
        "        try:\n",
        "            polling_thread.join(timeout=5)\n",
        "        except Exception as join_e:\n",
        "            print(f\"Error stopping polling thread after exception: {join_e}\")\n",
        "\n",
        "finally:\n",
        "    # Clean up GPU memory (No NVML Shutdown)\n",
        "    print(\"Starting cleanup for FP4 (FP32 Compute) cell...\")\n",
        "    if 'model_quantized_fp4' in locals() and model_quantized_fp4 is not None:\n",
        "        del model_quantized_fp4\n",
        "        print(\"FP4 (FP32 Compute) model deleted.\")\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    print(\"GPU cache cleared and garbage collected after FP4 (FP32 Compute) run.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396
        },
        "id": "yjN1xRiOIUp8",
        "outputId": "409bc516-3e5c-40e7-a166-55105d928cc7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model: Qwen/Qwen2.5-3B (FP4 Quantized, FP16 Compute)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "024faf88752d422eba3e9aa94b2ec950"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded.\n",
            "Loading tokenizer...\n",
            "Tokenizer loaded.\n",
            "Starting MMLU evaluation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [00:39<00:00,  5.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MMLU evaluation finished.\n",
            "\n",
            "===== FP4 (FP16 COMPUTE) DETAILED MODEL METRICS =====\n",
            "--------------------------------------------------\n",
            "PPL (Perplexity)                   : 9.2318\n",
            "Accuracy                           : 51.0000\n",
            "Memory Footprint (Model Size) (GB) : 1.8720\n",
            "Inference Latency (ms/token)       : 197.4802\n",
            "Avg GPU Utilization (%)            : 45.5652\n",
            "Avg GPU Memory Allocated (GB)      : 1.9578\n",
            "--------------------------------------------------\n",
            "Starting cleanup for FP4 (FP16 Compute) cell...\n",
            "FP4 (FP16 Compute) model deleted.\n",
            "GPU cache cleared and garbage collected after FP4 (FP16 Compute) run.\n"
          ]
        }
      ],
      "source": [
        "# --- FP4 (FP16 Compute) Evaluation Cell ---\n",
        "\n",
        "# Assume 'handle' and 'stop_polling_event' exist from a previous cell.\n",
        "if 'handle' not in locals() or not handle:\n",
        "    print(\"Error: NVML handle not found. Please initialize NVML in a prior cell.\")\n",
        "    handle = None # Prevent polling\n",
        "elif 'stop_polling_event' not in locals():\n",
        "    print(\"Error: stop_polling_event not found. Please initialize in a prior cell.\")\n",
        "    # If needed, define fallback: stop_polling_event = threading.Event()\n",
        "\n",
        "# Clear previous readings and reset event\n",
        "gpu_utilization_readings = []\n",
        "if 'stop_polling_event' in locals():\n",
        "    stop_polling_event.clear()\n",
        "\n",
        "# Start GPU polling thread\n",
        "polling_thread = None\n",
        "if handle:\n",
        "    polling_thread = threading.Thread(target=poll_gpu_utilization, args=(handle, 1.0), daemon=True)\n",
        "    polling_thread.start()\n",
        "\n",
        "# Clean memory before starting\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "# Configure quantization to FP4 with FP16 compute dtype\n",
        "bnb_config_fp4_fp16 = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"fp4\",           # Quantization type is FP4\n",
        "    bnb_4bit_compute_dtype=torch.float16,# Compute type is FP16\n",
        "    bnb_4bit_use_double_quant=True\n",
        ")\n",
        "\n",
        "fp4_fp16_metrics = {}\n",
        "fp4_fp16_results_data = {}\n",
        "model_quantized_fp4_fp16 = None # Define outside try\n",
        "\n",
        "try:\n",
        "    # Load the quantized model with the FP4 (FP16 compute) config\n",
        "    model_name = \"Qwen/Qwen2.5-3B\"\n",
        "    print(f\"Loading model: {model_name} (FP4 Quantized, FP16 Compute)...\")\n",
        "    model_quantized_fp4_fp16 = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        quantization_config=bnb_config_fp4_fp16, # Use the FP4/FP16 config\n",
        "        device_map=\"cuda:0\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "    print(\"Model loaded.\")\n",
        "\n",
        "    # Load tokenizer\n",
        "    print(\"Loading tokenizer...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    print(\"Tokenizer loaded.\")\n",
        "\n",
        "    # Run evaluation\n",
        "    print(\"Starting MMLU evaluation...\")\n",
        "    fp4_fp16_results_data = evaluate_mmlu(model_quantized_fp4_fp16, tokenizer, samples_200_mmlu_test, max_new_tokens=1)\n",
        "    print(\"MMLU evaluation finished.\")\n",
        "\n",
        "    # Stop polling thread and calculate average utilization\n",
        "    avg_gpu_utilization = 'N/A'\n",
        "    if polling_thread and polling_thread.is_alive():\n",
        "        stop_polling_event.set()\n",
        "        polling_thread.join(timeout=5)\n",
        "        if gpu_utilization_readings:\n",
        "            avg_gpu_utilization = statistics.mean(gpu_utilization_readings)\n",
        "        else:\n",
        "             avg_gpu_utilization = 0 # Polling ran but got no readings\n",
        "    elif handle:\n",
        "        avg_gpu_utilization = 0 # Polling failed to start/run correctly\n",
        "\n",
        "    # Collect metrics\n",
        "    fp4_fp16_metrics = {\n",
        "        \"PPL (Perplexity)\": fp4_fp16_results_data.get('perplexity', 'N/A'),\n",
        "        \"Accuracy\": fp4_fp16_results_data.get('accuracy', 'N/A') * 100,\n",
        "        \"Memory Footprint (Model Size) (GB)\": model_quantized_fp4_fp16.get_memory_footprint() / (1024 ** 3),\n",
        "        \"Inference Latency (ms/token)\": fp4_fp16_results_data.get('inference_latency', 'N/A'),\n",
        "        \"Avg GPU Utilization (%)\": avg_gpu_utilization,\n",
        "        \"Avg GPU Memory Allocated (GB)\": fp4_fp16_results_data.get('avg_gpu_memory_gb', 'N/A'),\n",
        "    }\n",
        "\n",
        "    # Print metrics\n",
        "    print(\"\\n===== FP4 (FP16 COMPUTE) DETAILED MODEL METRICS =====\")\n",
        "    print(\"-\" * 50)\n",
        "    if fp4_fp16_metrics:\n",
        "        max_key_length = max(len(key) for key in fp4_fp16_metrics.keys())\n",
        "        for key, value in fp4_fp16_metrics.items():\n",
        "             if isinstance(value, (float, int)) and value != 'N/A':\n",
        "                 print(f\"{key.ljust(max_key_length)} : {value:.4f}\")\n",
        "             else:\n",
        "                 print(f\"{key.ljust(max_key_length)} : {value}\")\n",
        "    else:\n",
        "        print(\"No metrics collected.\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "except pynvml.NVMLError as nvml_error:\n",
        "     print(f\"NVML Error during FP4 (FP16 Compute) evaluation steps: {nvml_error}\")\n",
        "except Exception as e:\n",
        "    print(f\"FP4 (FP16 Compute) Evaluation error: {e}\")\n",
        "    # Ensure polling stops on error\n",
        "    if polling_thread and polling_thread.is_alive():\n",
        "        stop_polling_event.set()\n",
        "        try:\n",
        "            polling_thread.join(timeout=5)\n",
        "        except Exception as join_e:\n",
        "            print(f\"Error stopping polling thread after exception: {join_e}\")\n",
        "\n",
        "finally:\n",
        "    # Clean up GPU memory (No NVML Shutdown)\n",
        "    print(\"Starting cleanup for FP4 (FP16 Compute) cell...\")\n",
        "    if 'model_quantized_fp4_fp16' in locals() and model_quantized_fp4_fp16 is not None:\n",
        "        del model_quantized_fp4_fp16\n",
        "        print(\"FP4 (FP16 Compute) model deleted.\")\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    print(\"GPU cache cleared and garbage collected after FP4 (FP16 Compute) run.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396
        },
        "id": "E-xxQfjeIpUu",
        "outputId": "d1dd307c-b8da-44fa-a602-964f565a5bf3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model: Qwen/Qwen2.5-3B (INT8 Quantized)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d21695edcf48406b9366f434debfc434"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded.\n",
            "Loading tokenizer...\n",
            "Tokenizer loaded.\n",
            "Starting MMLU evaluation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:21<00:00,  2.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MMLU evaluation finished.\n",
            "\n",
            "===== INT8 DETAILED MODEL METRICS =====\n",
            "--------------------------------------------------\n",
            "PPL (Perplexity)                   : 7.0461\n",
            "Accuracy                           : 60.0000\n",
            "Memory Footprint (Model Size) (GB) : 3.1640\n",
            "Inference Latency (ms/token)       : 409.0462\n",
            "Avg GPU Utilization (%)            : 20.2667\n",
            "Avg GPU Memory Allocated (GB)      : 3.2600\n",
            "--------------------------------------------------\n",
            "Starting cleanup for INT8 cell...\n",
            "INT8 model deleted.\n",
            "GPU cache cleared and garbage collected after INT8 run.\n"
          ]
        }
      ],
      "source": [
        "# --- INT8 (FP32 Compute) Evaluation Cell ---\n",
        "\n",
        "# Assume 'handle' and 'stop_polling_event' exist from a previous cell.\n",
        "if 'handle' not in locals() or not handle:\n",
        "    print(\"Error: NVML handle not found. Please initialize NVML in a prior cell.\")\n",
        "    handle = None # Prevent polling\n",
        "elif 'stop_polling_event' not in locals():\n",
        "    print(\"Error: stop_polling_event not found. Please initialize in a prior cell.\")\n",
        "    # If needed, define fallback: stop_polling_event = threading.Event()\n",
        "\n",
        "# Clear previous readings and reset event\n",
        "gpu_utilization_readings = []\n",
        "if 'stop_polling_event' in locals():\n",
        "    stop_polling_event.clear()\n",
        "\n",
        "# Start GPU polling thread\n",
        "polling_thread = None\n",
        "if handle:\n",
        "    polling_thread = threading.Thread(target=poll_gpu_utilization, args=(handle, 1.0), daemon=True)\n",
        "    polling_thread.start()\n",
        "\n",
        "# Clean memory before starting\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "# Configure quantization to INT8 using load_in_8bit\n",
        "bnb_config_int8 = BitsAndBytesConfig(\n",
        "    load_in_8bit=True\n",
        ")\n",
        "\n",
        "int8_metrics = {}\n",
        "int8_results_data = {}\n",
        "model_quantized_int8 = None # Define outside try\n",
        "\n",
        "try:\n",
        "    # Load the INT8 quantized model\n",
        "    model_name = \"Qwen/Qwen2.5-3B\"\n",
        "    print(f\"Loading model: {model_name} (INT8 Quantized)...\")\n",
        "    model_quantized_int8 = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        quantization_config=bnb_config_int8, # Use the INT8 config\n",
        "        device_map=\"cuda:0\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "    print(\"Model loaded.\")\n",
        "\n",
        "    # Load tokenizer\n",
        "    print(\"Loading tokenizer...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    print(\"Tokenizer loaded.\")\n",
        "\n",
        "    # Run evaluation\n",
        "    print(\"Starting MMLU evaluation...\")\n",
        "    int8_results_data = evaluate_mmlu(model_quantized_int8, tokenizer, samples_200_mmlu_test, max_new_tokens=1)\n",
        "    print(\"MMLU evaluation finished.\")\n",
        "\n",
        "    # Stop polling thread and calculate average utilization\n",
        "    avg_gpu_utilization = 'N/A'\n",
        "    if polling_thread and polling_thread.is_alive():\n",
        "        stop_polling_event.set()\n",
        "        polling_thread.join(timeout=5)\n",
        "        if gpu_utilization_readings:\n",
        "            avg_gpu_utilization = statistics.mean(gpu_utilization_readings)\n",
        "        else:\n",
        "             avg_gpu_utilization = 0 # Polling ran but got no readings\n",
        "    elif handle:\n",
        "        avg_gpu_utilization = 0 # Polling failed to start/run correctly\n",
        "\n",
        "    # Collect metrics\n",
        "    int8_metrics = {\n",
        "        \"PPL (Perplexity)\": int8_results_data.get('perplexity', 'N/A'),\n",
        "        \"Accuracy\": int8_results_data.get('accuracy', 'N/A') * 100,\n",
        "        \"Memory Footprint (Model Size) (GB)\": model_quantized_int8.get_memory_footprint() / (1024 ** 3),\n",
        "        \"Inference Latency (ms/token)\": int8_results_data.get('inference_latency', 'N/A'),\n",
        "        \"Avg GPU Utilization (%)\": avg_gpu_utilization,\n",
        "        \"Avg GPU Memory Allocated (GB)\": int8_results_data.get('avg_gpu_memory_gb', 'N/A'),\n",
        "    }\n",
        "\n",
        "    # Print metrics\n",
        "    print(\"\\n===== INT8 DETAILED MODEL METRICS =====\")\n",
        "    print(\"-\" * 50)\n",
        "    if int8_metrics:\n",
        "        max_key_length = max(len(key) for key in int8_metrics.keys())\n",
        "        for key, value in int8_metrics.items():\n",
        "             if isinstance(value, (float, int)) and value != 'N/A':\n",
        "                 print(f\"{key.ljust(max_key_length)} : {value:.4f}\")\n",
        "             else:\n",
        "                 print(f\"{key.ljust(max_key_length)} : {value}\")\n",
        "    else:\n",
        "        print(\"No metrics collected.\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "except pynvml.NVMLError as nvml_error:\n",
        "     print(f\"NVML Error during INT8 evaluation steps: {nvml_error}\")\n",
        "except Exception as e:\n",
        "    print(f\"INT8 Evaluation error: {e}\")\n",
        "    # Ensure polling stops on error\n",
        "    if polling_thread and polling_thread.is_alive():\n",
        "        stop_polling_event.set()\n",
        "        try:\n",
        "            polling_thread.join(timeout=5)\n",
        "        except Exception as join_e:\n",
        "            print(f\"Error stopping polling thread after exception: {join_e}\")\n",
        "\n",
        "finally:\n",
        "    # Clean up GPU memory (No NVML Shutdown)\n",
        "    print(\"Starting cleanup for INT8 cell...\")\n",
        "    if 'model_quantized_int8' in locals() and model_quantized_int8 is not None:\n",
        "        del model_quantized_int8\n",
        "        print(\"INT8 model deleted.\")\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    print(\"GPU cache cleared and garbage collected after INT8 run.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396
        },
        "id": "wRxAfjR9I2S5",
        "outputId": "4c5ad226-e4c3-48d9-9a4b-b545d68c9cd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model: Qwen/Qwen2.5-3B (INT8 Quantized, FP16 Compute)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cc34b38898ad4929927f006aaea705be"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded.\n",
            "Loading tokenizer...\n",
            "Tokenizer loaded.\n",
            "Starting MMLU evaluation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:21<00:00,  2.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MMLU evaluation finished.\n",
            "\n",
            "===== INT8 (FP16 COMPUTE) DETAILED MODEL METRICS =====\n",
            "--------------------------------------------------\n",
            "PPL (Perplexity)                   : 7.0461\n",
            "Accuracy                           : 60.0000\n",
            "Memory Footprint (Model Size) (GB) : 3.1640\n",
            "Inference Latency (ms/token)       : 409.0026\n",
            "Avg GPU Utilization (%)            : 20.3636\n",
            "Avg GPU Memory Allocated (GB)      : 3.2600\n",
            "--------------------------------------------------\n",
            "Starting cleanup for INT8 (FP16 Compute) cell...\n",
            "INT8 (FP16 compute) model deleted.\n",
            "GPU cache cleared and garbage collected after INT8 (FP16 Compute) run.\n"
          ]
        }
      ],
      "source": [
        "# --- INT8 (FP16 Compute) Evaluation Cell ---\n",
        "\n",
        "# Assume 'handle' and 'stop_polling_event' exist from a previous cell.\n",
        "if 'handle' not in locals() or not handle:\n",
        "    print(\"Error: NVML handle not found. Please initialize NVML in a prior cell.\")\n",
        "    handle = None # Prevent polling\n",
        "elif 'stop_polling_event' not in locals():\n",
        "    print(\"Error: stop_polling_event not found. Please initialize in a prior cell.\")\n",
        "    # If needed, define fallback: stop_polling_event = threading.Event()\n",
        "\n",
        "# Clear previous readings and reset event\n",
        "gpu_utilization_readings = []\n",
        "if 'stop_polling_event' in locals():\n",
        "    stop_polling_event.clear()\n",
        "\n",
        "# Start GPU polling thread\n",
        "polling_thread = None\n",
        "if handle:\n",
        "    polling_thread = threading.Thread(target=poll_gpu_utilization, args=(handle, 1.0), daemon=True)\n",
        "    polling_thread.start()\n",
        "\n",
        "# Clean memory before starting\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "# Configure quantization to INT8, explicitly requesting FP16 compute dtype\n",
        "bnb_config_int8_fp16 = BitsAndBytesConfig(\n",
        "    load_in_8bit=True,\n",
        "    bnb_8bit_compute_dtype=torch.float16 # Explicitly requested compute dtype\n",
        ")\n",
        "\n",
        "int8_fp16_metrics = {}\n",
        "int8_fp16_results_data = {}\n",
        "model_quantized_int8_fp16 = None # Define outside try\n",
        "\n",
        "try:\n",
        "    # Load the INT8 quantized model with specified compute config\n",
        "    model_name = \"Qwen/Qwen2.5-3B\"\n",
        "    print(f\"Loading model: {model_name} (INT8 Quantized, FP16 Compute)...\")\n",
        "    model_quantized_int8_fp16 = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        quantization_config=bnb_config_int8_fp16, # Use the INT8/FP16 config\n",
        "        device_map=\"cuda:0\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "    print(\"Model loaded.\")\n",
        "\n",
        "    # Load tokenizer\n",
        "    print(\"Loading tokenizer...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    print(\"Tokenizer loaded.\")\n",
        "\n",
        "    # Run evaluation\n",
        "    print(\"Starting MMLU evaluation...\")\n",
        "    int8_fp16_results_data = evaluate_mmlu(model_quantized_int8_fp16, tokenizer, samples_200_mmlu_test, max_new_tokens=1)\n",
        "    print(\"MMLU evaluation finished.\")\n",
        "\n",
        "    # Stop polling thread and calculate average utilization\n",
        "    avg_gpu_utilization = 'N/A'\n",
        "    if polling_thread and polling_thread.is_alive():\n",
        "        stop_polling_event.set()\n",
        "        polling_thread.join(timeout=5)\n",
        "        if gpu_utilization_readings:\n",
        "            avg_gpu_utilization = statistics.mean(gpu_utilization_readings)\n",
        "        else:\n",
        "             avg_gpu_utilization = 0 # Polling ran but got no readings\n",
        "    elif handle:\n",
        "        avg_gpu_utilization = 0 # Polling failed to start/run correctly\n",
        "\n",
        "    # Collect metrics\n",
        "    int8_fp16_metrics = {\n",
        "        \"PPL (Perplexity)\": int8_fp16_results_data.get('perplexity', 'N/A'),\n",
        "        \"Accuracy\": int8_fp16_results_data.get('accuracy', 'N/A') * 100,\n",
        "        \"Memory Footprint (Model Size) (GB)\": model_quantized_int8_fp16.get_memory_footprint() / (1024 ** 3),\n",
        "        \"Inference Latency (ms/token)\": int8_fp16_results_data.get('inference_latency', 'N/A'),\n",
        "        \"Avg GPU Utilization (%)\": avg_gpu_utilization,\n",
        "        \"Avg GPU Memory Allocated (GB)\": int8_fp16_results_data.get('avg_gpu_memory_gb', 'N/A'),\n",
        "    }\n",
        "\n",
        "    # Print metrics\n",
        "    print(\"\\n===== INT8 (FP16 COMPUTE) DETAILED MODEL METRICS =====\")\n",
        "    print(\"-\" * 50)\n",
        "    if int8_fp16_metrics:\n",
        "        max_key_length = max(len(key) for key in int8_fp16_metrics.keys())\n",
        "        for key, value in int8_fp16_metrics.items():\n",
        "             if isinstance(value, (float, int)) and value != 'N/A':\n",
        "                 print(f\"{key.ljust(max_key_length)} : {value:.4f}\")\n",
        "             else:\n",
        "                 print(f\"{key.ljust(max_key_length)} : {value}\")\n",
        "    else:\n",
        "        print(\"No metrics collected.\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "except pynvml.NVMLError as nvml_error:\n",
        "     print(f\"NVML Error during INT8 (FP16 Compute) evaluation steps: {nvml_error}\")\n",
        "except Exception as e:\n",
        "    print(f\"INT8 (FP16 Compute) Evaluation error: {e}\")\n",
        "    # Ensure polling stops on error\n",
        "    if polling_thread and polling_thread.is_alive():\n",
        "        stop_polling_event.set()\n",
        "        try:\n",
        "            polling_thread.join(timeout=5)\n",
        "        except Exception as join_e:\n",
        "            print(f\"Error stopping polling thread after exception: {join_e}\")\n",
        "\n",
        "finally:\n",
        "    # Clean up GPU memory (No NVML Shutdown)\n",
        "    print(\"Starting cleanup for INT8 (FP16 Compute) cell...\")\n",
        "    if 'model_quantized_int8_fp16' in locals() and model_quantized_int8_fp16 is not None:\n",
        "        del model_quantized_int8_fp16\n",
        "        print(\"INT8 (FP16 compute) model deleted.\")\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    print(\"GPU cache cleared and garbage collected after INT8 (FP16 Compute) run.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wZcv8a14qBW"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1MAoPKVKmYE"
      },
      "source": [
        "## ARC_easy Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "collapsed": true,
        "id": "CZI5M5rJMhcN",
        "outputId": "5c3b90bd-df0b-414a-f76c-70d7c525fcf1"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/9.00k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "313a1d1db83d4489ae8e5fab14a4efd2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00000-of-00001.parquet:   0%|          | 0.00/331k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "be10b4f254b5420c96e7126cefefda74"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "test-00000-of-00001.parquet:   0%|          | 0.00/346k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c084414073644333933bd3bdfd8d79c2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "validation-00000-of-00001.parquet:   0%|          | 0.00/86.1k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3bd919c75114460d960f782f3e86c137"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/2251 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4ed2d8f8a3d44881a475b77f5baefda1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/2376 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a1706107dc1b4a6a8d8b9f59f6a1e78d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating validation split:   0%|          | 0/570 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8585c7c93977432d985c3439ca25ef8b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------\n",
            "Test set size: 2376\n",
            "----------------------\n",
            "Dataset({\n",
            "    features: ['id', 'question', 'choices', 'answerKey'],\n",
            "    num_rows: 2376\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "# Load ARC test dataset\n",
        "arc_test = load_dataset(\"allenai/ai2_arc\", \"ARC-Easy\", split=\"test\")\n",
        "\n",
        "print('----------------------')\n",
        "print(f\"Test set size: {len(arc_test)}\")\n",
        "print('----------------------')\n",
        "print(arc_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W3Y0dncHLASu"
      },
      "outputs": [],
      "source": [
        "# Set a seed for reproducibility\n",
        "random.seed(42)\n",
        "\n",
        "# Take a random sample of 200 from the test dataset\n",
        "samples_200_arc_test = random.sample(list(arc_test), 200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hrl4XA1bTNgn"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import torch\n",
        "import statistics # Needed for mean calculation\n",
        "from tqdm.auto import tqdm # Use auto version for better notebook compatibility\n",
        "\n",
        "def evaluate_arc(model, tokenizer, dataset, max_new_tokens=1):\n",
        "    \"\"\"\n",
        "    Comprehensive evaluation function for ARC dataset\n",
        "    Aligned with MMLU evaluation approach, including average memory tracking.\n",
        "    \"\"\"\n",
        "    # Performance tracking\n",
        "    start_time = time.time()\n",
        "    total_tokens_processed = 0\n",
        "    results = {}\n",
        "    gpu_memory_readings_gb = [] # <<< Initialize list to store memory readings\n",
        "\n",
        "    # Accuracy tracking\n",
        "    correct = 0\n",
        "    total_perplexity = 0\n",
        "    perplexity_count = 0\n",
        "    choice_letters = [\"A\", \"B\", \"C\", \"D\"] # Keep for reference if needed later\n",
        "\n",
        "    # Ensure model is on GPU if available for memory tracking\n",
        "    device = model.device\n",
        "    if not str(device).startswith(\"cuda\"):\n",
        "        print(\"Warning: Model is not on CUDA device. GPU memory metrics will be 0.\")\n",
        "\n",
        "    # Reset peak memory stats *before* the loop if you want peak for this specific eval\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.reset_peak_memory_stats(device=device)\n",
        "\n",
        "    for sample in tqdm(dataset, desc=\"Evaluating ARC\"):\n",
        "        # Extract subject/category (using 'id' as category source for ARC)\n",
        "        # ARC doesn't have formal subjects like MMLU, using the first part of ID or 'default'\n",
        "        subject = sample['id'].split('_')[0] if '_' in sample['id'] else 'default'\n",
        "\n",
        "        # Initialize subject results (renamed key for clarity)\n",
        "        if subject not in results:\n",
        "            results[subject] = {\n",
        "                \"correct\": 0,\n",
        "                \"total\": 0,\n",
        "                \"perplexity_sum\": 0, # Store sum for later averaging per category\n",
        "                \"perplexity_count\": 0\n",
        "                # \"exact_matches\" was not used, removed for now\n",
        "            }\n",
        "\n",
        "        results[subject][\"total\"] += 1\n",
        "\n",
        "        # Format question and choices\n",
        "        question = sample[\"question\"] + \"\\n\"\n",
        "        choices = sample[\"choices\"][\"text\"]\n",
        "        choice_labels = sample[\"choices\"][\"label\"] # e.g., ['A', 'B', 'C', 'D'] or [1, 2, 3, 4]\n",
        "\n",
        "        # Handle numeric or letter choice labels consistently\n",
        "        choice_map = {}\n",
        "        formatted_choices = []\n",
        "        for i, choice_text in enumerate(choices):\n",
        "            choice_label = str(choice_labels[i]) # Ensure string\n",
        "            formatted_choices.append(f\"{choice_label}. {choice_text}\")\n",
        "            choice_map[choice_label] = choice_text # Store for checking prediction\n",
        "        question += \"\\n\".join(formatted_choices) + \"\\nAnswer:\"\n",
        "\n",
        "        # Get the correct answer key\n",
        "        correct_label = str(sample[\"answerKey\"]) # Ensure string\n",
        "\n",
        "        # Tokenize and generate\n",
        "        try:\n",
        "            inputs = tokenizer(question, return_tensors=\"pt\").to(device)\n",
        "            input_length = inputs.input_ids.shape[1]\n",
        "        except Exception as e:\n",
        "            print(f\"Tokenization error for sample ID {sample.get('id', 'N/A')}: {e}\")\n",
        "            continue # Skip this sample\n",
        "\n",
        "        # Performance and accuracy tracking within no_grad context\n",
        "        with torch.no_grad():\n",
        "            # Robust Perplexity Calculation\n",
        "            try:\n",
        "                outputs = model(inputs.input_ids, labels=inputs.input_ids)\n",
        "                loss = outputs.loss\n",
        "\n",
        "                if loss is not None and not torch.isnan(loss) and not torch.isinf(loss):\n",
        "                    perplexity_val = torch.exp(loss).item()\n",
        "                    if 0 < perplexity_val < float('inf'): # Filter extreme values\n",
        "                        total_perplexity += perplexity_val\n",
        "                        results[subject][\"perplexity_sum\"] += perplexity_val\n",
        "                        results[subject][\"perplexity_count\"] += 1\n",
        "                        perplexity_count += 1\n",
        "                    # else: print(f\"Filtered extreme perplexity: {perplexity_val}\") # Optional: Debugging\n",
        "                # else: print(\"Skipping NaN/Inf/None loss\") # Optional: Debugging\n",
        "            except Exception as e:\n",
        "                print(f\"Perplexity calculation error for sample ID {sample.get('id', 'N/A')}: {e}\")\n",
        "                # perplexity_val = None # Not strictly needed here\n",
        "\n",
        "            # Generate outputs\n",
        "            try:\n",
        "                gen_outputs = model.generate(\n",
        "                    **inputs,\n",
        "                    max_new_tokens=max_new_tokens,\n",
        "                    pad_token_id=tokenizer.eos_token_id,\n",
        "                    do_sample=False # Use greedy decoding for consistency\n",
        "                )\n",
        "                generated_length = gen_outputs.shape[1] - input_length\n",
        "                total_tokens_processed += generated_length # Count only generated tokens\n",
        "\n",
        "                # Decode the generated part\n",
        "                generated_token_ids = gen_outputs[0, input_length:]\n",
        "                predicted_text = tokenizer.decode(generated_token_ids, skip_special_tokens=True).strip()\n",
        "\n",
        "            except Exception as e:\n",
        "                 print(f\"Generation error for sample ID {sample.get('id', 'N/A')}: {e}\")\n",
        "                 predicted_text = \"\" # Assume failure\n",
        "                 generated_length = 0 # Don't count tokens if generation failed\n",
        "\n",
        "            # --- Track GPU Memory After Inference Step ---\n",
        "            if torch.cuda.is_available():\n",
        "                gpu_memory_readings_gb.append(torch.cuda.memory_allocated(device=device) / (1024 ** 3))\n",
        "            # ---------------------------------------------\n",
        "\n",
        "        # Simplified Prediction Logic (Handles cases like \"A\", \"A.\", \"A)\")\n",
        "        predicted_label = None\n",
        "        cleaned_prediction = predicted_text.strip().upper() # Normalize prediction slightly\n",
        "        if cleaned_prediction:\n",
        "            # Check if the first character is a valid choice label\n",
        "            first_char = cleaned_prediction[0]\n",
        "            if first_char in choice_map:\n",
        "                 predicted_label = first_char\n",
        "\n",
        "        # Accuracy tracking\n",
        "        is_correct = (predicted_label == correct_label)\n",
        "        if is_correct:\n",
        "            correct += 1\n",
        "            results[subject][\"correct\"] += 1\n",
        "\n",
        "    # <<< Final calculations >>>\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "    # Avoid division by zero if dataset is empty or no tokens processed\n",
        "    num_samples = len(dataset)\n",
        "    accuracy = (correct / num_samples) if num_samples > 0 else 0\n",
        "\n",
        "    # Robust average perplexity calculation\n",
        "    avg_perplexity = (total_perplexity / perplexity_count) if perplexity_count > 0 else None\n",
        "\n",
        "    # Performance metrics\n",
        "    avg_inference_latency_ms = ((total_time * 1000) / total_tokens_processed) if total_tokens_processed > 0 else 0\n",
        "    # Note: Speed/Throughput definitions can vary. Here: Avg tokens/sec over total time.\n",
        "    avg_tokens_per_sec = total_tokens_processed / total_time if total_time > 0 else 0\n",
        "\n",
        "    # --- Calculate Average and Peak Memory ---\n",
        "    avg_gpu_memory_allocated_gb = 0\n",
        "    peak_gpu_memory_gb = 0\n",
        "    if torch.cuda.is_available():\n",
        "        if gpu_memory_readings_gb:\n",
        "            # Use statistics.mean for safety\n",
        "            try:\n",
        "                avg_gpu_memory_allocated_gb = statistics.mean(gpu_memory_readings_gb)\n",
        "            except statistics.StatisticsError: # Handle empty list case just in case\n",
        "                 avg_gpu_memory_allocated_gb = 0\n",
        "        # Get peak memory recorded *during* the loop\n",
        "        peak_gpu_memory_gb = torch.cuda.max_memory_allocated(device=device) / (1024 ** 3)\n",
        "    # -----------------------------------------\n",
        "\n",
        "    # Calculate subject-level accuracy and average perplexity\n",
        "    category_results_final = {}\n",
        "    for subject, data in results.items():\n",
        "        cat_accuracy = (data[\"correct\"] / data[\"total\"]) if data[\"total\"] > 0 else 0\n",
        "        cat_avg_perplexity = (data[\"perplexity_sum\"] / data[\"perplexity_count\"]) if data[\"perplexity_count\"] > 0 else None\n",
        "        category_results_final[subject] = {\n",
        "            \"accuracy\": cat_accuracy,\n",
        "            \"correct\": data[\"correct\"],\n",
        "            \"total\": data[\"total\"],\n",
        "            \"avg_perplexity\": cat_avg_perplexity\n",
        "        }\n",
        "\n",
        "    # <<< Return Dictionary with Correct Keys >>>\n",
        "    return {\n",
        "        # Overall Metrics\n",
        "        \"accuracy\": accuracy,\n",
        "        \"perplexity\": avg_perplexity, # Overall average perplexity\n",
        "        \"inference_latency\": avg_inference_latency_ms, # Renamed for clarity (ms/token)\n",
        "\n",
        "        # Performance Metrics (Optional but potentially useful)\n",
        "        # \"inference_speed_tps\": avg_tokens_per_sec, # Tokens per second (overall)\n",
        "        # \"total_time_sec\": total_time,\n",
        "        # \"total_tokens_processed\": total_tokens_processed,\n",
        "\n",
        "        # Memory Metrics (Corrected)\n",
        "        \"peak_gpu_memory_gb\": peak_gpu_memory_gb, # Peak during evaluation\n",
        "        \"avg_gpu_memory_gb\": avg_gpu_memory_allocated_gb, # Average of readings during eval\n",
        "\n",
        "        # Aggregate Counts\n",
        "        # \"total_correct\": correct,\n",
        "        # \"total_samples\": num_samples,\n",
        "\n",
        "        # Category/Subject level results\n",
        "        \"category_results\": category_results_final,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "S0441PSwbu9L",
        "outputId": "e15d1216-89cb-4891-dbe0-093648f77fe2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model: Qwen/Qwen2.5-3B (FP32)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5b9a472ca2cd42049e357f5de0616053"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded.\n",
            "Loading tokenizer...\n",
            "Tokenizer loaded.\n",
            "Starting ARC evaluation...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating ARC:   0%|          | 0/200 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5615eaa88e3b43e2bb780f81fa2e4c67"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ARC evaluation finished.\n",
            "\n",
            "===== FP32 ARC DETAILED MODEL METRICS =====\n",
            "------------------------------------------------------------\n",
            "PPL (Perplexity)                   : 6.4848\n",
            "Accuracy                           : 90.5000\n",
            "Memory Footprint (Model Size) (GB) : 11.4960\n",
            "Inference Latency (ms/token)       : 174.9104\n",
            "Avg GPU Utilization (%)            : 87.0769\n",
            "Avg GPU Memory Allocated (GB)      : 11.5528\n",
            "------------------------------------------------------------\n",
            "Starting cleanup for FP32 ARC cell...\n",
            "FP32 model deleted.\n",
            "GPU cache cleared and garbage collected after FP32 ARC run.\n"
          ]
        }
      ],
      "source": [
        "# --- FP32 ARC-easy Benchmark Evaluation Cell ---\n",
        "\n",
        "# Assume 'handle' and 'stop_polling_event' exist from a previous cell.\n",
        "if 'handle' not in locals() or not handle:\n",
        "    print(\"Error: NVML handle not found. Please initialize NVML in a prior cell.\")\n",
        "    handle = None # Prevent polling\n",
        "elif 'stop_polling_event' not in locals():\n",
        "    print(\"Error: stop_polling_event not found. Please initialize in a prior cell.\")\n",
        "    # If needed, define fallback: stop_polling_event = threading.Event()\n",
        "\n",
        "# Clear previous readings and reset event\n",
        "gpu_utilization_readings = []\n",
        "if 'stop_polling_event' in locals():\n",
        "    stop_polling_event.clear()\n",
        "\n",
        "# Start GPU polling thread\n",
        "polling_thread = None\n",
        "if handle:\n",
        "    polling_thread = threading.Thread(target=poll_gpu_utilization, args=(handle, 1.0), daemon=True)\n",
        "    polling_thread.start()\n",
        "\n",
        "# Clean memory before starting\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "fp32_arc_metrics = {}\n",
        "fp32_arc_results_data = {}\n",
        "model_fp32 = None # Define outside try\n",
        "\n",
        "try:\n",
        "    # Load full precision model (FP32)\n",
        "    model_name = \"Qwen/Qwen2.5-3B\"\n",
        "    print(f\"Loading model: {model_name} (FP32)...\")\n",
        "    model_fp32 = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        torch_dtype=torch.float32,\n",
        "        device_map=\"cuda:0\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "    print(\"Model loaded.\")\n",
        "\n",
        "    # Load tokenizer\n",
        "    print(\"Loading tokenizer...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    print(\"Tokenizer loaded.\")\n",
        "\n",
        "    # --- No reset_peak_memory_stats needed for this metric set ---\n",
        "\n",
        "    # Run ARC evaluation\n",
        "    print(\"Starting ARC evaluation...\")\n",
        "    # *** Assuming evaluate_arc returns keys similar to evaluate_mmlu ***\n",
        "    # Specifically: 'perplexity', 'accuracy', 'inference_latency', 'avg_gpu_memory_gb'\n",
        "    fp32_arc_results_data = evaluate_arc(model_fp32, tokenizer, samples_200_arc_test, max_new_tokens=1)\n",
        "    print(\"ARC evaluation finished.\")\n",
        "\n",
        "    # Stop polling thread and calculate average utilization\n",
        "    avg_gpu_utilization = 'N/A'\n",
        "    if polling_thread and polling_thread.is_alive():\n",
        "        stop_polling_event.set()\n",
        "        polling_thread.join(timeout=5)\n",
        "        if gpu_utilization_readings:\n",
        "            avg_gpu_utilization = statistics.mean(gpu_utilization_readings)\n",
        "        else:\n",
        "             avg_gpu_utilization = 0 # Polling ran but got no readings\n",
        "    elif handle:\n",
        "        avg_gpu_utilization = 0 # Polling failed to start/run correctly\n",
        "\n",
        "    # Collect metrics in the MMLU format\n",
        "    fp32_arc_metrics = {\n",
        "        \"PPL (Perplexity)\": fp32_arc_results_data.get('perplexity', 'N/A'),\n",
        "        \"Accuracy\": fp32_arc_results_data.get('accuracy', 'N/A') * 100,\n",
        "        \"Memory Footprint (Model Size) (GB)\": model_fp32.get_memory_footprint() / (1024 ** 3),\n",
        "        \"Inference Latency (ms/token)\": fp32_arc_results_data.get('inference_latency', 'N/A'),\n",
        "        \"Avg GPU Utilization (%)\": avg_gpu_utilization, # From polling\n",
        "        \"Avg GPU Memory Allocated (GB)\": fp32_arc_results_data.get('avg_gpu_memory_gb', 'N/A'), # From evaluate_arc result\n",
        "    }\n",
        "\n",
        "    # Print metrics\n",
        "    print(\"\\n===== FP32 ARC DETAILED MODEL METRICS =====\")\n",
        "    print(\"-\" * 60)\n",
        "    if fp32_arc_metrics:\n",
        "        # Ensure consistent key width for alignment\n",
        "        try:\n",
        "            max_key_length = max(len(key) for key in fp32_arc_metrics.keys())\n",
        "        except ValueError: # Handle case where dict might be empty if error occurred before population\n",
        "             max_key_length = 35 # Default width\n",
        "\n",
        "        for key, value in fp32_arc_metrics.items():\n",
        "             # Format floating point numbers, handle 'N/A'\n",
        "             if isinstance(value, (float, int)):\n",
        "                 # Use fixed precision for consistency\n",
        "                 print(f\"{key.ljust(max_key_length)} : {value:.4f}\")\n",
        "             else:\n",
        "                 # Print N/A or other strings directly\n",
        "                 print(f\"{key.ljust(max_key_length)} : {value}\")\n",
        "    else:\n",
        "        print(\"No metrics collected.\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "except pynvml.NVMLError as nvml_error:\n",
        "     print(f\"NVML Error during FP32 ARC evaluation steps: {nvml_error}\")\n",
        "except Exception as e:\n",
        "    print(f\"FP32 ARC Evaluation error: {e}\")\n",
        "    # Ensure polling stops on error\n",
        "    if polling_thread and polling_thread.is_alive():\n",
        "        stop_polling_event.set()\n",
        "        try:\n",
        "            polling_thread.join(timeout=5)\n",
        "        except Exception as join_e:\n",
        "            print(f\"Error stopping polling thread after exception: {join_e}\")\n",
        "\n",
        "finally:\n",
        "    # Clean up GPU memory (No NVML Shutdown)\n",
        "    print(\"Starting cleanup for FP32 ARC cell...\")\n",
        "    if 'model_fp32' in locals() and model_fp32 is not None:\n",
        "        del model_fp32\n",
        "        print(\"FP32 model deleted.\")\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    print(\"GPU cache cleared and garbage collected after FP32 ARC run.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "xX7yj7O6iwxq",
        "outputId": "c43a852e-96a3-407c-d75f-09b7ef5e2402"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model: Qwen/Qwen2.5-3B (FP16)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f96bb7f58a1b424eaf5e906d48d43356"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded.\n",
            "Loading tokenizer...\n",
            "Tokenizer loaded.\n",
            "Starting ARC evaluation (FP16)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating ARC:   0%|          | 0/200 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bff4d629cae541b8a46aeb4003f5e6fb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ARC evaluation finished.\n",
            "\n",
            "===== FP16 ARC DETAILED MODEL METRICS =====\n",
            "------------------------------------------------------------\n",
            "PPL (Perplexity)                   : 6.4846\n",
            "Accuracy                           : 90.5000\n",
            "Memory Footprint (Model Size) (GB) : 5.7480\n",
            "Inference Latency (ms/token)       : 105.6825\n",
            "Avg GPU Utilization (%)            : 56.4800\n",
            "Avg GPU Memory Allocated (GB)      : 5.7745\n",
            "------------------------------------------------------------\n",
            "Starting cleanup for FP16 ARC cell...\n",
            "FP16 model deleted.\n",
            "GPU cache cleared and garbage collected after FP16 ARC run.\n"
          ]
        }
      ],
      "source": [
        "# --- FP16 ARC-easy Benchmark Evaluation Cell ---\n",
        "\n",
        "# Assume 'handle' and 'stop_polling_event' exist from a previous cell.\n",
        "if 'handle' not in locals() or not handle:\n",
        "    print(\"Error: NVML handle not found. Please initialize NVML in a prior cell.\")\n",
        "    handle = None # Prevent polling\n",
        "elif 'stop_polling_event' not in locals():\n",
        "    print(\"Error: stop_polling_event not found. Please initialize in a prior cell.\")\n",
        "    # If needed, define fallback: stop_polling_event = threading.Event()\n",
        "\n",
        "# Clear previous readings and reset event\n",
        "gpu_utilization_readings = []\n",
        "if 'stop_polling_event' in locals():\n",
        "    stop_polling_event.clear()\n",
        "\n",
        "# Start GPU polling thread\n",
        "polling_thread = None\n",
        "if handle:\n",
        "    polling_thread = threading.Thread(target=poll_gpu_utilization, args=(handle, 1.0), daemon=True)\n",
        "    polling_thread.start()\n",
        "\n",
        "# Clean memory before starting\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "fp16_arc_metrics = {}\n",
        "fp16_arc_results_data = {}\n",
        "model_fp16 = None # Define outside try\n",
        "\n",
        "try:\n",
        "    # Load half precision model (FP16)\n",
        "    model_name = \"Qwen/Qwen2.5-3B\" # Same base model\n",
        "    print(f\"Loading model: {model_name} (FP16)...\")\n",
        "    model_fp16 = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        torch_dtype=torch.float16, # <--- Use FP16\n",
        "        device_map=\"cuda:0\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "    print(\"Model loaded.\")\n",
        "\n",
        "    # Load tokenizer (usually the same for different precisions)\n",
        "    print(\"Loading tokenizer...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    print(\"Tokenizer loaded.\")\n",
        "\n",
        "    # --- No reset_peak_memory_stats needed for this metric set ---\n",
        "\n",
        "    # Run ARC evaluation using the updated evaluate_arc function\n",
        "    print(\"Starting ARC evaluation (FP16)...\")\n",
        "    # Assuming evaluate_arc can handle fp16 and returns the required keys\n",
        "    fp16_arc_results_data = evaluate_arc(model_fp16, tokenizer, samples_200_arc_test, max_new_tokens=1)\n",
        "    print(\"ARC evaluation finished.\")\n",
        "\n",
        "    # Stop polling thread and calculate average utilization\n",
        "    avg_gpu_utilization = 'N/A'\n",
        "    if polling_thread and polling_thread.is_alive():\n",
        "        stop_polling_event.set()\n",
        "        polling_thread.join(timeout=5)\n",
        "        if gpu_utilization_readings:\n",
        "            avg_gpu_utilization = statistics.mean(gpu_utilization_readings)\n",
        "        else:\n",
        "             avg_gpu_utilization = 0 # Polling ran but got no readings\n",
        "    elif handle:\n",
        "        avg_gpu_utilization = 0 # Polling failed to start/run correctly\n",
        "\n",
        "    # Collect metrics in the MMLU format using results from evaluate_arc\n",
        "    accuracy_val = fp16_arc_results_data.get('accuracy', 'N/A')\n",
        "    if isinstance(accuracy_val, (float, int)):\n",
        "        accuracy_val *= 100 # Multiply only if it's a number\n",
        "\n",
        "    fp16_arc_metrics = {\n",
        "        \"PPL (Perplexity)\": fp16_arc_results_data.get('perplexity', 'N/A'),\n",
        "        \"Accuracy\": accuracy_val,\n",
        "        \"Memory Footprint (Model Size) (GB)\": model_fp16.get_memory_footprint() / (1024 ** 3),\n",
        "        \"Inference Latency (ms/token)\": fp16_arc_results_data.get('inference_latency', 'N/A'),\n",
        "        \"Avg GPU Utilization (%)\": avg_gpu_utilization, # From polling\n",
        "        \"Avg GPU Memory Allocated (GB)\": fp16_arc_results_data.get('avg_gpu_memory_gb', 'N/A'), # From evaluate_arc result\n",
        "    }\n",
        "\n",
        "    # Print metrics\n",
        "    print(\"\\n===== FP16 ARC DETAILED MODEL METRICS =====\")\n",
        "    print(\"-\" * 60)\n",
        "    if fp16_arc_metrics:\n",
        "        try:\n",
        "            max_key_length = max(len(key) for key in fp16_arc_metrics.keys())\n",
        "        except ValueError:\n",
        "             max_key_length = 35 # Default width\n",
        "\n",
        "        for key, value in fp16_arc_metrics.items():\n",
        "             if isinstance(value, (float, int)):\n",
        "                 print(f\"{key.ljust(max_key_length)} : {value:.4f}\")\n",
        "             else:\n",
        "                 print(f\"{key.ljust(max_key_length)} : {value}\") # Handles 'N/A'\n",
        "    else:\n",
        "        print(\"No metrics collected.\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "except pynvml.NVMLError as nvml_error:\n",
        "     print(f\"NVML Error during FP16 ARC evaluation steps: {nvml_error}\")\n",
        "except Exception as e:\n",
        "    print(f\"FP16 ARC Evaluation error: {e}\")\n",
        "    # Ensure polling stops on error\n",
        "    if polling_thread and polling_thread.is_alive():\n",
        "        stop_polling_event.set()\n",
        "        try:\n",
        "            polling_thread.join(timeout=5)\n",
        "        except Exception as join_e:\n",
        "            print(f\"Error stopping polling thread after exception: {join_e}\")\n",
        "\n",
        "finally:\n",
        "    # Clean up GPU memory (No NVML Shutdown)\n",
        "    print(\"Starting cleanup for FP16 ARC cell...\")\n",
        "    if 'model_fp16' in locals() and model_fp16 is not None: # Check for model_fp16\n",
        "        del model_fp16 # Delete model_fp16\n",
        "        print(\"FP16 model deleted.\")\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    print(\"GPU cache cleared and garbage collected after FP16 ARC run.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "id": "YiNft20iTYD0",
        "outputId": "ba592c74-54b2-44d6-a64d-8474a28f7fc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuring NF4 quantization...\n",
            "Quantization config created.\n",
            "Loading model: Qwen/Qwen2.5-3B (NF4 Quantized)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a0033e47779e40fd9bdbf4a2478eeeb6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded.\n",
            "Loading tokenizer...\n",
            "Tokenizer loaded.\n",
            "Starting ARC evaluation (NF4)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating ARC:   0%|          | 0/200 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f8ac9f65e4b349beba166efcc9d31f77"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ARC evaluation finished.\n",
            "\n",
            "===== NF4 ARC DETAILED MODEL METRICS =====\n",
            "------------------------------------------------------------\n",
            "PPL (Perplexity)                   : 7.3644\n",
            "Accuracy                           : 90.0000\n",
            "Memory Footprint (Model Size) (GB) : 1.8720\n",
            "Inference Latency (ms/token)       : 343.5057\n",
            "Avg GPU Utilization (%)            : 86.4872\n",
            "Avg GPU Memory Allocated (GB)      : 1.9410\n",
            "------------------------------------------------------------\n",
            "Starting cleanup for NF4 ARC cell...\n",
            "NF4 model deleted.\n",
            "GPU cache cleared and garbage collected after NF4 ARC run.\n"
          ]
        }
      ],
      "source": [
        "# --- NF4 (FP32 Compute) ARC-easy Benchmark Evaluation Cell ---\n",
        "\n",
        "# Assume 'handle' and 'stop_polling_event' exist from a previous cell.\n",
        "if 'handle' not in locals() or not handle:\n",
        "    print(\"Error: NVML handle not found. Please initialize NVML in a prior cell.\")\n",
        "    handle = None # Prevent polling\n",
        "elif 'stop_polling_event' not in locals():\n",
        "    print(\"Error: stop_polling_event not found. Please initialize in a prior cell.\")\n",
        "    # If needed, define fallback: stop_polling_event = threading.Event()\n",
        "\n",
        "# Clear previous readings and reset event\n",
        "gpu_utilization_readings = []\n",
        "if 'stop_polling_event' in locals():\n",
        "    stop_polling_event.clear()\n",
        "\n",
        "# Start GPU polling thread\n",
        "polling_thread = None\n",
        "if handle:\n",
        "    polling_thread = threading.Thread(target=poll_gpu_utilization, args=(handle, 1.0), daemon=True)\n",
        "    polling_thread.start()\n",
        "\n",
        "# Clean memory before starting\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "nf4_arc_metrics = {}\n",
        "nf4_arc_results_data = {}\n",
        "model_nf4 = None # Define outside try\n",
        "\n",
        "try:\n",
        "    # Configure quantization to NF4\n",
        "    print(\"Configuring NF4 quantization...\")\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",  # Normalized Float 4 format\n",
        "        bnb_4bit_compute_dtype=torch.float32, # Or torch.bfloat16 if supported and desired\n",
        "        bnb_4bit_use_double_quant=True\n",
        "    )\n",
        "    print(\"Quantization config created.\")\n",
        "\n",
        "    # Load quantized model (NF4)\n",
        "    model_name = \"Qwen/Qwen2.5-3B\" # Base model name\n",
        "    print(f\"Loading model: {model_name} (NF4 Quantized)...\")\n",
        "    model_nf4 = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        quantization_config=bnb_config, # <--- Use NF4 quantization config\n",
        "        device_map=\"cuda:0\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "    print(\"Model loaded.\")\n",
        "\n",
        "    # Load tokenizer\n",
        "    print(\"Loading tokenizer...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    print(\"Tokenizer loaded.\")\n",
        "\n",
        "    # --- No reset_peak_memory_stats needed for this metric set ---\n",
        "\n",
        "    # Run ARC evaluation using the evaluate_arc function\n",
        "    print(\"Starting ARC evaluation (NF4)...\")\n",
        "    # Assuming evaluate_arc can handle the quantized model and returns required keys\n",
        "    # Using max_new_tokens=1 for consistency with FP16/FP32 cells\n",
        "    nf4_arc_results_data = evaluate_arc(model_nf4, tokenizer, samples_200_arc_test, max_new_tokens=1)\n",
        "    print(\"ARC evaluation finished.\")\n",
        "\n",
        "    # Stop polling thread and calculate average utilization\n",
        "    avg_gpu_utilization = 'N/A'\n",
        "    if polling_thread and polling_thread.is_alive():\n",
        "        stop_polling_event.set()\n",
        "        polling_thread.join(timeout=5)\n",
        "        if gpu_utilization_readings:\n",
        "            avg_gpu_utilization = statistics.mean(gpu_utilization_readings)\n",
        "        else:\n",
        "             avg_gpu_utilization = 0 # Polling ran but got no readings\n",
        "    elif handle:\n",
        "        avg_gpu_utilization = 0 # Polling failed to start/run correctly\n",
        "\n",
        "    # Collect metrics in the MMLU format using results from evaluate_arc\n",
        "    accuracy_val = nf4_arc_results_data.get('accuracy', 'N/A')\n",
        "    if isinstance(accuracy_val, (float, int)):\n",
        "        accuracy_val *= 100 # Multiply only if it's a number\n",
        "\n",
        "    nf4_arc_metrics = {\n",
        "        \"PPL (Perplexity)\": nf4_arc_results_data.get('perplexity', 'N/A'),\n",
        "        \"Accuracy\": accuracy_val,\n",
        "        \"Memory Footprint (Model Size) (GB)\": model_nf4.get_memory_footprint() / (1024 ** 3),\n",
        "        \"Inference Latency (ms/token)\": nf4_arc_results_data.get('inference_latency', 'N/A'),\n",
        "        \"Avg GPU Utilization (%)\": avg_gpu_utilization, # From polling\n",
        "        \"Avg GPU Memory Allocated (GB)\": nf4_arc_results_data.get('avg_gpu_memory_gb', 'N/A'), # From evaluate_arc result\n",
        "    }\n",
        "\n",
        "    # Print metrics\n",
        "    print(\"\\n===== NF4 ARC DETAILED MODEL METRICS =====\")\n",
        "    print(\"-\" * 60)\n",
        "    if nf4_arc_metrics:\n",
        "        try:\n",
        "            max_key_length = max(len(key) for key in nf4_arc_metrics.keys())\n",
        "        except ValueError:\n",
        "             max_key_length = 35 # Default width\n",
        "\n",
        "        for key, value in nf4_arc_metrics.items():\n",
        "             if isinstance(value, (float, int)):\n",
        "                 print(f\"{key.ljust(max_key_length)} : {value:.4f}\")\n",
        "             else:\n",
        "                 print(f\"{key.ljust(max_key_length)} : {value}\") # Handles 'N/A'\n",
        "    else:\n",
        "        print(\"No metrics collected.\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "except pynvml.NVMLError as nvml_error:\n",
        "     print(f\"NVML Error during NF4 ARC evaluation steps: {nvml_error}\")\n",
        "except Exception as e:\n",
        "    print(f\"NF4 ARC Evaluation error: {e}\")\n",
        "    # Ensure polling stops on error\n",
        "    if polling_thread and polling_thread.is_alive():\n",
        "        stop_polling_event.set()\n",
        "        try:\n",
        "            polling_thread.join(timeout=5)\n",
        "        except Exception as join_e:\n",
        "            print(f\"Error stopping polling thread after exception: {join_e}\")\n",
        "\n",
        "finally:\n",
        "    # Clean up GPU memory (No NVML Shutdown)\n",
        "    print(\"Starting cleanup for NF4 ARC cell...\")\n",
        "    if 'model_nf4' in locals() and model_nf4 is not None: # Check for model_nf4\n",
        "        del model_nf4 # Delete model_nf4\n",
        "        print(\"NF4 model deleted.\")\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    print(\"GPU cache cleared and garbage collected after NF4 ARC run.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "id": "oaD6c2IchlIt",
        "outputId": "1e29fe67-6c0e-464f-9959-428b3669d51e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuring NF4 quantization (FP16 Compute)...\n",
            "Quantization config created.\n",
            "Loading model: Qwen/Qwen2.5-3B (NF4 Quantized, FP16 Compute)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8f82b625b8914954b9b9c36ec62b6a06"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded.\n",
            "Loading tokenizer...\n",
            "Tokenizer loaded.\n",
            "Starting ARC evaluation (NF4, FP16 Compute)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating ARC:   0%|          | 0/200 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "566d04fa7cdd4ca9815b9ba294a680ce"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ARC evaluation finished.\n",
            "\n",
            "===== NF4 (FP16 Compute) ARC DETAILED MODEL METRICS =====\n",
            "------------------------------------------------------------\n",
            "PPL (Perplexity)                   : 7.3646\n",
            "Accuracy                           : 90.0000\n",
            "Memory Footprint (Model Size) (GB) : 1.8720\n",
            "Inference Latency (ms/token)       : 194.0452\n",
            "Avg GPU Utilization (%)            : 40.7292\n",
            "Avg GPU Memory Allocated (GB)      : 1.9410\n",
            "------------------------------------------------------------\n",
            "Starting cleanup for NF4 (FP16 Compute) ARC cell...\n",
            "NF4 model deleted.\n",
            "GPU cache cleared and garbage collected after NF4 (FP16 Compute) ARC run.\n"
          ]
        }
      ],
      "source": [
        "# --- NF4 (FP16 Compute) ARC-easy Benchmark Evaluation Cell --- # <<< Title updated for FP16 compute\n",
        "\n",
        "# Assume 'handle' and 'stop_polling_event' exist from a previous cell.\n",
        "if 'handle' not in locals() or not handle:\n",
        "    print(\"Error: NVML handle not found. Please initialize NVML in a prior cell.\")\n",
        "    handle = None # Prevent polling\n",
        "elif 'stop_polling_event' not in locals():\n",
        "    print(\"Error: stop_polling_event not found. Please initialize in a prior cell.\")\n",
        "    # If needed, define fallback: stop_polling_event = threading.Event()\n",
        "\n",
        "# Clear previous readings and reset event\n",
        "gpu_utilization_readings = []\n",
        "if 'stop_polling_event' in locals():\n",
        "    stop_polling_event.clear()\n",
        "\n",
        "# Start GPU polling thread\n",
        "polling_thread = None\n",
        "if handle:\n",
        "    polling_thread = threading.Thread(target=poll_gpu_utilization, args=(handle, 1.0), daemon=True)\n",
        "    polling_thread.start()\n",
        "\n",
        "# Clean memory before starting\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "nf4_arc_metrics = {}\n",
        "nf4_arc_results_data = {}\n",
        "model_nf4 = None # Define outside try\n",
        "\n",
        "try:\n",
        "    # Configure quantization to NF4 with FP16 compute\n",
        "    print(\"Configuring NF4 quantization (FP16 Compute)...\") # <<< Log updated\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",          # Normalized Float 4 format\n",
        "        bnb_4bit_compute_dtype=torch.float16, # <--- Compute dtype set to FP16\n",
        "        bnb_4bit_use_double_quant=True\n",
        "    )\n",
        "    print(\"Quantization config created.\")\n",
        "\n",
        "    # Load quantized model (NF4)\n",
        "    model_name = \"Qwen/Qwen2.5-3B\" # Base model name\n",
        "    print(f\"Loading model: {model_name} (NF4 Quantized, FP16 Compute)...\") # <<< Log updated\n",
        "    model_nf4 = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        quantization_config=bnb_config, # Use NF4 quantization config\n",
        "        device_map=\"cuda:0\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "    print(\"Model loaded.\")\n",
        "    # --- Removed loading of separate FP16 model ---\n",
        "\n",
        "    # Load tokenizer\n",
        "    print(\"Loading tokenizer...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    print(\"Tokenizer loaded.\")\n",
        "\n",
        "    # --- No reset_peak_memory_stats needed for this metric set ---\n",
        "\n",
        "    # Run ARC evaluation using the evaluate_arc function\n",
        "    print(\"Starting ARC evaluation (NF4, FP16 Compute)...\") # <<< Log updated\n",
        "    # Assuming evaluate_arc can handle the quantized model and returns required keys\n",
        "    # Using max_new_tokens=1 for consistency with the format\n",
        "    nf4_arc_results_data = evaluate_arc(model_nf4, tokenizer, samples_200_arc_test, max_new_tokens=1) # <<< max_new_tokens changed to 1\n",
        "    print(\"ARC evaluation finished.\")\n",
        "\n",
        "    # Stop polling thread and calculate average utilization\n",
        "    avg_gpu_utilization = 'N/A'\n",
        "    if polling_thread and polling_thread.is_alive():\n",
        "        stop_polling_event.set()\n",
        "        polling_thread.join(timeout=5)\n",
        "        if gpu_utilization_readings:\n",
        "            avg_gpu_utilization = statistics.mean(gpu_utilization_readings)\n",
        "        else:\n",
        "             avg_gpu_utilization = 0 # Polling ran but got no readings\n",
        "    elif handle:\n",
        "        avg_gpu_utilization = 0 # Polling failed to start/run correctly\n",
        "\n",
        "    # Collect metrics in the MMLU format using results from evaluate_arc\n",
        "    accuracy_val = nf4_arc_results_data.get('accuracy', 'N/A')\n",
        "    if isinstance(accuracy_val, (float, int)):\n",
        "        accuracy_val *= 100 # Multiply only if it's a number\n",
        "\n",
        "    nf4_arc_metrics = {\n",
        "        \"PPL (Perplexity)\": nf4_arc_results_data.get('perplexity', 'N/A'),\n",
        "        \"Accuracy\": accuracy_val,\n",
        "        \"Memory Footprint (Model Size) (GB)\": model_nf4.get_memory_footprint() / (1024 ** 3),\n",
        "        \"Inference Latency (ms/token)\": nf4_arc_results_data.get('inference_latency', 'N/A'),\n",
        "        \"Avg GPU Utilization (%)\": avg_gpu_utilization, # From polling\n",
        "        \"Avg GPU Memory Allocated (GB)\": nf4_arc_results_data.get('avg_gpu_memory_gb', 'N/A'), # From evaluate_arc result\n",
        "    }\n",
        "\n",
        "    # Print metrics\n",
        "    # <<< Title updated for FP16 compute\n",
        "    print(\"\\n===== NF4 (FP16 Compute) ARC DETAILED MODEL METRICS =====\")\n",
        "    print(\"-\" * 60)\n",
        "    if nf4_arc_metrics:\n",
        "        try:\n",
        "            max_key_length = max(len(key) for key in nf4_arc_metrics.keys())\n",
        "        except ValueError:\n",
        "             max_key_length = 35 # Default width\n",
        "\n",
        "        for key, value in nf4_arc_metrics.items():\n",
        "             if isinstance(value, (float, int)):\n",
        "                 print(f\"{key.ljust(max_key_length)} : {value:.4f}\")\n",
        "             else:\n",
        "                 print(f\"{key.ljust(max_key_length)} : {value}\") # Handles 'N/A'\n",
        "    else:\n",
        "        print(\"No metrics collected.\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "except pynvml.NVMLError as nvml_error:\n",
        "     print(f\"NVML Error during NF4 (FP16 Compute) ARC evaluation steps: {nvml_error}\") # <<< Log updated\n",
        "except Exception as e:\n",
        "    print(f\"NF4 (FP16 Compute) ARC Evaluation error: {e}\") # <<< Log updated\n",
        "    # Ensure polling stops on error\n",
        "    if polling_thread and polling_thread.is_alive():\n",
        "        stop_polling_event.set()\n",
        "        try:\n",
        "            polling_thread.join(timeout=5)\n",
        "        except Exception as join_e:\n",
        "            print(f\"Error stopping polling thread after exception: {join_e}\")\n",
        "\n",
        "finally:\n",
        "    # Clean up GPU memory (No NVML Shutdown)\n",
        "    print(\"Starting cleanup for NF4 (FP16 Compute) ARC cell...\")\n",
        "    if 'model_nf4' in locals() and model_nf4 is not None: # Check for model_nf4\n",
        "        del model_nf4 # Delete model_nf4\n",
        "        print(\"NF4 model deleted.\")\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    print(\"GPU cache cleared and garbage collected after NF4 (FP16 Compute) ARC run.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "id": "4vtMEaVQbQwH",
        "outputId": "d3bf7186-7e9d-4e38-900b-f70f6c9a9ccc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuring FP4 quantization (FP32 Compute)...\n",
            "Quantization config created.\n",
            "Loading model: Qwen/Qwen2.5-3B (FP4 Quantized, FP32 Compute)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "383a1ce341624b9ab83ebbe827a81e3d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded.\n",
            "Loading tokenizer...\n",
            "Tokenizer loaded.\n",
            "Starting ARC evaluation (FP4, FP32 Compute)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating ARC:   0%|          | 0/200 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6877108fd92542d5afa1e771a95e9462"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ARC evaluation finished.\n",
            "\n",
            "===== FP4 (FP32 Compute) ARC DETAILED MODEL METRICS =====\n",
            "------------------------------------------------------------\n",
            "PPL (Perplexity)                   : 8.3459\n",
            "Accuracy                           : 84.0000\n",
            "Memory Footprint (Model Size) (GB) : 1.8720\n",
            "Inference Latency (ms/token)       : 333.8665\n",
            "Avg GPU Utilization (%)            : 84.8052\n",
            "Avg GPU Memory Allocated (GB)      : 1.9410\n",
            "------------------------------------------------------------\n",
            "Starting cleanup for FP4 (FP32 Compute) ARC cell...\n",
            "FP4 model deleted.\n",
            "GPU cache cleared and garbage collected after FP4 (FP32 Compute) ARC run.\n"
          ]
        }
      ],
      "source": [
        "# --- FP4 (FP32 Compute) ARC-easy Benchmark Evaluation Cell --- # <<< Title updated\n",
        "\n",
        "# Assume 'handle' and 'stop_polling_event' exist from a previous cell.\n",
        "if 'handle' not in locals() or not handle:\n",
        "    print(\"Error: NVML handle not found. Please initialize NVML in a prior cell.\")\n",
        "    handle = None # Prevent polling\n",
        "elif 'stop_polling_event' not in locals():\n",
        "    print(\"Error: stop_polling_event not found. Please initialize in a prior cell.\")\n",
        "    # If needed, define fallback: stop_polling_event = threading.Event()\n",
        "\n",
        "# Clear previous readings and reset event\n",
        "gpu_utilization_readings = []\n",
        "if 'stop_polling_event' in locals():\n",
        "    stop_polling_event.clear()\n",
        "\n",
        "# Start GPU polling thread\n",
        "polling_thread = None\n",
        "if handle:\n",
        "    polling_thread = threading.Thread(target=poll_gpu_utilization, args=(handle, 1.0), daemon=True)\n",
        "    polling_thread.start()\n",
        "\n",
        "# Clean memory before starting\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "fp4_arc_metrics = {}          # <<< Variable renamed\n",
        "fp4_arc_results_data = {}     # <<< Variable renamed\n",
        "model_fp4 = None # Define outside try <<< Variable renamed\n",
        "\n",
        "try:\n",
        "    # Configure quantization to FP4\n",
        "    print(\"Configuring FP4 quantization (FP32 Compute)...\") # <<< Log updated\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"fp4\",             # <--- Set to FP4\n",
        "        bnb_4bit_compute_dtype=torch.float32, # <--- Set to FP32 compute\n",
        "        bnb_4bit_use_double_quant=True         # Or False depending on desired config\n",
        "    )\n",
        "    print(\"Quantization config created.\")\n",
        "\n",
        "    # Load quantized model (FP4)\n",
        "    model_name = \"Qwen/Qwen2.5-3B\" # Base model name\n",
        "    print(f\"Loading model: {model_name} (FP4 Quantized, FP32 Compute)...\") # <<< Log updated\n",
        "    model_fp4 = AutoModelForCausalLM.from_pretrained( # <<< Variable renamed\n",
        "        model_name,\n",
        "        quantization_config=bnb_config, # Use FP4 quantization config\n",
        "        device_map=\"cuda:0\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "    print(\"Model loaded.\")\n",
        "\n",
        "    # Load tokenizer\n",
        "    print(\"Loading tokenizer...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    print(\"Tokenizer loaded.\")\n",
        "\n",
        "    # --- No reset_peak_memory_stats needed for this metric set ---\n",
        "\n",
        "    # Run ARC evaluation using the evaluate_arc function\n",
        "    print(\"Starting ARC evaluation (FP4, FP32 Compute)...\") # <<< Log updated\n",
        "    # Assuming evaluate_arc can handle the quantized model and returns required keys\n",
        "    # Using max_new_tokens=1 for consistency with the format\n",
        "    fp4_arc_results_data = evaluate_arc(model_fp4, tokenizer, samples_200_arc_test, max_new_tokens=1) # <<< Variable renamed, max_new_tokens=1\n",
        "    print(\"ARC evaluation finished.\")\n",
        "\n",
        "    # Stop polling thread and calculate average utilization\n",
        "    avg_gpu_utilization = 'N/A'\n",
        "    if polling_thread and polling_thread.is_alive():\n",
        "        stop_polling_event.set()\n",
        "        polling_thread.join(timeout=5)\n",
        "        if gpu_utilization_readings:\n",
        "            avg_gpu_utilization = statistics.mean(gpu_utilization_readings)\n",
        "        else:\n",
        "             avg_gpu_utilization = 0 # Polling ran but got no readings\n",
        "    elif handle:\n",
        "        avg_gpu_utilization = 0 # Polling failed to start/run correctly\n",
        "\n",
        "    # Collect metrics in the MMLU format using results from evaluate_arc\n",
        "    accuracy_val = fp4_arc_results_data.get('accuracy', 'N/A') # <<< Use renamed variable\n",
        "    if isinstance(accuracy_val, (float, int)):\n",
        "        accuracy_val *= 100 # Multiply only if it's a number\n",
        "\n",
        "    fp4_arc_metrics = { # <<< Variable renamed\n",
        "        \"PPL (Perplexity)\": fp4_arc_results_data.get('perplexity', 'N/A'),\n",
        "        \"Accuracy\": accuracy_val,\n",
        "        \"Memory Footprint (Model Size) (GB)\": model_fp4.get_memory_footprint() / (1024 ** 3), # <<< Use renamed variable\n",
        "        \"Inference Latency (ms/token)\": fp4_arc_results_data.get('inference_latency', 'N/A'),\n",
        "        \"Avg GPU Utilization (%)\": avg_gpu_utilization, # From polling\n",
        "        \"Avg GPU Memory Allocated (GB)\": fp4_arc_results_data.get('avg_gpu_memory_gb', 'N/A'), # From evaluate_arc result\n",
        "    }\n",
        "\n",
        "    # Print metrics\n",
        "    print(\"\\n===== FP4 (FP32 Compute) ARC DETAILED MODEL METRICS =====\") # <<< Title updated\n",
        "    print(\"-\" * 60)\n",
        "    if fp4_arc_metrics: # <<< Use renamed variable\n",
        "        try:\n",
        "            max_key_length = max(len(key) for key in fp4_arc_metrics.keys()) # <<< Use renamed variable\n",
        "        except ValueError:\n",
        "             max_key_length = 35 # Default width\n",
        "\n",
        "        for key, value in fp4_arc_metrics.items(): # <<< Use renamed variable\n",
        "             if isinstance(value, (float, int)):\n",
        "                 print(f\"{key.ljust(max_key_length)} : {value:.4f}\")\n",
        "             else:\n",
        "                 print(f\"{key.ljust(max_key_length)} : {value}\") # Handles 'N/A'\n",
        "    else:\n",
        "        print(\"No metrics collected.\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "except pynvml.NVMLError as nvml_error:\n",
        "     print(f\"NVML Error during FP4 (FP32 Compute) ARC evaluation steps: {nvml_error}\") # <<< Log updated\n",
        "except Exception as e:\n",
        "    print(f\"FP4 (FP32 Compute) ARC Evaluation error: {e}\") # <<< Log updated\n",
        "    # Ensure polling stops on error\n",
        "    if polling_thread and polling_thread.is_alive():\n",
        "        stop_polling_event.set()\n",
        "        try:\n",
        "            polling_thread.join(timeout=5)\n",
        "        except Exception as join_e:\n",
        "            print(f\"Error stopping polling thread after exception: {join_e}\")\n",
        "\n",
        "finally:\n",
        "    # Clean up GPU memory (No NVML Shutdown)\n",
        "    print(\"Starting cleanup for FP4 (FP32 Compute) ARC cell...\") # <<< Log updated\n",
        "    if 'model_fp4' in locals() and model_fp4 is not None: # Check for model_fp4 <<< Use renamed variable\n",
        "        del model_fp4 # Delete model_fp4 <<< Use renamed variable\n",
        "        print(\"FP4 model deleted.\")\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    print(\"GPU cache cleared and garbage collected after FP4 (FP32 Compute) ARC run.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "id": "18f98LbqrBeF",
        "outputId": "768cf6dd-bd04-4f5a-dbd6-308e58a0a571"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuring FP4 quantization (FP16 Compute)...\n",
            "Quantization config created.\n",
            "Loading model: Qwen/Qwen2.5-3B (FP4 Quantized, FP16 Compute)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "82851831ad8446b5b12fb37277c31a05"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded.\n",
            "Loading tokenizer...\n",
            "Tokenizer loaded.\n",
            "Starting ARC evaluation (FP4, FP16 Compute)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating ARC:   0%|          | 0/200 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "10d1562ea5c64641a7486509f533286a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ARC evaluation finished.\n",
            "\n",
            "===== FP4 (FP16 Compute) ARC DETAILED MODEL METRICS =====\n",
            "------------------------------------------------------------\n",
            "PPL (Perplexity)                   : 8.3389\n",
            "Accuracy                           : 84.0000\n",
            "Memory Footprint (Model Size) (GB) : 1.8720\n",
            "Inference Latency (ms/token)       : 193.9434\n",
            "Avg GPU Utilization (%)            : 38.7292\n",
            "Avg GPU Memory Allocated (GB)      : 1.9410\n",
            "------------------------------------------------------------\n",
            "Starting cleanup for FP4 (FP16 Compute) ARC cell...\n",
            "FP4 model deleted.\n",
            "GPU cache cleared and garbage collected after FP4 (FP16 Compute) ARC run.\n"
          ]
        }
      ],
      "source": [
        "# --- FP4 (FP16 Compute) ARC-easy Benchmark Evaluation Cell --- # <<< Title updated\n",
        "\n",
        "# Assume 'handle' and 'stop_polling_event' exist from a previous cell.\n",
        "if 'handle' not in locals() or not handle:\n",
        "    print(\"Error: NVML handle not found. Please initialize NVML in a prior cell.\")\n",
        "    handle = None # Prevent polling\n",
        "elif 'stop_polling_event' not in locals():\n",
        "    print(\"Error: stop_polling_event not found. Please initialize in a prior cell.\")\n",
        "    # If needed, define fallback: stop_polling_event = threading.Event()\n",
        "\n",
        "# Clear previous readings and reset event\n",
        "gpu_utilization_readings = []\n",
        "if 'stop_polling_event' in locals():\n",
        "    stop_polling_event.clear()\n",
        "\n",
        "# Start GPU polling thread\n",
        "polling_thread = None\n",
        "if handle:\n",
        "    polling_thread = threading.Thread(target=poll_gpu_utilization, args=(handle, 1.0), daemon=True)\n",
        "    polling_thread.start()\n",
        "\n",
        "# Clean memory before starting\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "fp4_arc_metrics = {}          # <<< Variable renamed\n",
        "fp4_arc_results_data = {}     # <<< Variable renamed\n",
        "model_fp4 = None # Define outside try <<< Variable renamed\n",
        "\n",
        "try:\n",
        "    # Configure quantization to FP4 with FP16 compute\n",
        "    print(\"Configuring FP4 quantization (FP16 Compute)...\") # <<< Log updated\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"fp4\",             # <--- Set to FP4\n",
        "        bnb_4bit_compute_dtype=torch.float16, # <--- Set to FP16 compute\n",
        "        bnb_4bit_use_double_quant=True         # Or False depending on desired config\n",
        "    )\n",
        "    print(\"Quantization config created.\")\n",
        "\n",
        "    # Load quantized model (FP4)\n",
        "    model_name = \"Qwen/Qwen2.5-3B\" # Base model name\n",
        "    print(f\"Loading model: {model_name} (FP4 Quantized, FP16 Compute)...\") # <<< Log updated\n",
        "    model_fp4 = AutoModelForCausalLM.from_pretrained( # <<< Variable renamed\n",
        "        model_name,\n",
        "        quantization_config=bnb_config, # Use FP4 quantization config\n",
        "        device_map=\"cuda:0\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "    print(\"Model loaded.\")\n",
        "    # --- Removed loading of separate FP16 model ---\n",
        "\n",
        "    # Load tokenizer\n",
        "    print(\"Loading tokenizer...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    print(\"Tokenizer loaded.\")\n",
        "\n",
        "    # --- No reset_peak_memory_stats needed for this metric set ---\n",
        "\n",
        "    # Run ARC evaluation using the evaluate_arc function\n",
        "    print(\"Starting ARC evaluation (FP4, FP16 Compute)...\") # <<< Log updated\n",
        "    # Assuming evaluate_arc can handle the quantized model and returns required keys\n",
        "    # Using max_new_tokens=1 for consistency with the format\n",
        "    fp4_arc_results_data = evaluate_arc(model_fp4, tokenizer, samples_200_arc_test, max_new_tokens=1) # <<< Variable renamed, max_new_tokens=1\n",
        "    print(\"ARC evaluation finished.\")\n",
        "\n",
        "    # Stop polling thread and calculate average utilization\n",
        "    avg_gpu_utilization = 'N/A'\n",
        "    if polling_thread and polling_thread.is_alive():\n",
        "        stop_polling_event.set()\n",
        "        polling_thread.join(timeout=5)\n",
        "        if gpu_utilization_readings:\n",
        "            avg_gpu_utilization = statistics.mean(gpu_utilization_readings)\n",
        "        else:\n",
        "             avg_gpu_utilization = 0 # Polling ran but got no readings\n",
        "    elif handle:\n",
        "        avg_gpu_utilization = 0 # Polling failed to start/run correctly\n",
        "\n",
        "    # Collect metrics in the MMLU format using results from evaluate_arc\n",
        "    accuracy_val = fp4_arc_results_data.get('accuracy', 'N/A') # <<< Use renamed variable\n",
        "    if isinstance(accuracy_val, (float, int)):\n",
        "        accuracy_val *= 100 # Multiply only if it's a number\n",
        "\n",
        "    fp4_arc_metrics = { # <<< Variable renamed\n",
        "        \"PPL (Perplexity)\": fp4_arc_results_data.get('perplexity', 'N/A'),\n",
        "        \"Accuracy\": accuracy_val,\n",
        "        \"Memory Footprint (Model Size) (GB)\": model_fp4.get_memory_footprint() / (1024 ** 3), # <<< Use renamed variable\n",
        "        \"Inference Latency (ms/token)\": fp4_arc_results_data.get('inference_latency', 'N/A'),\n",
        "        \"Avg GPU Utilization (%)\": avg_gpu_utilization, # From polling\n",
        "        \"Avg GPU Memory Allocated (GB)\": fp4_arc_results_data.get('avg_gpu_memory_gb', 'N/A'), # From evaluate_arc result\n",
        "    }\n",
        "\n",
        "    # Print metrics\n",
        "    print(\"\\n===== FP4 (FP16 Compute) ARC DETAILED MODEL METRICS =====\") # <<< Title updated\n",
        "    print(\"-\" * 60)\n",
        "    if fp4_arc_metrics: # <<< Use renamed variable\n",
        "        try:\n",
        "            max_key_length = max(len(key) for key in fp4_arc_metrics.keys()) # <<< Use renamed variable\n",
        "        except ValueError:\n",
        "             max_key_length = 35 # Default width\n",
        "\n",
        "        for key, value in fp4_arc_metrics.items(): # <<< Use renamed variable\n",
        "             if isinstance(value, (float, int)):\n",
        "                 print(f\"{key.ljust(max_key_length)} : {value:.4f}\")\n",
        "             else:\n",
        "                 print(f\"{key.ljust(max_key_length)} : {value}\") # Handles 'N/A'\n",
        "    else:\n",
        "        print(\"No metrics collected.\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "except pynvml.NVMLError as nvml_error:\n",
        "     print(f\"NVML Error during FP4 (FP16 Compute) ARC evaluation steps: {nvml_error}\") # <<< Log updated\n",
        "except Exception as e:\n",
        "    print(f\"FP4 (FP16 Compute) ARC Evaluation error: {e}\") # <<< Log updated\n",
        "    # Ensure polling stops on error\n",
        "    if polling_thread and polling_thread.is_alive():\n",
        "        stop_polling_event.set()\n",
        "        try:\n",
        "            polling_thread.join(timeout=5)\n",
        "        except Exception as join_e:\n",
        "            print(f\"Error stopping polling thread after exception: {join_e}\")\n",
        "\n",
        "finally:\n",
        "    # Clean up GPU memory (No NVML Shutdown)\n",
        "    print(\"Starting cleanup for FP4 (FP16 Compute) ARC cell...\") # <<< Log updated\n",
        "    if 'model_fp4' in locals() and model_fp4 is not None: # Check for model_fp4 <<< Use renamed variable\n",
        "        del model_fp4 # Delete model_fp4 <<< Use renamed variable\n",
        "        print(\"FP4 model deleted.\")\n",
        "    # --- Removed deletion of separate model_fp16 ---\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    print(\"GPU cache cleared and garbage collected after FP4 (FP16 Compute) ARC run.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "PWeaZPd2cYaz",
        "outputId": "ca496138-2b55-43a7-afce-2efc5dc94f57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuring INT8 quantization (via load_in_8bit=True)...\n",
            "Loading model: Qwen/Qwen2.5-3B (INT8 Quantized)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ada91bc3a80e43df9d078511c62a732e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded.\n",
            "Loading tokenizer...\n",
            "Tokenizer loaded.\n",
            "Starting ARC evaluation (INT8, FP32 Compute)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating ARC:   0%|          | 0/200 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "171ad7310c9844e6b15cba295410165b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ARC evaluation finished.\n",
            "\n",
            "===== INT8 (FP32 Compute) ARC DETAILED MODEL METRICS =====\n",
            "------------------------------------------------------------\n",
            "PPL (Perplexity)                   : 6.4720\n",
            "Accuracy                           : 89.5000\n",
            "Memory Footprint (Model Size) (GB) : 3.1640\n",
            "Inference Latency (ms/token)       : 397.7153\n",
            "Avg GPU Utilization (%)            : 18.9773\n",
            "Avg GPU Memory Allocated (GB)      : 3.2701\n",
            "------------------------------------------------------------\n",
            "Starting cleanup for INT8 (FP32 Compute) ARC cell...\n",
            "INT8 model deleted.\n",
            "GPU cache cleared and garbage collected after INT8 (FP32 Compute) ARC run.\n"
          ]
        }
      ],
      "source": [
        "# --- INT8 (FP32 Compute) ARC-easy Benchmark Evaluation Cell --- # <<< Title updated\n",
        "\n",
        "# Assume 'handle' and 'stop_polling_event' exist from a previous cell.\n",
        "if 'handle' not in locals() or not handle:\n",
        "    print(\"Error: NVML handle not found. Please initialize NVML in a prior cell.\")\n",
        "    handle = None # Prevent polling\n",
        "elif 'stop_polling_event' not in locals():\n",
        "    print(\"Error: stop_polling_event not found. Please initialize in a prior cell.\")\n",
        "    # If needed, define fallback: stop_polling_event = threading.Event()\n",
        "\n",
        "# Clear previous readings and reset event\n",
        "gpu_utilization_readings = []\n",
        "if 'stop_polling_event' in locals():\n",
        "    stop_polling_event.clear()\n",
        "\n",
        "# Start GPU polling thread\n",
        "polling_thread = None\n",
        "if handle:\n",
        "    polling_thread = threading.Thread(target=poll_gpu_utilization, args=(handle, 1.0), daemon=True)\n",
        "    polling_thread.start()\n",
        "\n",
        "# Clean memory before starting\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "int8_arc_metrics = {}          # <<< Variable renamed\n",
        "int8_arc_results_data = {}     # <<< Variable renamed\n",
        "model_int8 = None # Define outside try <<< Variable renamed\n",
        "\n",
        "try:\n",
        "    # --- INT8 Configuration Note ---\n",
        "    # For BitsAndBytes INT8, we pass parameters directly to from_pretrained\n",
        "    # No separate BitsAndBytesConfig object is typically used like for 4-bit.\n",
        "    print(\"Configuring INT8 quantization (via load_in_8bit=True)...\")\n",
        "\n",
        "    # Load quantized model (INT8)\n",
        "    model_name = \"Qwen/Qwen2.5-3B\" # Base model name\n",
        "    print(f\"Loading model: {model_name} (INT8 Quantized)...\") # <<< Log updated\n",
        "    model_int8 = AutoModelForCausalLM.from_pretrained( # <<< Variable renamed\n",
        "        model_name,\n",
        "        load_in_8bit=True,       # <--- Key parameter for INT8\n",
        "        device_map=\"cuda:0\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "    print(\"Model loaded.\")\n",
        "\n",
        "    # Load tokenizer\n",
        "    print(\"Loading tokenizer...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    print(\"Tokenizer loaded.\")\n",
        "\n",
        "    # --- No reset_peak_memory_stats needed for this metric set ---\n",
        "\n",
        "    # Run ARC evaluation using the evaluate_arc function\n",
        "    print(\"Starting ARC evaluation (INT8, FP32 Compute)...\") # <<< Log updated (Assuming FP32 compute default)\n",
        "    # Assuming evaluate_arc can handle the quantized model and returns required keys\n",
        "    # Using max_new_tokens=1 for consistency with the format\n",
        "    int8_arc_results_data = evaluate_arc(model_int8, tokenizer, samples_200_arc_test, max_new_tokens=1) # <<< Variable renamed, max_new_tokens=1\n",
        "    print(\"ARC evaluation finished.\")\n",
        "\n",
        "    # Stop polling thread and calculate average utilization\n",
        "    avg_gpu_utilization = 'N/A'\n",
        "    if polling_thread and polling_thread.is_alive():\n",
        "        stop_polling_event.set()\n",
        "        polling_thread.join(timeout=5)\n",
        "        if gpu_utilization_readings:\n",
        "            avg_gpu_utilization = statistics.mean(gpu_utilization_readings)\n",
        "        else:\n",
        "             avg_gpu_utilization = 0 # Polling ran but got no readings\n",
        "    elif handle:\n",
        "        avg_gpu_utilization = 0 # Polling failed to start/run correctly\n",
        "\n",
        "    # Collect metrics in the MMLU format using results from evaluate_arc\n",
        "    accuracy_val = int8_arc_results_data.get('accuracy', 'N/A') # <<< Use renamed variable\n",
        "    if isinstance(accuracy_val, (float, int)):\n",
        "        accuracy_val *= 100 # Multiply only if it's a number\n",
        "\n",
        "    int8_arc_metrics = { # <<< Variable renamed\n",
        "        \"PPL (Perplexity)\": int8_arc_results_data.get('perplexity', 'N/A'),\n",
        "        \"Accuracy\": accuracy_val,\n",
        "        # get_memory_footprint might be less accurate for 8-bit via load_in_8bit\n",
        "        # Consider alternative measurement if needed, but using standard call for consistency\n",
        "        \"Memory Footprint (Model Size) (GB)\": model_int8.get_memory_footprint() / (1024 ** 3), # <<< Use renamed variable\n",
        "        \"Inference Latency (ms/token)\": int8_arc_results_data.get('inference_latency', 'N/A'),\n",
        "        \"Avg GPU Utilization (%)\": avg_gpu_utilization, # From polling\n",
        "        \"Avg GPU Memory Allocated (GB)\": int8_arc_results_data.get('avg_gpu_memory_gb', 'N/A'), # From evaluate_arc result\n",
        "    }\n",
        "\n",
        "    # Print metrics\n",
        "    print(\"\\n===== INT8 (FP32 Compute) ARC DETAILED MODEL METRICS =====\") # <<< Title updated\n",
        "    print(\"-\" * 60)\n",
        "    if int8_arc_metrics: # <<< Use renamed variable\n",
        "        try:\n",
        "            max_key_length = max(len(key) for key in int8_arc_metrics.keys()) # <<< Use renamed variable\n",
        "        except ValueError:\n",
        "             max_key_length = 35 # Default width\n",
        "\n",
        "        for key, value in int8_arc_metrics.items(): # <<< Use renamed variable\n",
        "             if isinstance(value, (float, int)):\n",
        "                 print(f\"{key.ljust(max_key_length)} : {value:.4f}\")\n",
        "             else:\n",
        "                 print(f\"{key.ljust(max_key_length)} : {value}\") # Handles 'N/A'\n",
        "    else:\n",
        "        print(\"No metrics collected.\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "except pynvml.NVMLError as nvml_error:\n",
        "     print(f\"NVML Error during INT8 (FP32 Compute) ARC evaluation steps: {nvml_error}\") # <<< Log updated\n",
        "except Exception as e:\n",
        "    print(f\"INT8 (FP32 Compute) ARC Evaluation error: {e}\") # <<< Log updated\n",
        "    # Ensure polling stops on error\n",
        "    # --- Removed traceback import and print ---\n",
        "    if polling_thread and polling_thread.is_alive():\n",
        "        stop_polling_event.set()\n",
        "        try:\n",
        "            polling_thread.join(timeout=5)\n",
        "        except Exception as join_e:\n",
        "            print(f\"Error stopping polling thread after exception: {join_e}\")\n",
        "\n",
        "finally:\n",
        "    # Clean up GPU memory (No NVML Shutdown)\n",
        "    print(\"Starting cleanup for INT8 (FP32 Compute) ARC cell...\") # <<< Log updated\n",
        "    if 'model_int8' in locals() and model_int8 is not None: # Check for model_int8 <<< Use renamed variable\n",
        "        del model_int8 # Delete model_int8 <<< Use renamed variable\n",
        "        print(\"INT8 model deleted.\")\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    print(\"GPU cache cleared and garbage collected after INT8 (FP32 Compute) ARC run.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "id": "eqQuqFvYgjlJ",
        "outputId": "ffdf6fa6-f47c-4241-d14b-351016649ef7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuring INT8 quantization (FP16 Compute)...\n",
            "Quantization config created.\n",
            "Loading model: Qwen/Qwen2.5-3B (INT8 Quantized, FP16 Compute)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3a04bb1b954d439c8ccc63d36bb7bc7f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded.\n",
            "Loading tokenizer...\n",
            "Tokenizer loaded.\n",
            "Starting ARC evaluation (INT8, FP16 Compute)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating ARC:   0%|          | 0/200 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "34b0d0731aa74319bceb3f3fa351552d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ARC evaluation finished.\n",
            "\n",
            "===== INT8 (FP16 Compute) ARC DETAILED MODEL METRICS =====\n",
            "------------------------------------------------------------\n",
            "PPL (Perplexity)                   : 6.4720\n",
            "Accuracy                           : 89.5000\n",
            "Memory Footprint (Model Size) (GB) : 3.1640\n",
            "Inference Latency (ms/token)       : 398.0718\n",
            "Avg GPU Utilization (%)            : 19.0000\n",
            "Avg GPU Memory Allocated (GB)      : 3.2321\n",
            "------------------------------------------------------------\n",
            "Starting cleanup for INT8 (FP16 Compute) ARC cell...\n",
            "INT8 model deleted.\n",
            "GPU cache cleared and garbage collected after INT8 (FP16 Compute) ARC run.\n"
          ]
        }
      ],
      "source": [
        "# --- INT8 (FP16 Compute) ARC-easy Benchmark Evaluation Cell --- # <<< Title updated\n",
        "\n",
        "# Assume 'handle' and 'stop_polling_event' exist from a previous cell.\n",
        "if 'handle' not in locals() or not handle:\n",
        "    print(\"Error: NVML handle not found. Please initialize NVML in a prior cell.\")\n",
        "    handle = None # Prevent polling\n",
        "elif 'stop_polling_event' not in locals():\n",
        "    print(\"Error: stop_polling_event not found. Please initialize in a prior cell.\")\n",
        "    # If needed, define fallback: stop_polling_event = threading.Event()\n",
        "\n",
        "# Clear previous readings and reset event\n",
        "gpu_utilization_readings = []\n",
        "if 'stop_polling_event' in locals():\n",
        "    stop_polling_event.clear()\n",
        "\n",
        "# Start GPU polling thread\n",
        "polling_thread = None\n",
        "if handle:\n",
        "    polling_thread = threading.Thread(target=poll_gpu_utilization, args=(handle, 1.0), daemon=True)\n",
        "    polling_thread.start()\n",
        "\n",
        "# Clean memory before starting\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "int8_arc_metrics = {}          # <<< Variable renamed\n",
        "int8_arc_results_data = {}     # <<< Variable renamed\n",
        "model_int8 = None # Define outside try <<< Variable renamed\n",
        "\n",
        "try:\n",
        "    # Configure quantization to INT8 with FP16 compute\n",
        "    print(\"Configuring INT8 quantization (FP16 Compute)...\") # <<< Log updated\n",
        "    # Note: For INT8 with a specific compute dtype (like FP16),\n",
        "    # we *do* use BitsAndBytesConfig unlike the simple load_in_8bit case.\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_8bit=True,\n",
        "        # bnb_8bit_quant_type=\"int8\", # Often implicit or not needed when load_in_8bit=True\n",
        "        bnb_8bit_compute_dtype=torch.float16, # <--- Specify FP16 compute\n",
        "        # bnb_8bit_use_double_quant might not be applicable/needed for int8\n",
        "    )\n",
        "    print(\"Quantization config created.\")\n",
        "\n",
        "\n",
        "    # Load quantized model (INT8)\n",
        "    model_name = \"Qwen/Qwen2.5-3B\" # Base model name\n",
        "    print(f\"Loading model: {model_name} (INT8 Quantized, FP16 Compute)...\") # <<< Log updated\n",
        "    model_int8 = AutoModelForCausalLM.from_pretrained( # <<< Variable renamed\n",
        "        model_name,\n",
        "        quantization_config=bnb_config, # <--- Pass the config for FP16 compute\n",
        "        device_map=\"cuda:0\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "    print(\"Model loaded.\")\n",
        "\n",
        "    # Load tokenizer\n",
        "    print(\"Loading tokenizer...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    print(\"Tokenizer loaded.\")\n",
        "\n",
        "    # --- No reset_peak_memory_stats needed for this metric set ---\n",
        "\n",
        "    # Run ARC evaluation using the evaluate_arc function\n",
        "    print(\"Starting ARC evaluation (INT8, FP16 Compute)...\") # <<< Log updated\n",
        "    # Assuming evaluate_arc can handle the quantized model and returns required keys\n",
        "    # Using max_new_tokens=1 for consistency with the format\n",
        "    int8_arc_results_data = evaluate_arc(model_int8, tokenizer, samples_200_arc_test, max_new_tokens=1) # <<< Variable renamed, max_new_tokens=1\n",
        "    print(\"ARC evaluation finished.\")\n",
        "\n",
        "    # Stop polling thread and calculate average utilization\n",
        "    avg_gpu_utilization = 'N/A'\n",
        "    if polling_thread and polling_thread.is_alive():\n",
        "        stop_polling_event.set()\n",
        "        polling_thread.join(timeout=5)\n",
        "        if gpu_utilization_readings:\n",
        "            avg_gpu_utilization = statistics.mean(gpu_utilization_readings)\n",
        "        else:\n",
        "             avg_gpu_utilization = 0 # Polling ran but got no readings\n",
        "    elif handle:\n",
        "        avg_gpu_utilization = 0 # Polling failed to start/run correctly\n",
        "\n",
        "    # Collect metrics in the MMLU format using results from evaluate_arc\n",
        "    accuracy_val = int8_arc_results_data.get('accuracy', 'N/A') # <<< Use renamed variable\n",
        "    if isinstance(accuracy_val, (float, int)):\n",
        "        accuracy_val *= 100 # Multiply only if it's a number\n",
        "\n",
        "    int8_arc_metrics = { # <<< Variable renamed\n",
        "        \"PPL (Perplexity)\": int8_arc_results_data.get('perplexity', 'N/A'),\n",
        "        \"Accuracy\": accuracy_val,\n",
        "        \"Memory Footprint (Model Size) (GB)\": model_int8.get_memory_footprint() / (1024 ** 3), # <<< Use renamed variable\n",
        "        \"Inference Latency (ms/token)\": int8_arc_results_data.get('inference_latency', 'N/A'),\n",
        "        \"Avg GPU Utilization (%)\": avg_gpu_utilization, # From polling\n",
        "        \"Avg GPU Memory Allocated (GB)\": int8_arc_results_data.get('avg_gpu_memory_gb', 'N/A'), # From evaluate_arc result\n",
        "    }\n",
        "\n",
        "    # Print metrics\n",
        "    print(\"\\n===== INT8 (FP16 Compute) ARC DETAILED MODEL METRICS =====\") # <<< Title updated\n",
        "    print(\"-\" * 60)\n",
        "    if int8_arc_metrics: # <<< Use renamed variable\n",
        "        try:\n",
        "            max_key_length = max(len(key) for key in int8_arc_metrics.keys()) # <<< Use renamed variable\n",
        "        except ValueError:\n",
        "             max_key_length = 35 # Default width\n",
        "\n",
        "        for key, value in int8_arc_metrics.items(): # <<< Use renamed variable\n",
        "             if isinstance(value, (float, int)):\n",
        "                 print(f\"{key.ljust(max_key_length)} : {value:.4f}\")\n",
        "             else:\n",
        "                 print(f\"{key.ljust(max_key_length)} : {value}\") # Handles 'N/A'\n",
        "    else:\n",
        "        print(\"No metrics collected.\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "except pynvml.NVMLError as nvml_error:\n",
        "     print(f\"NVML Error during INT8 (FP16 Compute) ARC evaluation steps: {nvml_error}\") # <<< Log updated\n",
        "except Exception as e:\n",
        "    print(f\"INT8 (FP16 Compute) ARC Evaluation error: {e}\") # <<< Log updated\n",
        "    # Ensure polling stops on error\n",
        "    # --- Removed traceback import and print ---\n",
        "    if polling_thread and polling_thread.is_alive():\n",
        "        stop_polling_event.set()\n",
        "        try:\n",
        "            polling_thread.join(timeout=5)\n",
        "        except Exception as join_e:\n",
        "            print(f\"Error stopping polling thread after exception: {join_e}\")\n",
        "\n",
        "finally:\n",
        "    # Clean up GPU memory (No NVML Shutdown)\n",
        "    print(\"Starting cleanup for INT8 (FP16 Compute) ARC cell...\") # <<< Log updated\n",
        "    if 'model_int8' in locals() and model_int8 is not None: # Check for model_int8 <<< Use renamed variable\n",
        "        del model_int8 # Delete model_int8 <<< Use renamed variable\n",
        "        print(\"INT8 model deleted.\")\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    print(\"GPU cache cleared and garbage collected after INT8 (FP16 Compute) ARC run.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhEO7gf8yYzu"
      },
      "source": [
        "## ARC_challenge Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "collapsed": true,
        "id": "kjxis2KGyosm",
        "outputId": "bb89a775-e617-48b0-ec73-64d7cb140385"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00000-of-00001.parquet:   0%|          | 0.00/190k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a69ee08306dd4d7ba4f9ba901c661b2e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "test-00000-of-00001.parquet:   0%|          | 0.00/204k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5d105d1765654778a03c51a3c9fa7d7a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "validation-00000-of-00001.parquet:   0%|          | 0.00/55.7k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7431b68ed318422a80e68058ad3f3f12"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/1119 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8cbaa5d9be4447a8908e100f2a877179"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/1172 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1e2c9fb090c74914888fc9edd5adfcc7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating validation split:   0%|          | 0/299 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f7ad34bc43e14b0390b9a7536e27b8bc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------\n",
            "Test set size: 2376\n",
            "----------------------\n",
            "Dataset({\n",
            "    features: ['id', 'question', 'choices', 'answerKey'],\n",
            "    num_rows: 2376\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "# Load ARC test dataset\n",
        "from datasets import load_dataset\n",
        "arc_challenge_test = load_dataset(\"allenai/ai2_arc\", \"ARC-Challenge\", split=\"test\")\n",
        "\n",
        "print('----------------------')\n",
        "print(f\"Test set size: {len(arc_test)}\")\n",
        "print('----------------------')\n",
        "print(arc_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aFSy1g_Iyosn"
      },
      "outputs": [],
      "source": [
        "# Set a seed for reproducibility\n",
        "random.seed(42)\n",
        "\n",
        "# Take a random sample of 200 from the test dataset\n",
        "samples_200_arc_challenge_test = random.sample(list(arc_test), 200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "vWNAfgAAyoso",
        "outputId": "db68dd6e-daaf-481b-d138-5ac48a5bd007"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model: Qwen/Qwen2.5-3B (FP32)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "001a5886d8c7403eb591d1e1e359da24"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded.\n",
            "Loading tokenizer...\n",
            "Tokenizer loaded.\n",
            "Starting ARC-Challenge evaluation (FP32)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating ARC:   0%|          | 0/200 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "24fa5acb6e1b443ea3cdcac0899d3cf1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ARC-Challenge evaluation finished.\n",
            "\n",
            "===== FP32 ARC-Challenge DETAILED MODEL METRICS =====\n",
            "------------------------------------------------------------\n",
            "PPL (Perplexity)                   : 6.4848\n",
            "Accuracy                           : 90.5000\n",
            "Memory Footprint (Model Size) (GB) : 11.4960\n",
            "Inference Latency (ms/token)       : 175.8868\n",
            "Avg GPU Utilization (%)            : 85.2000\n",
            "Avg GPU Memory Allocated (GB)      : 11.5528\n",
            "------------------------------------------------------------\n",
            "Starting cleanup for FP32 ARC-Challenge cell...\n",
            "FP32 model deleted.\n",
            "GPU cache cleared and garbage collected after FP32 ARC-Challenge run.\n"
          ]
        }
      ],
      "source": [
        "# --- FP32 ARC-Challenge Benchmark Evaluation Cell --- # <<< Title updated for ARC-Challenge\n",
        "\n",
        "# Assume 'handle' and 'stop_polling_event' exist from a previous cell.\n",
        "if 'handle' not in locals() or not handle:\n",
        "    print(\"Error: NVML handle not found. Please initialize NVML in a prior cell.\")\n",
        "    handle = None # Prevent polling\n",
        "elif 'stop_polling_event' not in locals():\n",
        "    print(\"Error: stop_polling_event not found. Please initialize in a prior cell.\")\n",
        "    # If needed, define fallback: stop_polling_event = threading.Event()\n",
        "\n",
        "# Clear previous readings and reset event\n",
        "gpu_utilization_readings = []\n",
        "if 'stop_polling_event' in locals():\n",
        "    stop_polling_event.clear()\n",
        "\n",
        "# Start GPU polling thread\n",
        "polling_thread = None\n",
        "if handle:\n",
        "    polling_thread = threading.Thread(target=poll_gpu_utilization, args=(handle, 1.0), daemon=True)\n",
        "    polling_thread.start()\n",
        "\n",
        "# Clean memory before starting\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "fp32_arc_metrics = {}          # <<< Variable renamed\n",
        "fp32_arc_results_data = {}     # <<< Variable renamed\n",
        "model_fp32 = None # Define outside try <<< Variable renamed\n",
        "\n",
        "try:\n",
        "    # Load full precision model (FP32)\n",
        "    model_name = \"Qwen/Qwen2.5-3B\" # Base model name\n",
        "    print(f\"Loading model: {model_name} (FP32)...\") # <<< Log updated\n",
        "    model_fp32 = AutoModelForCausalLM.from_pretrained( # <<< Variable renamed\n",
        "        model_name,\n",
        "        torch_dtype=torch.float32,      # <<< Explicitly FP32\n",
        "        device_map=\"cuda:0\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "    print(\"Model loaded.\")\n",
        "\n",
        "    # Load tokenizer\n",
        "    print(\"Loading tokenizer...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    print(\"Tokenizer loaded.\")\n",
        "\n",
        "    # --- No reset_peak_memory_stats needed for this metric set ---\n",
        "\n",
        "    # Run ARC-Challenge evaluation using the evaluate_arc function\n",
        "    print(\"Starting ARC-Challenge evaluation (FP32)...\") # <<< Log updated\n",
        "    # Using the correct ARC-Challenge dataset variable\n",
        "    # Assuming evaluate_arc returns required keys\n",
        "    fp32_arc_results_data = evaluate_arc(model_fp32, tokenizer, samples_200_arc_challenge_test, max_new_tokens=1) # <<< Variable renamed, dataset var checked, max_new_tokens=1\n",
        "    print(\"ARC-Challenge evaluation finished.\") # <<< Log updated\n",
        "\n",
        "    # Stop polling thread and calculate average utilization\n",
        "    avg_gpu_utilization = 'N/A'\n",
        "    if polling_thread and polling_thread.is_alive():\n",
        "        stop_polling_event.set()\n",
        "        polling_thread.join(timeout=5)\n",
        "        if gpu_utilization_readings:\n",
        "            avg_gpu_utilization = statistics.mean(gpu_utilization_readings)\n",
        "        else:\n",
        "             avg_gpu_utilization = 0 # Polling ran but got no readings\n",
        "    elif handle:\n",
        "        avg_gpu_utilization = 0 # Polling failed to start/run correctly\n",
        "\n",
        "    # Collect metrics in the MMLU format using results from evaluate_arc\n",
        "    accuracy_val = fp32_arc_results_data.get('accuracy', 'N/A') # <<< Use renamed variable\n",
        "    if isinstance(accuracy_val, (float, int)):\n",
        "        accuracy_val *= 100 # Multiply only if it's a number\n",
        "\n",
        "    fp32_arc_metrics = { # <<< Variable renamed\n",
        "        \"PPL (Perplexity)\": fp32_arc_results_data.get('perplexity', 'N/A'),\n",
        "        \"Accuracy\": accuracy_val,\n",
        "        \"Memory Footprint (Model Size) (GB)\": model_fp32.get_memory_footprint() / (1024 ** 3), # <<< Use renamed variable\n",
        "        \"Inference Latency (ms/token)\": fp32_arc_results_data.get('inference_latency', 'N/A'),\n",
        "        \"Avg GPU Utilization (%)\": avg_gpu_utilization, # From polling\n",
        "        \"Avg GPU Memory Allocated (GB)\": fp32_arc_results_data.get('avg_gpu_memory_gb', 'N/A'), # From evaluate_arc result\n",
        "    }\n",
        "\n",
        "    # Print metrics\n",
        "    print(\"\\n===== FP32 ARC-Challenge DETAILED MODEL METRICS =====\") # <<< Title updated\n",
        "    print(\"-\" * 60)\n",
        "    if fp32_arc_metrics: # <<< Use renamed variable\n",
        "        try:\n",
        "            max_key_length = max(len(key) for key in fp32_arc_metrics.keys()) # <<< Use renamed variable\n",
        "        except ValueError:\n",
        "             max_key_length = 35 # Default width\n",
        "\n",
        "        for key, value in fp32_arc_metrics.items(): # <<< Use renamed variable\n",
        "             if isinstance(value, (float, int)):\n",
        "                 print(f\"{key.ljust(max_key_length)} : {value:.4f}\")\n",
        "             else:\n",
        "                 print(f\"{key.ljust(max_key_length)} : {value}\") # Handles 'N/A'\n",
        "    else:\n",
        "        print(\"No metrics collected.\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "except pynvml.NVMLError as nvml_error:\n",
        "     print(f\"NVML Error during FP32 ARC-Challenge evaluation steps: {nvml_error}\") # <<< Log updated\n",
        "except Exception as e:\n",
        "    print(f\"FP32 ARC-Challenge Evaluation error: {e}\") # <<< Log updated\n",
        "    # Ensure polling stops on error\n",
        "    if polling_thread and polling_thread.is_alive():\n",
        "        stop_polling_event.set()\n",
        "        try:\n",
        "            polling_thread.join(timeout=5)\n",
        "        except Exception as join_e:\n",
        "            print(f\"Error stopping polling thread after exception: {join_e}\")\n",
        "\n",
        "finally:\n",
        "    # Clean up GPU memory (No NVML Shutdown)\n",
        "    print(\"Starting cleanup for FP32 ARC-Challenge cell...\") # <<< Log updated\n",
        "    if 'model_fp32' in locals() and model_fp32 is not None: # Check for model_fp32 <<< Use renamed variable\n",
        "        del model_fp32 # Delete model_fp32 <<< Use renamed variable\n",
        "        print(\"FP32 model deleted.\")\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    print(\"GPU cache cleared and garbage collected after FP32 ARC-Challenge run.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "LnnMGRwSyoso",
        "outputId": "b63784cc-3aa3-439d-a136-04406de58cb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model: Qwen/Qwen2.5-3B (FP16)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3e238ad5d73245efa2e88d9242df4787"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded.\n",
            "Loading tokenizer...\n",
            "Tokenizer loaded.\n",
            "Starting ARC-Challenge evaluation (FP16)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating ARC:   0%|          | 0/200 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9c5a024747fd4af6a2e1d2c1dfc9d10c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ARC-Challenge evaluation finished.\n",
            "\n",
            "===== FP16 ARC-Challenge DETAILED MODEL METRICS (MMLU Format) =====\n",
            "------------------------------------------------------------\n",
            "PPL (Perplexity)                   : 6.4846\n",
            "Accuracy                           : 90.5000\n",
            "Memory Footprint (Model Size) (GB) : 5.7480\n",
            "Inference Latency (ms/token)       : 104.6681\n",
            "Avg GPU Utilization (%)            : 58.5417\n",
            "Avg GPU Memory Allocated (GB)      : 5.7754\n",
            "------------------------------------------------------------\n",
            "Starting cleanup for FP16 ARC-Challenge cell...\n",
            "FP16 model deleted.\n",
            "GPU cache cleared and garbage collected after FP16 ARC-Challenge run.\n"
          ]
        }
      ],
      "source": [
        "# --- FP16 ARC-Challenge Benchmark Evaluation Cell --- # <<< Title updated for FP16 & ARC-Challenge\n",
        "\n",
        "# Assume 'handle' and 'stop_polling_event' exist from a previous cell.\n",
        "if 'handle' not in locals() or not handle:\n",
        "    print(\"Error: NVML handle not found. Please initialize NVML in a prior cell.\")\n",
        "    handle = None # Prevent polling\n",
        "elif 'stop_polling_event' not in locals():\n",
        "    print(\"Error: stop_polling_event not found. Please initialize in a prior cell.\")\n",
        "    # If needed, define fallback: stop_polling_event = threading.Event()\n",
        "\n",
        "# Clear previous readings and reset event\n",
        "gpu_utilization_readings = []\n",
        "if 'stop_polling_event' in locals():\n",
        "    stop_polling_event.clear()\n",
        "\n",
        "# Start GPU polling thread\n",
        "polling_thread = None\n",
        "if handle:\n",
        "    polling_thread = threading.Thread(target=poll_gpu_utilization, args=(handle, 1.0), daemon=True)\n",
        "    polling_thread.start()\n",
        "\n",
        "# Clean memory before starting\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "fp16_arc_metrics = {}          # <<< Variable renamed\n",
        "fp16_arc_results_data = {}     # <<< Variable renamed\n",
        "model_fp16 = None # Define outside try <<< Variable renamed\n",
        "\n",
        "try:\n",
        "    # Load half precision model (FP16)\n",
        "    model_name = \"Qwen/Qwen2.5-3B\" # Base model name\n",
        "    print(f\"Loading model: {model_name} (FP16)...\") # <<< Log updated\n",
        "    model_fp16 = AutoModelForCausalLM.from_pretrained( # <<< Variable renamed\n",
        "        model_name,\n",
        "        torch_dtype=torch.float16,      # <<< Explicitly FP16\n",
        "        device_map=\"cuda:0\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "    print(\"Model loaded.\")\n",
        "\n",
        "    # Load tokenizer\n",
        "    print(\"Loading tokenizer...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    print(\"Tokenizer loaded.\")\n",
        "\n",
        "    # --- No reset_peak_memory_stats needed for this metric set ---\n",
        "\n",
        "    # Run ARC-Challenge evaluation using the evaluate_arc function\n",
        "    print(\"Starting ARC-Challenge evaluation (FP16)...\") # <<< Log updated\n",
        "    # Using the correct ARC-Challenge dataset variable\n",
        "    # Assuming evaluate_arc returns required keys\n",
        "    fp16_arc_results_data = evaluate_arc(model_fp16, tokenizer, samples_200_arc_challenge_test, max_new_tokens=1) # <<< Variable renamed, dataset var checked, max_new_tokens=1\n",
        "    print(\"ARC-Challenge evaluation finished.\") # <<< Log updated\n",
        "\n",
        "    # Stop polling thread and calculate average utilization\n",
        "    avg_gpu_utilization = 'N/A'\n",
        "    if polling_thread and polling_thread.is_alive():\n",
        "        stop_polling_event.set()\n",
        "        polling_thread.join(timeout=5)\n",
        "        if gpu_utilization_readings:\n",
        "            avg_gpu_utilization = statistics.mean(gpu_utilization_readings)\n",
        "        else:\n",
        "             avg_gpu_utilization = 0 # Polling ran but got no readings\n",
        "    elif handle:\n",
        "        avg_gpu_utilization = 0 # Polling failed to start/run correctly\n",
        "\n",
        "    # Collect metrics in the MMLU format using results from evaluate_arc\n",
        "    accuracy_val = fp16_arc_results_data.get('accuracy', 'N/A') # <<< Use renamed variable\n",
        "    if isinstance(accuracy_val, (float, int)):\n",
        "        accuracy_val *= 100 # Multiply only if it's a number\n",
        "\n",
        "    fp16_arc_metrics = { # <<< Variable renamed\n",
        "        \"PPL (Perplexity)\": fp16_arc_results_data.get('perplexity', 'N/A'),\n",
        "        \"Accuracy\": accuracy_val,\n",
        "        \"Memory Footprint (Model Size) (GB)\": model_fp16.get_memory_footprint() / (1024 ** 3), # <<< Use renamed variable\n",
        "        \"Inference Latency (ms/token)\": fp16_arc_results_data.get('inference_latency', 'N/A'),\n",
        "        \"Avg GPU Utilization (%)\": avg_gpu_utilization, # From polling\n",
        "        \"Avg GPU Memory Allocated (GB)\": fp16_arc_results_data.get('avg_gpu_memory_gb', 'N/A'), # From evaluate_arc result\n",
        "    }\n",
        "\n",
        "    # Print metrics\n",
        "    print(\"\\n===== FP16 ARC-Challenge DETAILED MODEL METRICS (MMLU Format) =====\") # <<< Title updated\n",
        "    print(\"-\" * 60)\n",
        "    if fp16_arc_metrics: # <<< Use renamed variable\n",
        "        try:\n",
        "            max_key_length = max(len(key) for key in fp16_arc_metrics.keys()) # <<< Use renamed variable\n",
        "        except ValueError:\n",
        "             max_key_length = 35 # Default width\n",
        "\n",
        "        for key, value in fp16_arc_metrics.items(): # <<< Use renamed variable\n",
        "             if isinstance(value, (float, int)):\n",
        "                 print(f\"{key.ljust(max_key_length)} : {value:.4f}\")\n",
        "             else:\n",
        "                 print(f\"{key.ljust(max_key_length)} : {value}\") # Handles 'N/A'\n",
        "    else:\n",
        "        print(\"No metrics collected.\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "except pynvml.NVMLError as nvml_error:\n",
        "     print(f\"NVML Error during FP16 ARC-Challenge evaluation steps: {nvml_error}\") # <<< Log updated\n",
        "except Exception as e:\n",
        "    print(f\"FP16 ARC-Challenge Evaluation error: {e}\") # <<< Log updated\n",
        "    # Ensure polling stops on error\n",
        "    if polling_thread and polling_thread.is_alive():\n",
        "        stop_polling_event.set()\n",
        "        try:\n",
        "            polling_thread.join(timeout=5)\n",
        "        except Exception as join_e:\n",
        "            print(f\"Error stopping polling thread after exception: {join_e}\")\n",
        "\n",
        "finally:\n",
        "    # Clean up GPU memory (No NVML Shutdown)\n",
        "    print(\"Starting cleanup for FP16 ARC-Challenge cell...\") # <<< Log updated\n",
        "    if 'model_fp16' in locals() and model_fp16 is not None: # Check for model_fp16 <<< Use renamed variable\n",
        "        del model_fp16 # Delete model_fp16 <<< Use renamed variable\n",
        "        print(\"FP16 model deleted.\")\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    print(\"GPU cache cleared and garbage collected after FP16 ARC-Challenge run.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "id": "ERcdLNDVyoso",
        "outputId": "e52089dc-9c5c-4402-8440-28f4d5cbc6a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuring NF4 quantization (FP32 Compute)...\n",
            "Quantization config created.\n",
            "Loading model: Qwen/Qwen2.5-3B (NF4 Quantized, FP32 Compute)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "694299b795684c478202aeed162f3031"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded.\n",
            "Loading tokenizer...\n",
            "Tokenizer loaded.\n",
            "Starting ARC-Challenge evaluation (NF4, FP32 Compute)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating ARC:   0%|          | 0/200 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2812a460cd0a4cdabc984550c9772e02"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ARC-Challenge evaluation finished.\n",
            "\n",
            "===== NF4 (FP32 Compute) ARC-Challenge DETAILED MODEL METRICS =====\n",
            "------------------------------------------------------------\n",
            "PPL (Perplexity)                   : 7.3644\n",
            "Accuracy                           : 90.0000\n",
            "Memory Footprint (Model Size) (GB) : 1.8720\n",
            "Inference Latency (ms/token)       : 343.3849\n",
            "Avg GPU Utilization (%)            : 89.8533\n",
            "Avg GPU Memory Allocated (GB)      : 1.9410\n",
            "------------------------------------------------------------\n",
            "Starting cleanup for NF4 (FP32 Compute) ARC-Challenge cell...\n",
            "NF4 model deleted.\n",
            "GPU cache cleared and garbage collected after NF4 (FP32 Compute) ARC-Challenge run.\n"
          ]
        }
      ],
      "source": [
        "# --- NF4 (FP32 Compute) ARC-Challenge Benchmark Evaluation Cell --- # <<< Title updated\n",
        "\n",
        "# Assume 'handle' and 'stop_polling_event' exist from a previous cell.\n",
        "if 'handle' not in locals() or not handle:\n",
        "    print(\"Error: NVML handle not found. Please initialize NVML in a prior cell.\")\n",
        "    handle = None # Prevent polling\n",
        "elif 'stop_polling_event' not in locals():\n",
        "    print(\"Error: stop_polling_event not found. Please initialize in a prior cell.\")\n",
        "    # If needed, define fallback: stop_polling_event = threading.Event()\n",
        "\n",
        "# Clear previous readings and reset event\n",
        "gpu_utilization_readings = []\n",
        "if 'stop_polling_event' in locals():\n",
        "    stop_polling_event.clear()\n",
        "\n",
        "# Start GPU polling thread\n",
        "polling_thread = None\n",
        "if handle:\n",
        "    polling_thread = threading.Thread(target=poll_gpu_utilization, args=(handle, 1.0), daemon=True)\n",
        "    polling_thread.start()\n",
        "\n",
        "# Clean memory before starting\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "nf4_arc_metrics = {}          # <<< Variable renamed\n",
        "nf4_arc_results_data = {}     # <<< Variable renamed\n",
        "model_nf4 = None # Define outside try <<< Variable renamed\n",
        "\n",
        "try:\n",
        "    # Configure quantization to NF4 with FP32 compute\n",
        "    print(\"Configuring NF4 quantization (FP32 Compute)...\") # <<< Log updated\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",             # <--- Set to NF4\n",
        "        bnb_4bit_compute_dtype=torch.float32, # <--- Set to FP32 compute\n",
        "        bnb_4bit_use_double_quant=True\n",
        "    )\n",
        "    print(\"Quantization config created.\")\n",
        "\n",
        "    # Load quantized model (NF4)\n",
        "    model_name = \"Qwen/Qwen2.5-3B\" # Base model name\n",
        "    print(f\"Loading model: {model_name} (NF4 Quantized, FP32 Compute)...\") # <<< Log updated\n",
        "    model_nf4 = AutoModelForCausalLM.from_pretrained( # <<< Variable renamed\n",
        "        model_name,\n",
        "        quantization_config=bnb_config, # Use NF4 quantization config\n",
        "        device_map=\"cuda:0\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "    print(\"Model loaded.\")\n",
        "\n",
        "    # Load tokenizer\n",
        "    print(\"Loading tokenizer...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    print(\"Tokenizer loaded.\")\n",
        "\n",
        "    # --- No reset_peak_memory_stats needed for this metric set ---\n",
        "\n",
        "    # Run ARC-Challenge evaluation using the evaluate_arc function\n",
        "    print(\"Starting ARC-Challenge evaluation (NF4, FP32 Compute)...\") # <<< Log updated\n",
        "    # Assuming evaluate_arc can handle the quantized model and returns required keys\n",
        "    # Using the correct ARC-Challenge dataset and max_new_tokens=1\n",
        "    nf4_arc_results_data = evaluate_arc(model_nf4, tokenizer, samples_200_arc_challenge_test, max_new_tokens=1) # <<< Variable renamed, dataset var updated, max_new_tokens=1\n",
        "    print(\"ARC-Challenge evaluation finished.\") # <<< Log updated\n",
        "\n",
        "    # Stop polling thread and calculate average utilization\n",
        "    avg_gpu_utilization = 'N/A'\n",
        "    if polling_thread and polling_thread.is_alive():\n",
        "        stop_polling_event.set()\n",
        "        polling_thread.join(timeout=5)\n",
        "        if gpu_utilization_readings:\n",
        "            avg_gpu_utilization = statistics.mean(gpu_utilization_readings)\n",
        "        else:\n",
        "             avg_gpu_utilization = 0 # Polling ran but got no readings\n",
        "    elif handle:\n",
        "        avg_gpu_utilization = 0 # Polling failed to start/run correctly\n",
        "\n",
        "    # Collect metrics in the MMLU format using results from evaluate_arc\n",
        "    accuracy_val = nf4_arc_results_data.get('accuracy', 'N/A') # <<< Use renamed variable\n",
        "    if isinstance(accuracy_val, (float, int)):\n",
        "        accuracy_val *= 100 # Multiply only if it's a number\n",
        "\n",
        "    nf4_arc_metrics = { # <<< Variable renamed\n",
        "        \"PPL (Perplexity)\": nf4_arc_results_data.get('perplexity', 'N/A'),\n",
        "        \"Accuracy\": accuracy_val,\n",
        "        \"Memory Footprint (Model Size) (GB)\": model_nf4.get_memory_footprint() / (1024 ** 3), # <<< Use renamed variable\n",
        "        \"Inference Latency (ms/token)\": nf4_arc_results_data.get('inference_latency', 'N/A'),\n",
        "        \"Avg GPU Utilization (%)\": avg_gpu_utilization, # From polling\n",
        "        \"Avg GPU Memory Allocated (GB)\": nf4_arc_results_data.get('avg_gpu_memory_gb', 'N/A'), # From evaluate_arc result\n",
        "    }\n",
        "\n",
        "    # Print metrics\n",
        "    print(\"\\n===== NF4 (FP32 Compute) ARC-Challenge DETAILED MODEL METRICS =====\") # <<< Title updated\n",
        "    print(\"-\" * 60)\n",
        "    if nf4_arc_metrics: # <<< Use renamed variable\n",
        "        try:\n",
        "            max_key_length = max(len(key) for key in nf4_arc_metrics.keys()) # <<< Use renamed variable\n",
        "        except ValueError:\n",
        "             max_key_length = 35 # Default width\n",
        "\n",
        "        for key, value in nf4_arc_metrics.items(): # <<< Use renamed variable\n",
        "             if isinstance(value, (float, int)):\n",
        "                 print(f\"{key.ljust(max_key_length)} : {value:.4f}\")\n",
        "             else:\n",
        "                 print(f\"{key.ljust(max_key_length)} : {value}\") # Handles 'N/A'\n",
        "    else:\n",
        "        print(\"No metrics collected.\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "except pynvml.NVMLError as nvml_error:\n",
        "     print(f\"NVML Error during NF4 (FP32 Compute) ARC-Challenge evaluation steps: {nvml_error}\") # <<< Log updated\n",
        "except Exception as e:\n",
        "    print(f\"NF4 (FP32 Compute) ARC-Challenge Evaluation error: {e}\") # <<< Log updated\n",
        "    # Ensure polling stops on error\n",
        "    if polling_thread and polling_thread.is_alive():\n",
        "        stop_polling_event.set()\n",
        "        try:\n",
        "            polling_thread.join(timeout=5)\n",
        "        except Exception as join_e:\n",
        "            print(f\"Error stopping polling thread after exception: {join_e}\")\n",
        "\n",
        "finally:\n",
        "    # Clean up GPU memory (No NVML Shutdown)\n",
        "    print(\"Starting cleanup for NF4 (FP32 Compute) ARC-Challenge cell...\") # <<< Log updated\n",
        "    if 'model_nf4' in locals() and model_nf4 is not None: # Check for model_nf4 <<< Use renamed variable\n",
        "        del model_nf4 # Delete model_nf4 <<< Use renamed variable\n",
        "        print(\"NF4 model deleted.\")\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    print(\"GPU cache cleared and garbage collected after NF4 (FP32 Compute) ARC-Challenge run.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "id": "sm5I8qlOyosp",
        "outputId": "78bf86f7-e2de-4772-d44d-b62f2eaacbe1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuring NF4 quantization (FP16 Compute)...\n",
            "Quantization config created.\n",
            "Loading model: Qwen/Qwen2.5-3B (NF4 Quantized, FP16 Compute)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8e1fa97b614e4922ab4b9f4c0b9bf965"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded.\n",
            "Loading tokenizer...\n",
            "Tokenizer loaded.\n",
            "Starting ARC-Challenge evaluation (NF4, FP16 Compute)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating ARC:   0%|          | 0/200 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "98d5bf4b788b417c8bb72f36dcd2e931"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ARC-Challenge evaluation finished.\n",
            "\n",
            "===== NF4 (FP16 Compute) ARC-Challenge DETAILED MODEL METRICS =====\n",
            "------------------------------------------------------------\n",
            "PPL (Perplexity)                   : 7.3646\n",
            "Accuracy                           : 90.0000\n",
            "Memory Footprint (Model Size) (GB) : 1.8720\n",
            "Inference Latency (ms/token)       : 193.8892\n",
            "Avg GPU Utilization (%)            : 42.8667\n",
            "Avg GPU Memory Allocated (GB)      : 1.9410\n",
            "------------------------------------------------------------\n",
            "Starting cleanup for NF4 (FP16 Compute) ARC-Challenge cell...\n",
            "NF4 model deleted.\n",
            "GPU cache cleared and garbage collected after NF4 (FP16 Compute) ARC-Challenge run.\n"
          ]
        }
      ],
      "source": [
        "# --- NF4 (FP16 Compute) ARC-Challenge Benchmark Evaluation Cell --- # <<< Title updated\n",
        "\n",
        "# Assume 'handle' and 'stop_polling_event' exist from a previous cell.\n",
        "if 'handle' not in locals() or not handle:\n",
        "    print(\"Error: NVML handle not found. Please initialize NVML in a prior cell.\")\n",
        "    handle = None # Prevent polling\n",
        "elif 'stop_polling_event' not in locals():\n",
        "    print(\"Error: stop_polling_event not found. Please initialize in a prior cell.\")\n",
        "    # If needed, define fallback: stop_polling_event = threading.Event()\n",
        "\n",
        "# Clear previous readings and reset event\n",
        "gpu_utilization_readings = []\n",
        "if 'stop_polling_event' in locals():\n",
        "    stop_polling_event.clear()\n",
        "\n",
        "# Start GPU polling thread\n",
        "polling_thread = None\n",
        "if handle:\n",
        "    polling_thread = threading.Thread(target=poll_gpu_utilization, args=(handle, 1.0), daemon=True)\n",
        "    polling_thread.start()\n",
        "\n",
        "# Clean memory before starting\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "nf4_arc_metrics = {}          # <<< Variable renamed\n",
        "nf4_arc_results_data = {}     # <<< Variable renamed\n",
        "model_nf4 = None # Define outside try <<< Variable renamed\n",
        "\n",
        "try:\n",
        "    # Configure quantization to NF4 with FP16 compute\n",
        "    print(\"Configuring NF4 quantization (FP16 Compute)...\") # <<< Log updated\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",             # <--- Set to NF4\n",
        "        bnb_4bit_compute_dtype=torch.float16, # <--- Set to FP16 compute\n",
        "        bnb_4bit_use_double_quant=True\n",
        "    )\n",
        "    print(\"Quantization config created.\")\n",
        "\n",
        "    # --- Removed loading of separate FP16 model ---\n",
        "\n",
        "    # Load quantized model (NF4)\n",
        "    model_name = \"Qwen/Qwen2.5-3B\" # Base model name\n",
        "    print(f\"Loading model: {model_name} (NF4 Quantized, FP16 Compute)...\") # <<< Log updated\n",
        "    model_nf4 = AutoModelForCausalLM.from_pretrained( # <<< Variable renamed\n",
        "        model_name,\n",
        "        quantization_config=bnb_config, # Use NF4 quantization config\n",
        "        device_map=\"cuda:0\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "    print(\"Model loaded.\")\n",
        "\n",
        "    # Load tokenizer\n",
        "    print(\"Loading tokenizer...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    print(\"Tokenizer loaded.\")\n",
        "\n",
        "    # --- No reset_peak_memory_stats needed for this metric set ---\n",
        "\n",
        "    # Run ARC-Challenge evaluation using the evaluate_arc function\n",
        "    print(\"Starting ARC-Challenge evaluation (NF4, FP16 Compute)...\") # <<< Log updated\n",
        "    # Assuming evaluate_arc can handle the quantized model and returns required keys\n",
        "    # Using the correct ARC-Challenge dataset and max_new_tokens=1\n",
        "    nf4_arc_results_data = evaluate_arc(model_nf4, tokenizer, samples_200_arc_challenge_test, max_new_tokens=1) # <<< Variable renamed, dataset var updated, max_new_tokens=1\n",
        "    print(\"ARC-Challenge evaluation finished.\") # <<< Log updated\n",
        "\n",
        "    # Stop polling thread and calculate average utilization\n",
        "    avg_gpu_utilization = 'N/A'\n",
        "    if polling_thread and polling_thread.is_alive():\n",
        "        stop_polling_event.set()\n",
        "        polling_thread.join(timeout=5)\n",
        "        if gpu_utilization_readings:\n",
        "            avg_gpu_utilization = statistics.mean(gpu_utilization_readings)\n",
        "        else:\n",
        "             avg_gpu_utilization = 0 # Polling ran but got no readings\n",
        "    elif handle:\n",
        "        avg_gpu_utilization = 0 # Polling failed to start/run correctly\n",
        "\n",
        "    # Collect metrics in the MMLU format using results from evaluate_arc\n",
        "    accuracy_val = nf4_arc_results_data.get('accuracy', 'N/A') # <<< Use renamed variable\n",
        "    if isinstance(accuracy_val, (float, int)):\n",
        "        accuracy_val *= 100 # Multiply only if it's a number\n",
        "\n",
        "    nf4_arc_metrics = { # <<< Variable renamed\n",
        "        \"PPL (Perplexity)\": nf4_arc_results_data.get('perplexity', 'N/A'),\n",
        "        \"Accuracy\": accuracy_val,\n",
        "        \"Memory Footprint (Model Size) (GB)\": model_nf4.get_memory_footprint() / (1024 ** 3), # <<< Use renamed variable\n",
        "        \"Inference Latency (ms/token)\": nf4_arc_results_data.get('inference_latency', 'N/A'),\n",
        "        \"Avg GPU Utilization (%)\": avg_gpu_utilization, # From polling\n",
        "        \"Avg GPU Memory Allocated (GB)\": nf4_arc_results_data.get('avg_gpu_memory_gb', 'N/A'), # From evaluate_arc result\n",
        "    }\n",
        "\n",
        "    # Print metrics\n",
        "    print(\"\\n===== NF4 (FP16 Compute) ARC-Challenge DETAILED MODEL METRICS =====\") # <<< Title updated\n",
        "    print(\"-\" * 60)\n",
        "    if nf4_arc_metrics: # <<< Use renamed variable\n",
        "        try:\n",
        "            max_key_length = max(len(key) for key in nf4_arc_metrics.keys()) # <<< Use renamed variable\n",
        "        except ValueError:\n",
        "             max_key_length = 35 # Default width\n",
        "\n",
        "        for key, value in nf4_arc_metrics.items(): # <<< Use renamed variable\n",
        "             if isinstance(value, (float, int)):\n",
        "                 print(f\"{key.ljust(max_key_length)} : {value:.4f}\")\n",
        "             else:\n",
        "                 print(f\"{key.ljust(max_key_length)} : {value}\") # Handles 'N/A'\n",
        "    else:\n",
        "        print(\"No metrics collected.\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "except pynvml.NVMLError as nvml_error:\n",
        "     print(f\"NVML Error during NF4 (FP16 Compute) ARC-Challenge evaluation steps: {nvml_error}\") # <<< Log updated\n",
        "except Exception as e:\n",
        "    print(f\"NF4 (FP16 Compute) ARC-Challenge Evaluation error: {e}\") # <<< Log updated\n",
        "    # Ensure polling stops on error\n",
        "    if polling_thread and polling_thread.is_alive():\n",
        "        stop_polling_event.set()\n",
        "        try:\n",
        "            polling_thread.join(timeout=5)\n",
        "        except Exception as join_e:\n",
        "            print(f\"Error stopping polling thread after exception: {join_e}\")\n",
        "\n",
        "finally:\n",
        "    # Clean up GPU memory (No NVML Shutdown)\n",
        "    print(\"Starting cleanup for NF4 (FP16 Compute) ARC-Challenge cell...\") # <<< Log updated\n",
        "    if 'model_nf4' in locals() and model_nf4 is not None: # Check for model_nf4 <<< Use renamed variable\n",
        "        del model_nf4 # Delete model_nf4 <<< Use renamed variable\n",
        "        print(\"NF4 model deleted.\")\n",
        "    # --- Removed deletion of separate model_fp16 ---\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    print(\"GPU cache cleared and garbage collected after NF4 (FP16 Compute) ARC-Challenge run.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "id": "l2ocLT7Xyosp",
        "outputId": "e1814acc-2b10-4e45-9794-7cf41371b31a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuring FP4 quantization (FP32 Compute)...\n",
            "Quantization config created.\n",
            "Loading model: Qwen/Qwen2.5-3B (FP4 Quantized, FP32 Compute)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ec5f6aa4158343769020292f9175365b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded.\n",
            "Loading tokenizer...\n",
            "Tokenizer loaded.\n",
            "Starting ARC-Challenge evaluation (FP4, FP32 Compute)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating ARC:   0%|          | 0/200 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8ddd511f403a49248ad63ff3a5d565b5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ARC-Challenge evaluation finished.\n",
            "\n",
            "===== FP4 (FP32 Compute) ARC-Challenge DETAILED MODEL METRICS =====\n",
            "------------------------------------------------------------\n",
            "PPL (Perplexity)                   : 8.3459\n",
            "Accuracy                           : 84.0000\n",
            "Memory Footprint (Model Size) (GB) : 1.8720\n",
            "Inference Latency (ms/token)       : 333.9153\n",
            "Avg GPU Utilization (%)            : 87.1316\n",
            "Avg GPU Memory Allocated (GB)      : 1.9410\n",
            "------------------------------------------------------------\n",
            "Starting cleanup for FP4 (FP32 Compute) ARC-Challenge cell...\n",
            "FP4 model deleted.\n",
            "GPU cache cleared and garbage collected after FP4 (FP32 Compute) ARC-Challenge run.\n"
          ]
        }
      ],
      "source": [
        "# --- FP4 (FP32 Compute) ARC-Challenge Benchmark Evaluation Cell --- # <<< Title updated\n",
        "\n",
        "# Assume 'handle' and 'stop_polling_event' exist from a previous cell.\n",
        "if 'handle' not in locals() or not handle:\n",
        "    print(\"Error: NVML handle not found. Please initialize NVML in a prior cell.\")\n",
        "    handle = None # Prevent polling\n",
        "elif 'stop_polling_event' not in locals():\n",
        "    print(\"Error: stop_polling_event not found. Please initialize in a prior cell.\")\n",
        "    # If needed, define fallback: stop_polling_event = threading.Event()\n",
        "\n",
        "# Clear previous readings and reset event\n",
        "gpu_utilization_readings = []\n",
        "if 'stop_polling_event' in locals():\n",
        "    stop_polling_event.clear()\n",
        "\n",
        "# Start GPU polling thread\n",
        "polling_thread = None\n",
        "if handle:\n",
        "    polling_thread = threading.Thread(target=poll_gpu_utilization, args=(handle, 1.0), daemon=True)\n",
        "    polling_thread.start()\n",
        "\n",
        "# Clean memory before starting\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "fp4_arc_metrics = {}          # <<< Variable renamed\n",
        "fp4_arc_results_data = {}     # <<< Variable renamed\n",
        "model_fp4 = None # Define outside try <<< Variable renamed\n",
        "\n",
        "try:\n",
        "    # Configure quantization to FP4 with FP32 compute\n",
        "    print(\"Configuring FP4 quantization (FP32 Compute)...\") # <<< Log updated\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"fp4\",             # <--- Set to FP4\n",
        "        bnb_4bit_compute_dtype=torch.float32, # <--- Set to FP32 compute\n",
        "        bnb_4bit_use_double_quant=True\n",
        "    )\n",
        "    print(\"Quantization config created.\")\n",
        "\n",
        "    # Load quantized model (FP4)\n",
        "    model_name = \"Qwen/Qwen2.5-3B\" # Base model name\n",
        "    print(f\"Loading model: {model_name} (FP4 Quantized, FP32 Compute)...\") # <<< Log updated\n",
        "    model_fp4 = AutoModelForCausalLM.from_pretrained( # <<< Variable renamed\n",
        "        model_name,\n",
        "        quantization_config=bnb_config, # Use FP4 quantization config\n",
        "        device_map=\"cuda:0\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "    print(\"Model loaded.\")\n",
        "\n",
        "    # Load tokenizer\n",
        "    print(\"Loading tokenizer...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    print(\"Tokenizer loaded.\")\n",
        "\n",
        "    # --- No reset_peak_memory_stats needed for this metric set ---\n",
        "\n",
        "    # Run ARC-Challenge evaluation using the evaluate_arc function\n",
        "    print(\"Starting ARC-Challenge evaluation (FP4, FP32 Compute)...\") # <<< Log updated\n",
        "    # Assuming evaluate_arc can handle the quantized model and returns required keys\n",
        "    # Using the correct ARC-Challenge dataset and max_new_tokens=1\n",
        "    fp4_arc_results_data = evaluate_arc(model_fp4, tokenizer, samples_200_arc_challenge_test, max_new_tokens=1) # <<< Variable renamed, dataset var updated, max_new_tokens=1\n",
        "    print(\"ARC-Challenge evaluation finished.\") # <<< Log updated\n",
        "\n",
        "    # Stop polling thread and calculate average utilization\n",
        "    avg_gpu_utilization = 'N/A'\n",
        "    if polling_thread and polling_thread.is_alive():\n",
        "        stop_polling_event.set()\n",
        "        polling_thread.join(timeout=5)\n",
        "        if gpu_utilization_readings:\n",
        "            avg_gpu_utilization = statistics.mean(gpu_utilization_readings)\n",
        "        else:\n",
        "             avg_gpu_utilization = 0 # Polling ran but got no readings\n",
        "    elif handle:\n",
        "        avg_gpu_utilization = 0 # Polling failed to start/run correctly\n",
        "\n",
        "    # Collect metrics in the MMLU format using results from evaluate_arc\n",
        "    accuracy_val = fp4_arc_results_data.get('accuracy', 'N/A') # <<< Use renamed variable\n",
        "    if isinstance(accuracy_val, (float, int)):\n",
        "        accuracy_val *= 100 # Multiply only if it's a number\n",
        "\n",
        "    fp4_arc_metrics = { # <<< Variable renamed\n",
        "        \"PPL (Perplexity)\": fp4_arc_results_data.get('perplexity', 'N/A'),\n",
        "        \"Accuracy\": accuracy_val,\n",
        "        \"Memory Footprint (Model Size) (GB)\": model_fp4.get_memory_footprint() / (1024 ** 3), # <<< Use renamed variable\n",
        "        \"Inference Latency (ms/token)\": fp4_arc_results_data.get('inference_latency', 'N/A'),\n",
        "        \"Avg GPU Utilization (%)\": avg_gpu_utilization, # From polling\n",
        "        \"Avg GPU Memory Allocated (GB)\": fp4_arc_results_data.get('avg_gpu_memory_gb', 'N/A'), # From evaluate_arc result\n",
        "    }\n",
        "\n",
        "    # Print metrics\n",
        "    print(\"\\n===== FP4 (FP32 Compute) ARC-Challenge DETAILED MODEL METRICS =====\") # <<< Title updated\n",
        "    print(\"-\" * 60)\n",
        "    if fp4_arc_metrics: # <<< Use renamed variable\n",
        "        try:\n",
        "            max_key_length = max(len(key) for key in fp4_arc_metrics.keys()) # <<< Use renamed variable\n",
        "        except ValueError:\n",
        "             max_key_length = 35 # Default width\n",
        "\n",
        "        for key, value in fp4_arc_metrics.items(): # <<< Use renamed variable\n",
        "             if isinstance(value, (float, int)):\n",
        "                 print(f\"{key.ljust(max_key_length)} : {value:.4f}\")\n",
        "             else:\n",
        "                 print(f\"{key.ljust(max_key_length)} : {value}\") # Handles 'N/A'\n",
        "    else:\n",
        "        print(\"No metrics collected.\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "except pynvml.NVMLError as nvml_error:\n",
        "     print(f\"NVML Error during FP4 (FP32 Compute) ARC-Challenge evaluation steps: {nvml_error}\") # <<< Log updated\n",
        "except Exception as e:\n",
        "    print(f\"FP4 (FP32 Compute) ARC-Challenge Evaluation error: {e}\") # <<< Log updated\n",
        "    # Ensure polling stops on error\n",
        "    if polling_thread and polling_thread.is_alive():\n",
        "        stop_polling_event.set()\n",
        "        try:\n",
        "            polling_thread.join(timeout=5)\n",
        "        except Exception as join_e:\n",
        "            print(f\"Error stopping polling thread after exception: {join_e}\")\n",
        "\n",
        "finally:\n",
        "    # Clean up GPU memory (No NVML Shutdown)\n",
        "    print(\"Starting cleanup for FP4 (FP32 Compute) ARC-Challenge cell...\") # <<< Log updated\n",
        "    if 'model_fp4' in locals() and model_fp4 is not None: # Check for model_fp4 <<< Use renamed variable\n",
        "        del model_fp4 # Delete model_fp4 <<< Use renamed variable\n",
        "        print(\"FP4 model deleted.\")\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    print(\"GPU cache cleared and garbage collected after FP4 (FP32 Compute) ARC-Challenge run.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "id": "xXj6Op37yosp",
        "outputId": "1298d7fe-0e49-4503-8f89-05ca99cac073"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuring FP4 quantization (FP16 Compute)...\n",
            "Quantization config created.\n",
            "Loading model: Qwen/Qwen2.5-3B (FP4 Quantized, FP16 Compute)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f25c5596713743e4b4d32fb794d79d47"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded.\n",
            "Loading tokenizer...\n",
            "Tokenizer loaded.\n",
            "Starting ARC-Challenge evaluation (FP4, FP16 Compute)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating ARC:   0%|          | 0/200 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b19c241e3fc5477f87fe11571cc982e7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ARC-Challenge evaluation finished.\n",
            "\n",
            "===== FP4 (FP16 Compute) ARC-Challenge DETAILED MODEL METRICS =====\n",
            "------------------------------------------------------------\n",
            "PPL (Perplexity)                   : 8.3389\n",
            "Accuracy                           : 84.0000\n",
            "Memory Footprint (Model Size) (GB) : 1.8720\n",
            "Inference Latency (ms/token)       : 194.0847\n",
            "Avg GPU Utilization (%)            : 41.9778\n",
            "Avg GPU Memory Allocated (GB)      : 1.9410\n",
            "------------------------------------------------------------\n",
            "Starting cleanup for FP4 (FP16 Compute) ARC-Challenge cell...\n",
            "FP4 model deleted.\n",
            "GPU cache cleared and garbage collected after FP4 (FP16 Compute) ARC-Challenge run.\n"
          ]
        }
      ],
      "source": [
        "# --- FP4 (FP16 Compute) ARC-Challenge Benchmark Evaluation Cell --- # <<< Title updated\n",
        "\n",
        "# Assume 'handle' and 'stop_polling_event' exist from a previous cell.\n",
        "if 'handle' not in locals() or not handle:\n",
        "    print(\"Error: NVML handle not found. Please initialize NVML in a prior cell.\")\n",
        "    handle = None # Prevent polling\n",
        "elif 'stop_polling_event' not in locals():\n",
        "    print(\"Error: stop_polling_event not found. Please initialize in a prior cell.\")\n",
        "    # If needed, define fallback: stop_polling_event = threading.Event()\n",
        "\n",
        "# Clear previous readings and reset event\n",
        "gpu_utilization_readings = []\n",
        "if 'stop_polling_event' in locals():\n",
        "    stop_polling_event.clear()\n",
        "\n",
        "# Start GPU polling thread\n",
        "polling_thread = None\n",
        "if handle:\n",
        "    polling_thread = threading.Thread(target=poll_gpu_utilization, args=(handle, 1.0), daemon=True)\n",
        "    polling_thread.start()\n",
        "\n",
        "# Clean memory before starting\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "fp4_arc_metrics = {}          # <<< Variable renamed\n",
        "fp4_arc_results_data = {}     # <<< Variable renamed\n",
        "model_fp4 = None # Define outside try <<< Variable renamed\n",
        "\n",
        "try:\n",
        "    # Configure quantization to FP4 with FP16 compute\n",
        "    print(\"Configuring FP4 quantization (FP16 Compute)...\") # <<< Log updated\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"fp4\",             # <--- Set to FP4\n",
        "        bnb_4bit_compute_dtype=torch.float16, # <--- Set to FP16 compute\n",
        "        bnb_4bit_use_double_quant=True\n",
        "    )\n",
        "    print(\"Quantization config created.\")\n",
        "\n",
        "    # --- Removed loading of separate FP16 model ---\n",
        "\n",
        "    # Load quantized model (FP4)\n",
        "    model_name = \"Qwen/Qwen2.5-3B\" # Base model name\n",
        "    print(f\"Loading model: {model_name} (FP4 Quantized, FP16 Compute)...\") # <<< Log updated\n",
        "    model_fp4 = AutoModelForCausalLM.from_pretrained( # <<< Variable renamed\n",
        "        model_name,\n",
        "        quantization_config=bnb_config, # Use FP4 quantization config\n",
        "        device_map=\"cuda:0\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "    print(\"Model loaded.\")\n",
        "\n",
        "    # Load tokenizer\n",
        "    print(\"Loading tokenizer...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    print(\"Tokenizer loaded.\")\n",
        "\n",
        "    # --- No reset_peak_memory_stats needed for this metric set ---\n",
        "\n",
        "    # Run ARC-Challenge evaluation using the evaluate_arc function\n",
        "    print(\"Starting ARC-Challenge evaluation (FP4, FP16 Compute)...\") # <<< Log updated\n",
        "    # Assuming evaluate_arc can handle the quantized model and returns required keys\n",
        "    # Using the correct ARC-Challenge dataset and max_new_tokens=1\n",
        "    fp4_arc_results_data = evaluate_arc(model_fp4, tokenizer, samples_200_arc_challenge_test, max_new_tokens=1) # <<< Variable renamed, dataset var updated, max_new_tokens=1\n",
        "    print(\"ARC-Challenge evaluation finished.\") # <<< Log updated\n",
        "\n",
        "    # Stop polling thread and calculate average utilization\n",
        "    avg_gpu_utilization = 'N/A'\n",
        "    if polling_thread and polling_thread.is_alive():\n",
        "        stop_polling_event.set()\n",
        "        polling_thread.join(timeout=5)\n",
        "        if gpu_utilization_readings:\n",
        "            avg_gpu_utilization = statistics.mean(gpu_utilization_readings)\n",
        "        else:\n",
        "             avg_gpu_utilization = 0 # Polling ran but got no readings\n",
        "    elif handle:\n",
        "        avg_gpu_utilization = 0 # Polling failed to start/run correctly\n",
        "\n",
        "    # Collect metrics in the MMLU format using results from evaluate_arc\n",
        "    accuracy_val = fp4_arc_results_data.get('accuracy', 'N/A') # <<< Use renamed variable\n",
        "    if isinstance(accuracy_val, (float, int)):\n",
        "        accuracy_val *= 100 # Multiply only if it's a number\n",
        "\n",
        "    fp4_arc_metrics = { # <<< Variable renamed\n",
        "        \"PPL (Perplexity)\": fp4_arc_results_data.get('perplexity', 'N/A'),\n",
        "        \"Accuracy\": accuracy_val,\n",
        "        \"Memory Footprint (Model Size) (GB)\": model_fp4.get_memory_footprint() / (1024 ** 3), # <<< Use renamed variable\n",
        "        \"Inference Latency (ms/token)\": fp4_arc_results_data.get('inference_latency', 'N/A'),\n",
        "        \"Avg GPU Utilization (%)\": avg_gpu_utilization, # From polling\n",
        "        \"Avg GPU Memory Allocated (GB)\": fp4_arc_results_data.get('avg_gpu_memory_gb', 'N/A'), # From evaluate_arc result\n",
        "    }\n",
        "\n",
        "    # Print metrics\n",
        "    print(\"\\n===== FP4 (FP16 Compute) ARC-Challenge DETAILED MODEL METRICS =====\") # <<< Title updated\n",
        "    print(\"-\" * 60)\n",
        "    if fp4_arc_metrics: # <<< Use renamed variable\n",
        "        try:\n",
        "            max_key_length = max(len(key) for key in fp4_arc_metrics.keys()) # <<< Use renamed variable\n",
        "        except ValueError:\n",
        "             max_key_length = 35 # Default width\n",
        "\n",
        "        for key, value in fp4_arc_metrics.items(): # <<< Use renamed variable\n",
        "             if isinstance(value, (float, int)):\n",
        "                 print(f\"{key.ljust(max_key_length)} : {value:.4f}\")\n",
        "             else:\n",
        "                 print(f\"{key.ljust(max_key_length)} : {value}\") # Handles 'N/A'\n",
        "    else:\n",
        "        print(\"No metrics collected.\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "except pynvml.NVMLError as nvml_error:\n",
        "     print(f\"NVML Error during FP4 (FP16 Compute) ARC-Challenge evaluation steps: {nvml_error}\") # <<< Log updated\n",
        "except Exception as e:\n",
        "    print(f\"FP4 (FP16 Compute) ARC-Challenge Evaluation error: {e}\") # <<< Log updated\n",
        "    # Ensure polling stops on error\n",
        "    # --- Removed traceback import and print ---\n",
        "    if polling_thread and polling_thread.is_alive():\n",
        "        stop_polling_event.set()\n",
        "        try:\n",
        "            polling_thread.join(timeout=5)\n",
        "        except Exception as join_e:\n",
        "            print(f\"Error stopping polling thread after exception: {join_e}\")\n",
        "\n",
        "finally:\n",
        "    # Clean up GPU memory (No NVML Shutdown)\n",
        "    print(\"Starting cleanup for FP4 (FP16 Compute) ARC-Challenge cell...\") # <<< Log updated\n",
        "    if 'model_fp4' in locals() and model_fp4 is not None: # Check for model_fp4 <<< Use renamed variable\n",
        "        del model_fp4 # Delete model_fp4 <<< Use renamed variable\n",
        "        print(\"FP4 model deleted.\")\n",
        "    # --- Removed deletion of separate model_fp16 ---\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    print(\"GPU cache cleared and garbage collected after FP4 (FP16 Compute) ARC-Challenge run.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "id": "EuRjw7Xfyosp",
        "outputId": "71205b94-ebd3-491e-fab3-0651382b7450"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuring INT8 quantization (FP32 Compute)...\n",
            "Quantization config created.\n",
            "Loading model: Qwen/Qwen2.5-3B (INT8 Quantized, FP32 Compute)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a2bd15e9c8714e9a8b241f5f9aa70f07"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded.\n",
            "Loading tokenizer...\n",
            "Tokenizer loaded.\n",
            "Starting ARC-Challenge evaluation (INT8, FP32 Compute)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating ARC:   0%|          | 0/200 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aa468f552bee436fa2e21dad1a0f2dfd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ARC-Challenge evaluation finished.\n",
            "\n",
            "===== INT8 (FP32 Compute) ARC-Challenge DETAILED MODEL METRICS =====\n",
            "------------------------------------------------------------\n",
            "PPL (Perplexity)                   : 6.4720\n",
            "Accuracy                           : 89.5000\n",
            "Memory Footprint (Model Size) (GB) : 3.1640\n",
            "Inference Latency (ms/token)       : 398.8616\n",
            "Avg GPU Utilization (%)            : 19.2989\n",
            "Avg GPU Memory Allocated (GB)      : 3.2321\n",
            "------------------------------------------------------------\n",
            "Starting cleanup for INT8 (FP32 Compute) ARC-Challenge cell...\n",
            "INT8 model deleted.\n",
            "GPU cache cleared and garbage collected after INT8 (FP32 Compute) ARC-Challenge run.\n"
          ]
        }
      ],
      "source": [
        "# --- INT8 (FP32 Compute) ARC-Challenge Benchmark Evaluation Cell --- # <<< Title updated\n",
        "\n",
        "# Assume 'handle' and 'stop_polling_event' exist from a previous cell.\n",
        "if 'handle' not in locals() or not handle:\n",
        "    print(\"Error: NVML handle not found. Please initialize NVML in a prior cell.\")\n",
        "    handle = None # Prevent polling\n",
        "elif 'stop_polling_event' not in locals():\n",
        "    print(\"Error: stop_polling_event not found. Please initialize in a prior cell.\")\n",
        "    # If needed, define fallback: stop_polling_event = threading.Event()\n",
        "\n",
        "# Clear previous readings and reset event\n",
        "gpu_utilization_readings = []\n",
        "if 'stop_polling_event' in locals():\n",
        "    stop_polling_event.clear()\n",
        "\n",
        "# Start GPU polling thread\n",
        "polling_thread = None\n",
        "if handle:\n",
        "    polling_thread = threading.Thread(target=poll_gpu_utilization, args=(handle, 1.0), daemon=True)\n",
        "    polling_thread.start()\n",
        "\n",
        "# Clean memory before starting\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "int8_arc_metrics = {}          # <<< Variable renamed\n",
        "int8_arc_results_data = {}     # <<< Variable renamed\n",
        "model_int8 = None # Define outside try <<< Variable renamed\n",
        "\n",
        "try:\n",
        "    # Configure quantization to INT8 with FP32 compute\n",
        "    print(\"Configuring INT8 quantization (FP32 Compute)...\") # <<< Log updated\n",
        "    # Note: For INT8 with a specific compute dtype (like FP32),\n",
        "    # we use BitsAndBytesConfig.\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_8bit=True,\n",
        "        # bnb_8bit_quant_type=\"int8\", # Often implicit when load_in_8bit=True\n",
        "        bnb_8bit_compute_dtype=torch.float32, # <--- Specify FP32 compute\n",
        "        # bnb_8bit_use_double_quant=True # Check if necessary/applicable for INT8\n",
        "    )\n",
        "    print(\"Quantization config created.\")\n",
        "\n",
        "    # Load quantized model (INT8)\n",
        "    model_name = \"Qwen/Qwen2.5-3B\" # Base model name\n",
        "    print(f\"Loading model: {model_name} (INT8 Quantized, FP32 Compute)...\") # <<< Log updated\n",
        "    model_int8 = AutoModelForCausalLM.from_pretrained( # <<< Variable renamed\n",
        "        model_name,\n",
        "        quantization_config=bnb_config, # <--- Pass the config for FP32 compute\n",
        "        device_map=\"cuda:0\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "    print(\"Model loaded.\")\n",
        "\n",
        "    # Load tokenizer\n",
        "    print(\"Loading tokenizer...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    print(\"Tokenizer loaded.\")\n",
        "\n",
        "    # --- No reset_peak_memory_stats needed for this metric set ---\n",
        "\n",
        "    # Run ARC-Challenge evaluation using the evaluate_arc function\n",
        "    print(\"Starting ARC-Challenge evaluation (INT8, FP32 Compute)...\") # <<< Log updated\n",
        "    # Assuming evaluate_arc can handle the quantized model and returns required keys\n",
        "    # Using the correct dataset and max_new_tokens=1\n",
        "    int8_arc_results_data = evaluate_arc(model_int8, tokenizer, samples_200_arc_challenge_test, max_new_tokens=1) # <<< Variable renamed, dataset var updated, max_new_tokens=1\n",
        "    print(\"ARC-Challenge evaluation finished.\") # <<< Log updated\n",
        "\n",
        "    # Stop polling thread and calculate average utilization\n",
        "    avg_gpu_utilization = 'N/A'\n",
        "    if polling_thread and polling_thread.is_alive():\n",
        "        stop_polling_event.set()\n",
        "        polling_thread.join(timeout=5)\n",
        "        if gpu_utilization_readings:\n",
        "            avg_gpu_utilization = statistics.mean(gpu_utilization_readings)\n",
        "        else:\n",
        "             avg_gpu_utilization = 0 # Polling ran but got no readings\n",
        "    elif handle:\n",
        "        avg_gpu_utilization = 0 # Polling failed to start/run correctly\n",
        "\n",
        "    # Collect metrics in the MMLU format using results from evaluate_arc\n",
        "    accuracy_val = int8_arc_results_data.get('accuracy', 'N/A') # <<< Use renamed variable\n",
        "    if isinstance(accuracy_val, (float, int)):\n",
        "        accuracy_val *= 100 # Multiply only if it's a number\n",
        "\n",
        "    int8_arc_metrics = { # <<< Variable renamed\n",
        "        \"PPL (Perplexity)\": int8_arc_results_data.get('perplexity', 'N/A'),\n",
        "        \"Accuracy\": accuracy_val,\n",
        "        \"Memory Footprint (Model Size) (GB)\": model_int8.get_memory_footprint() / (1024 ** 3), # <<< Use renamed variable\n",
        "        \"Inference Latency (ms/token)\": int8_arc_results_data.get('inference_latency', 'N/A'),\n",
        "        \"Avg GPU Utilization (%)\": avg_gpu_utilization, # From polling\n",
        "        \"Avg GPU Memory Allocated (GB)\": int8_arc_results_data.get('avg_gpu_memory_gb', 'N/A'), # From evaluate_arc result\n",
        "    }\n",
        "\n",
        "    # Print metrics\n",
        "    print(\"\\n===== INT8 (FP32 Compute) ARC-Challenge DETAILED MODEL METRICS =====\") # <<< Title updated\n",
        "    print(\"-\" * 60)\n",
        "    if int8_arc_metrics: # <<< Use renamed variable\n",
        "        try:\n",
        "            max_key_length = max(len(key) for key in int8_arc_metrics.keys()) # <<< Use renamed variable\n",
        "        except ValueError:\n",
        "             max_key_length = 35 # Default width\n",
        "\n",
        "        for key, value in int8_arc_metrics.items(): # <<< Use renamed variable\n",
        "             if isinstance(value, (float, int)):\n",
        "                 print(f\"{key.ljust(max_key_length)} : {value:.4f}\")\n",
        "             else:\n",
        "                 print(f\"{key.ljust(max_key_length)} : {value}\") # Handles 'N/A'\n",
        "    else:\n",
        "        print(\"No metrics collected.\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "except pynvml.NVMLError as nvml_error:\n",
        "     print(f\"NVML Error during INT8 (FP32 Compute) ARC-Challenge evaluation steps: {nvml_error}\") # <<< Log updated\n",
        "except Exception as e:\n",
        "    print(f\"INT8 (FP32 Compute) ARC-Challenge Evaluation error: {e}\") # <<< Log updated\n",
        "    # Ensure polling stops on error\n",
        "    # --- Removed traceback import and print ---\n",
        "    if polling_thread and polling_thread.is_alive():\n",
        "        stop_polling_event.set()\n",
        "        try:\n",
        "            polling_thread.join(timeout=5)\n",
        "        except Exception as join_e:\n",
        "            print(f\"Error stopping polling thread after exception: {join_e}\")\n",
        "\n",
        "finally:\n",
        "    # Clean up GPU memory (No NVML Shutdown)\n",
        "    print(\"Starting cleanup for INT8 (FP32 Compute) ARC-Challenge cell...\") # <<< Log updated\n",
        "    if 'model_int8' in locals() and model_int8 is not None: # Check for model_int8 <<< Use renamed variable\n",
        "        del model_int8 # Delete model_int8 <<< Use renamed variable\n",
        "        print(\"INT8 model deleted.\")\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    print(\"GPU cache cleared and garbage collected after INT8 (FP32 Compute) ARC-Challenge run.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "id": "vfh6uIyMyosq",
        "outputId": "8cc007cc-71e4-4fb9-fecc-3952bf9576ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuring INT8 quantization (FP16 Compute)...\n",
            "Quantization config created.\n",
            "Loading model: Qwen/Qwen2.5-3B (INT8 Quantized, FP16 Compute)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3bfa74eeaec146f9af35b17f25b63289"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded.\n",
            "Loading tokenizer...\n",
            "Tokenizer loaded.\n",
            "Starting ARC-Challenge evaluation (INT8, FP16 Compute)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating ARC:   0%|          | 0/200 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "962894fcdc1d4fc88606e75a97c4502a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ARC-Challenge evaluation finished.\n",
            "\n",
            "===== INT8 (FP16 Compute) ARC-Challenge DETAILED MODEL METRICS =====\n",
            "------------------------------------------------------------\n",
            "PPL (Perplexity)                   : 6.4720\n",
            "Accuracy                           : 89.5000\n",
            "Memory Footprint (Model Size) (GB) : 3.1640\n",
            "Inference Latency (ms/token)       : 402.2875\n",
            "Avg GPU Utilization (%)            : 18.7753\n",
            "Avg GPU Memory Allocated (GB)      : 3.2321\n",
            "------------------------------------------------------------\n",
            "Starting cleanup for INT8 (FP16 Compute) ARC-Challenge cell...\n",
            "INT8 model deleted.\n",
            "GPU cache cleared and garbage collected after INT8 (FP16 Compute) ARC-Challenge run.\n"
          ]
        }
      ],
      "source": [
        "# --- INT8 (FP16 Compute) ARC-Challenge Benchmark Evaluation Cell --- # <<< Title updated\n",
        "\n",
        "# Assume 'handle' and 'stop_polling_event' exist from a previous cell.\n",
        "if 'handle' not in locals() or not handle:\n",
        "    print(\"Error: NVML handle not found. Please initialize NVML in a prior cell.\")\n",
        "    handle = None # Prevent polling\n",
        "elif 'stop_polling_event' not in locals():\n",
        "    print(\"Error: stop_polling_event not found. Please initialize in a prior cell.\")\n",
        "    # If needed, define fallback: stop_polling_event = threading.Event()\n",
        "\n",
        "# Clear previous readings and reset event\n",
        "gpu_utilization_readings = []\n",
        "if 'stop_polling_event' in locals():\n",
        "    stop_polling_event.clear()\n",
        "\n",
        "# Start GPU polling thread\n",
        "polling_thread = None\n",
        "if handle:\n",
        "    polling_thread = threading.Thread(target=poll_gpu_utilization, args=(handle, 1.0), daemon=True)\n",
        "    polling_thread.start()\n",
        "\n",
        "# Clean memory before starting\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "int8_arc_metrics = {}          # <<< Variable renamed\n",
        "int8_arc_results_data = {}     # <<< Variable renamed\n",
        "model_int8 = None # Define outside try <<< Variable renamed\n",
        "\n",
        "try:\n",
        "    # Configure quantization to INT8 with FP16 compute\n",
        "    print(\"Configuring INT8 quantization (FP16 Compute)...\") # <<< Log updated\n",
        "    # Note: For INT8 with a specific compute dtype (like FP16),\n",
        "    # we use BitsAndBytesConfig.\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_8bit=True,\n",
        "        # bnb_8bit_quant_type=\"int8\", # Often implicit when load_in_8bit=True\n",
        "        bnb_8bit_compute_dtype=torch.float16, # <--- Specify FP16 compute\n",
        "        # bnb_8bit_use_double_quant=True # Check if necessary/applicable for INT8\n",
        "    )\n",
        "    print(\"Quantization config created.\")\n",
        "\n",
        "    # Load quantized model (INT8)\n",
        "    model_name = \"Qwen/Qwen2.5-3B\" # Base model name\n",
        "    print(f\"Loading model: {model_name} (INT8 Quantized, FP16 Compute)...\") # <<< Log updated\n",
        "    model_int8 = AutoModelForCausalLM.from_pretrained( # <<< Variable renamed\n",
        "        model_name,\n",
        "        quantization_config=bnb_config, # <--- Pass the config for FP16 compute\n",
        "        device_map=\"cuda:0\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "    print(\"Model loaded.\")\n",
        "\n",
        "    # Load tokenizer\n",
        "    print(\"Loading tokenizer...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    print(\"Tokenizer loaded.\")\n",
        "\n",
        "    # --- No reset_peak_memory_stats needed for this metric set ---\n",
        "\n",
        "    # Run ARC-Challenge evaluation using the evaluate_arc function\n",
        "    print(\"Starting ARC-Challenge evaluation (INT8, FP16 Compute)...\") # <<< Log updated\n",
        "    # Assuming evaluate_arc can handle the quantized model and returns required keys\n",
        "    # Using the correct dataset and max_new_tokens=1\n",
        "    int8_arc_results_data = evaluate_arc(model_int8, tokenizer, samples_200_arc_challenge_test, max_new_tokens=1) # <<< Variable renamed, dataset var updated, max_new_tokens=1\n",
        "    print(\"ARC-Challenge evaluation finished.\") # <<< Log updated\n",
        "\n",
        "    # Stop polling thread and calculate average utilization\n",
        "    avg_gpu_utilization = 'N/A'\n",
        "    if polling_thread and polling_thread.is_alive():\n",
        "        stop_polling_event.set()\n",
        "        polling_thread.join(timeout=5)\n",
        "        if gpu_utilization_readings:\n",
        "            avg_gpu_utilization = statistics.mean(gpu_utilization_readings)\n",
        "        else:\n",
        "             avg_gpu_utilization = 0 # Polling ran but got no readings\n",
        "    elif handle:\n",
        "        avg_gpu_utilization = 0 # Polling failed to start/run correctly\n",
        "\n",
        "    # Collect metrics in the MMLU format using results from evaluate_arc\n",
        "    accuracy_val = int8_arc_results_data.get('accuracy', 'N/A') # <<< Use renamed variable\n",
        "    if isinstance(accuracy_val, (float, int)):\n",
        "        accuracy_val *= 100 # Multiply only if it's a number\n",
        "\n",
        "    int8_arc_metrics = { # <<< Variable renamed\n",
        "        \"PPL (Perplexity)\": int8_arc_results_data.get('perplexity', 'N/A'),\n",
        "        \"Accuracy\": accuracy_val,\n",
        "        \"Memory Footprint (Model Size) (GB)\": model_int8.get_memory_footprint() / (1024 ** 3), # <<< Use renamed variable\n",
        "        \"Inference Latency (ms/token)\": int8_arc_results_data.get('inference_latency', 'N/A'),\n",
        "        \"Avg GPU Utilization (%)\": avg_gpu_utilization, # From polling\n",
        "        \"Avg GPU Memory Allocated (GB)\": int8_arc_results_data.get('avg_gpu_memory_gb', 'N/A'), # From evaluate_arc result\n",
        "    }\n",
        "\n",
        "    # Print metrics\n",
        "    print(\"\\n===== INT8 (FP16 Compute) ARC-Challenge DETAILED MODEL METRICS =====\") # <<< Title updated\n",
        "    print(\"-\" * 60)\n",
        "    if int8_arc_metrics: # <<< Use renamed variable\n",
        "        try:\n",
        "            max_key_length = max(len(key) for key in int8_arc_metrics.keys()) # <<< Use renamed variable\n",
        "        except ValueError:\n",
        "             max_key_length = 35 # Default width\n",
        "\n",
        "        for key, value in int8_arc_metrics.items(): # <<< Use renamed variable\n",
        "             if isinstance(value, (float, int)):\n",
        "                 print(f\"{key.ljust(max_key_length)} : {value:.4f}\")\n",
        "             else:\n",
        "                 print(f\"{key.ljust(max_key_length)} : {value}\") # Handles 'N/A'\n",
        "    else:\n",
        "        print(\"No metrics collected.\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "except pynvml.NVMLError as nvml_error:\n",
        "     print(f\"NVML Error during INT8 (FP16 Compute) ARC-Challenge evaluation steps: {nvml_error}\") # <<< Log updated\n",
        "except Exception as e:\n",
        "    print(f\"INT8 (FP16 Compute) ARC-Challenge Evaluation error: {e}\") # <<< Log updated\n",
        "    # Ensure polling stops on error\n",
        "    # --- Removed traceback import and print ---\n",
        "    if polling_thread and polling_thread.is_alive():\n",
        "        stop_polling_event.set()\n",
        "        try:\n",
        "            polling_thread.join(timeout=5)\n",
        "        except Exception as join_e:\n",
        "            print(f\"Error stopping polling thread after exception: {join_e}\")\n",
        "\n",
        "finally:\n",
        "    # Clean up GPU memory (No NVML Shutdown)\n",
        "    print(\"Starting cleanup for INT8 (FP16 Compute) ARC-Challenge cell...\") # <<< Log updated\n",
        "    if 'model_int8' in locals() and model_int8 is not None: # Check for model_int8 <<< Use renamed variable\n",
        "        del model_int8 # Delete model_int8 <<< Use renamed variable\n",
        "        print(\"INT8 model deleted.\")\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    print(\"GPU cache cleared and garbage collected after INT8 (FP16 Compute) ARC-Challenge run.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
